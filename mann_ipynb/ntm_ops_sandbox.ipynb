{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head\n",
    "## Focusing by location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys= Tensor(\"Const:0\", shape=(2, 3), dtype=float32)\n",
      "memory= Tensor(\"transpose:0\", shape=(3, 4), dtype=float32)\n",
      "keys =\n",
      " [array([[ 0.2       ,  0.30000001,  0.40000001],\n",
      "       [ 0.1       ,  0.2       ,  0.69999999]], dtype=float32)]\n",
      "memory =\n",
      " [array([[ 0.2       ,  0.2       ,  0.30000001,  0.1       ],\n",
      "       [ 0.30000001,  0.30000001,  0.30000001,  0.2       ],\n",
      "       [ 0.40000001,  0.40000001,  0.30000001,  0.69999999]], dtype=float32)]\n",
      "focus =\n",
      " [array([[  4.92605865e-01,   4.92605865e-01,   1.47291617e-02,\n",
      "          5.90993950e-05],\n",
      "       [  1.19946955e-04,   1.19946955e-04,   4.91867769e-10,\n",
      "          9.99760211e-01]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "def build_focusing_by_location_2D(keys, beta, memory):\n",
    "    \"\"\"Computes content addressing. Uses both address and content part for calculation of the similarity.\n",
    "    Memory is 2D i.e. shared between batch samples.\n",
    "    Args:\n",
    "        key: a 2-D Tensor [BATCH_SIZE x SLOT_SIZE] \n",
    "        beta: a 1-D Tensor - key strength [BATCH_SIZE x 1]\n",
    "        memory: a 2-D Tensor [SLOT_SIZE x NUMBER_OF_SLOTS]\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"focusing_by_location\"):\n",
    "    \n",
    "        # Normalize batch - along samples.\n",
    "        norm_keys = tf.nn.l2_normalize(keys,1, name=\"normalized_keys\")\n",
    "        # Normalize memory - along slots \n",
    "        norm_memory = tf.nn.l2_normalize(memory, 0)\n",
    "\n",
    "        # Calculate cosine similarity [BATCH_SIZE x NUMBER_OF_SLOTS].\n",
    "        similarity = tf.matmul(norm_keys, norm_memory, name=\"similarity\")\n",
    "\n",
    "        # Element-wise multiplication [BATCH_SIZE x NUMBER_OF_SLOTS]\n",
    "        strengthtened_similarity = beta * similarity\n",
    "\n",
    "        # Calculate weighting based on similarity along the \"slot dimension\" [BATCH_SIZE x NUMBER_OF_SLOTS].\n",
    "        result = tf.nn.softmax(strengthtened_similarity, dim=1)\n",
    "        return result\n",
    "\n",
    "\n",
    "# test focusing\n",
    "keys = tf.constant([[0.2, 0.3, 0.4],[0.1, 0.2, 0.7]], dtype=tf.float32)\n",
    "print(\"keys=\",keys)\n",
    "beta= tf.constant([[100.0],[100.0]], dtype=tf.float32)\n",
    "\n",
    "memory = tf.transpose(tf.constant([[0.2, 0.3, 0.4],[0.2, 0.3, 0.4],[0.3, 0.3, 0.3],[0.1, 0.2, 0.7]], dtype=tf.float32))\n",
    "print(\"memory=\",memory)\n",
    "\n",
    "focus = build_focusing_by_location_2D(keys, beta, memory)\n",
    "\n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "    \n",
    "# Execute graph.\n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(initialize_model)\n",
    "print(\"keys =\\n\",sess.run([keys]))\n",
    "print(\"memory =\\n\",sess.run([memory]))\n",
    "print(\"focus =\\n\",sess.run([focus]))\n",
    "# [7,1,2,3,4,5,6]\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys= Tensor(\"Const:0\", shape=(2, 3), dtype=float32)\n",
      "memory= Tensor(\"stack:0\", shape=(2, 3, 4), dtype=float32)\n",
      "norm_keys_slice= Tensor(\"focusing_by_location/Slice:0\", shape=(1, 3), dtype=float32)\n",
      "norm_memory= Tensor(\"focusing_by_location/l2_normalize:0\", shape=(3, 4), dtype=float32)\n",
      "norm_keys_slice= Tensor(\"focusing_by_location/Slice_1:0\", shape=(1, 3), dtype=float32)\n",
      "norm_memory= Tensor(\"focusing_by_location/l2_normalize_1:0\", shape=(3, 4), dtype=float32)\n",
      "Tensor(\"focusing_by_location/stack:0\", shape=(2, 1, 4), dtype=float32)\n",
      "keys =\n",
      " [array([[ 0.2       ,  0.30000001,  0.40000001],\n",
      "       [ 0.1       ,  0.2       ,  0.69999999]], dtype=float32)]\n",
      "memory =\n",
      " [array([[[ 0.2       ,  0.2       ,  0.30000001,  0.1       ],\n",
      "        [ 0.30000001,  0.30000001,  0.30000001,  0.2       ],\n",
      "        [ 0.40000001,  0.40000001,  0.30000001,  0.69999999]],\n",
      "\n",
      "       [[ 0.1       ,  0.        ,  0.89999998,  0.80000001],\n",
      "        [ 0.2       ,  0.60000002,  0.        ,  0.1       ],\n",
      "        [ 0.69999999,  0.80000001,  0.1       ,  0.1       ]]], dtype=float32)]\n",
      "focus =\n",
      " [array([[[  4.92605865e-01,   4.92605865e-01,   1.47291617e-02,\n",
      "           5.90993950e-05]],\n",
      "\n",
      "       [[  4.31139648e-01,   3.71354699e-01,   9.43793803e-02,\n",
      "           1.03126228e-01]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "def build_focusing_by_location_3D_iterative(keys, beta, memory_batch):\n",
    "    \"\"\"Computes content addressing. Uses both address and content part for calculation of the similarity.\n",
    "    Memory is 3D i.e. every sample in the batch has its own \"memory slice\".\n",
    "    Version that iterates through samples in the batch.\n",
    "    Args:\n",
    "        keys: a 2-D Tensor [BATCH_SIZE x SLOT_SIZE] \n",
    "        beta: a 1-D Tensor - key strength [BATCH_SIZE x 1]\n",
    "        memory: a 3-D !! Tensor [BATCH_SIZE x SLOT_SIZE x NUMBER_OF_SLOTS]\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"focusing_by_location\"):\n",
    "        \n",
    "        batch_size = int(memory_batch.get_shape()[0])\n",
    "        slot_size = int(keys.get_shape()[1])\n",
    "        # Decompose memory into list of BATCH_SIZE memories of size [SLOT_SIZE x NUMBER_OF_SLOTS].\n",
    "        memory_list = tf.unstack(memory_batch, axis=0)\n",
    "\n",
    "        # Normalize batch - along samples.\n",
    "        norm_keys = tf.nn.l2_normalize(keys,1, name=\"normalized_keys\")\n",
    "\n",
    "        weighting = list()\n",
    "        # Iterate through samples in batch.\n",
    "        for b in range(batch_size):\n",
    "                        \n",
    "            # Normalize memory - along slots \n",
    "            norm_memory = tf.nn.l2_normalize(memory_list[b], 0)\n",
    "\n",
    "            # Get a single key [1 x SLOT_SIZE]\n",
    "            norm_keys_slice = tf.slice(norm_keys, [b, 0], [1, slot_size])\n",
    "            \n",
    "            # Calculate cosine similarity [BATCH_SIZE x NUMBER_OF_SLOTS].\n",
    "            print(\"norm_keys_slice=\", norm_keys_slice)\n",
    "            print(\"norm_memory=\",norm_memory)\n",
    "            similarity = tf.matmul(norm_keys_slice, norm_memory, name=\"similarity\")\n",
    "\n",
    "            # Element-wise multiplication [BATCH_SIZE x NUMBER_OF_SLOTS]\n",
    "            strengthtened_similarity = beta[b] * similarity\n",
    "\n",
    "            # Calculate weighting based on similarity along the \"slot dimension\" [BATCH_SIZE x NUMBER_OF_SLOTS].\n",
    "            weighting.append(tf.nn.softmax(strengthtened_similarity, dim=1))\n",
    "            \n",
    "        # \"Stack\" results into 2D tensor [BATCH_SIZE x NUMBER_OF_SLOTS]\n",
    "        result = tf.stack(weighting, axis=0)\n",
    "        print (result)\n",
    "        return result\n",
    "\n",
    "\n",
    "# test focusing\n",
    "keys = tf.constant([[0.2, 0.3, 0.4],[0.1, 0.2, 0.7]], dtype=tf.float32)\n",
    "print(\"keys=\",keys)\n",
    "beta= tf.constant([[100.0],[2.0]], dtype=tf.float32)\n",
    "\n",
    "memory_0 = tf.transpose(tf.constant([[0.2, 0.3, 0.4],[0.2, 0.3, 0.4],[0.3, 0.3, 0.3],[0.1, 0.2, 0.7]], dtype=tf.float32))\n",
    "memory_1 = tf.transpose(tf.constant([[0.1, 0.2, 0.7],[0.0, 0.6, 0.8],[0.9, 0.0, 0.1],[0.8, 0.1, 0.1]], dtype=tf.float32))\n",
    "memory = tf.stack([memory_0, memory_1])\n",
    "print(\"memory=\",memory)\n",
    "\n",
    "focus = build_focusing_by_location_3D_iterative(keys, beta, memory)\n",
    "\n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "    \n",
    "# Execute graph.\n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(initialize_model)\n",
    "print(\"keys_BxS =\\n\",sess.run([keys]))\n",
    "print(\"memory =\\n\",sess.run([memory]))\n",
    "print(\"focus =\\n\",sess.run([focus]))\n",
    "# [7,1,2,3,4,5,6]\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys= Tensor(\"Const:0\", shape=(2, 3), dtype=float32)\n",
      "memory= Tensor(\"stack:0\", shape=(2, 3, 4), dtype=float32)\n",
      "weighting_BxN= Tensor(\"focusing_by_location/Squeeze:0\", shape=(2, 4), dtype=float32)\n",
      "keys_BxS =\n",
      " [array([[ 0.2       ,  0.30000001,  0.40000001],\n",
      "       [ 0.1       ,  0.2       ,  0.69999999]], dtype=float32)]\n",
      "memory =\n",
      " [array([[[ 0.2       ,  0.2       ,  0.30000001,  0.1       ],\n",
      "        [ 0.30000001,  0.30000001,  0.30000001,  0.2       ],\n",
      "        [ 0.40000001,  0.40000001,  0.30000001,  0.69999999]],\n",
      "\n",
      "       [[ 0.1       ,  0.        ,  0.89999998,  0.80000001],\n",
      "        [ 0.2       ,  0.60000002,  0.        ,  0.1       ],\n",
      "        [ 0.69999999,  0.80000001,  0.1       ,  0.1       ]]], dtype=float32)]\n",
      "focus =\n",
      " [array([[  4.92605865e-01,   4.92605865e-01,   1.47291617e-02,\n",
      "          5.90993950e-05],\n",
      "       [  4.31139648e-01,   3.71354699e-01,   9.43793803e-02,\n",
      "          1.03126228e-01]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "def build_focusing_by_location_3D(keys_BxS_, beta_Bx1_, prev_memory_BxSxN_):\n",
    "    \"\"\"Computes content addressing. Uses both address and content part for calculation of the similarity.\n",
    "    Memory is 3D i.e. every sample in the batch has its own \"memory slice\".\n",
    "    Args:\n",
    "        keys_BxS_: a 2-D Tensor [BATCH_SIZE x SLOT_SIZE] \n",
    "        beta_Bx1_: a 2-D Tensor - key strength [BATCH_SIZE x 1]\n",
    "        prev_memory_BxSxN_: a 3-D !! Tensor [BATCH_SIZE x SLOT_SIZE x NUMBER_OF_SLOTS]\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"focusing_by_location\"):\n",
    "        \n",
    "        # Normalize key - along samples [BATCH_SIZE x 1 x SLOT_SIZE]\n",
    "        keys_Bx1xS = tf.expand_dims(keys_BxS_, 1)\n",
    "        norm_keys_Bx1xS = tf.nn.l2_normalize(keys_Bx1xS,2, name=\"norm_keys_Bx1xS\")\n",
    "\n",
    "        # Normalize memory - along slots [BATCH_SIZE x SLOT_SIZE x NUMBER_OF_SLOTS]\n",
    "        norm_memory_BxSxN = tf.nn.l2_normalize(prev_memory_BxSxN_, 1, name=\"norm_memory_BxSxN\")\n",
    "        \n",
    "        # Calculate batched cosine similarity [BATCH_SIZE x 1 x NUMBER_OF_SLOTS]\n",
    "        similarity_Bx1xN= tf.matmul(norm_keys_Bx1xS, norm_memory_BxSxN, name=\"similarity_Bx1xN\")\n",
    "        \n",
    "        # Element-wise multiplication [BATCH_SIZE x 1 x NUMBER_OF_SLOTS]\n",
    "        beta_Bx1x1 = tf.expand_dims(beta_Bx1_, 1)\n",
    "        strengthtened_similarity_Bx1xN = tf.matmul(beta_Bx1x1, similarity_Bx1xN, name=\"strengthtened_similarity_Bx1xN\")\n",
    "\n",
    "        # Calculate weighting based on similarity along the \"slot dimension\" [BATCH_SIZE x NUMBER_OF_SLOTS].\n",
    "        weighting_Bx1xN = tf.nn.softmax(strengthtened_similarity_Bx1xN, dim=2)\n",
    "            \n",
    "        # \"Squeeze\" results into 2D tensor [BATCH_SIZE x NUMBER_OF_SLOTS]\n",
    "        weighting_BxN = tf.squeeze(weighting_Bx1xN)\n",
    "        print (\"weighting_BxN=\",weighting_BxN)\n",
    "        return weighting_BxN\n",
    "\n",
    "\n",
    "# test focusing\n",
    "keys = tf.constant([[0.2, 0.3, 0.4],[0.1, 0.2, 0.7]], dtype=tf.float32)\n",
    "print(\"keys=\",keys)\n",
    "beta= tf.constant([[100.0],[2.0]], dtype=tf.float32)\n",
    "\n",
    "memory_0 = tf.transpose(tf.constant([[0.2, 0.3, 0.4],[0.2, 0.3, 0.4],[0.3, 0.3, 0.3],[0.1, 0.2, 0.7]], dtype=tf.float32))\n",
    "memory_1 = tf.transpose(tf.constant([[0.1, 0.2, 0.7],[0.0, 0.6, 0.8],[0.9, 0.0, 0.1],[0.8, 0.1, 0.1]], dtype=tf.float32))\n",
    "memory = tf.stack([memory_0, memory_1])\n",
    "print(\"memory=\",memory)\n",
    "\n",
    "focus = build_focusing_by_location_3D(keys, beta, memory)\n",
    "\n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "    \n",
    "# Execute graph.\n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(initialize_model)\n",
    "print(\"keys_BxS =\\n\",sess.run([keys]))\n",
    "print(\"memory =\\n\",sess.run([memory]))\n",
    "print(\"focus =\\n\",sess.run([focus]))\n",
    "# [7,1,2,3,4,5,6]\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circular convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v= Tensor(\"Const:0\", shape=(3, 7), dtype=float32)\n",
      "k= Tensor(\"Const_1:0\", shape=(3, 3), dtype=float32)\n",
      "conv =\n",
      " [array([[ 7.        ,  1.        ,  2.        ,  3.        ,  4.        ,\n",
      "         5.        ,  6.        ],\n",
      "       [ 4.        ,  1.5       ,  2.5       ,  3.5       ,  4.5       ,\n",
      "         5.5       ,  6.5       ],\n",
      "       [ 0.2       ,  0.30000001,  0.40000001,  0.5       ,  0.60000002,\n",
      "         0.69999999,  0.1       ]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "def circular_index(idx, size):\n",
    "    if idx < 0: return size + idx\n",
    "    if idx >= size : return idx - size\n",
    "    else: return idx\n",
    "\n",
    "def build_circular_convolution(batch, kernel):\n",
    "    \"\"\"Computes circular convolution.\n",
    "    Args:\n",
    "        batch: a 2-D Tensor [BATCH_SIZE x NUMBER_OF_SLOTS] \n",
    "        kernel: a 2-D Tensor [BATCH_SIZE x KERNEL_SIZE (e.g. 3)]\n",
    "    \"\"\"\n",
    "    size = int(batch.get_shape()[1])\n",
    "    kernel_size = int(k.get_shape()[1])\n",
    "    kernel_shift = int(math.floor(kernel_size/2.0))\n",
    "\n",
    "    kernels = []\n",
    "    for i in range(size):\n",
    "        # Create a list of index vectors.\n",
    "        indices = [circular_index(i+j, size) for j in range(kernel_shift, -kernel_shift-1, -1)]\n",
    "        # Reorganize batch according to indices. \n",
    "        reorganized_batch = tf.gather(batch, indices, axis=1)\n",
    "        # Perform convolution.\n",
    "        kernels.append(tf.reduce_sum(reorganized_batch * kernel, 1))\n",
    "    # Sum elements lying on the same positions.\n",
    "    result = tf.transpose(tf.dynamic_stitch([i for i in range(size)], kernels))\n",
    "    return result\n",
    "\n",
    "# test circular convolution\n",
    "v = tf.constant([[1,2,3,4,5,6,7],[1,2,3,4,5,6,7],[0.1,0.2,0.3,0.4,0.5,0.6,0.7]], dtype=tf.float32)\n",
    "print(\"v=\",v)\n",
    "k = tf.constant([[0,0,1],[0,0.5,0.5],[1,0,0]], dtype=tf.float32)\n",
    "print(\"k=\",k)\n",
    "\n",
    "conv = build_circular_convolution(v, k)\n",
    "\n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "    \n",
    "# Execute graph.\n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(initialize_model)\n",
    "print(\"conv =\\n\",sess.run([conv]))\n",
    "# [7,1,2,3,4,5,6]\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sharpening\n",
    "Requirements: EPS = 1e-40, gamma truncated to 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v= Tensor(\"Placeholder:0\", shape=(?, 3), dtype=float32)\n",
      "g= Tensor(\"Placeholder_1:0\", shape=(?, 1), dtype=float32)\n",
      "gammas_stacked= Tensor(\"sharpening/transpose:0\", shape=(?, 3), dtype=float32)\n",
      "powed_batch= Tensor(\"sharpening/add:0\", shape=(?, 3), dtype=float32)\n",
      "sharpened_batch= Tensor(\"sharpening/truediv:0\", shape=(?, 3), dtype=float32)\n",
      "v =\n",
      " [array([[ 0.2       ,  0.30000001,  0.40000001],\n",
      "       [ 0.        ,  0.30000001,  0.89999998],\n",
      "       [ 0.30000001,  0.30000001,  0.30000001],\n",
      "       [ 0.1       ,  0.2       ,  0.69999999]], dtype=float32)]\n",
      "sharp_v =\n",
      " [array([[  7.88868831e-11,   5.66400843e-07,   9.99999404e-01],\n",
      "       [  8.33333336e-31,   2.50000000e-01,   7.49999940e-01],\n",
      "       [  3.33333313e-01,   3.33333313e-01,   3.33333313e-01],\n",
      "       [  5.56030187e-23,   5.56036435e-23,   1.00000000e+00]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "def build_sharpening(batch, gamma):\n",
    "    \"\"\"Computes sharpening.\n",
    "    Args:\n",
    "        batch: a 2-D Tensor [BATCH_SIZE x NUMBER_OF_SLOTS] \n",
    "        gamma: a 1-D Tensor [BATCH_SIZE x 1]\n",
    "    \"\"\"\n",
    "    EPS = 1e-30\n",
    "    with tf.name_scope(\"sharpening\"):    \n",
    "        number_of_slots = int(batch.get_shape()[1])\n",
    "\n",
    "        # Duplicate gammas - tf.tile is not working for partially unknown shape :] \n",
    "        gammas = []\n",
    "        for i in range(number_of_slots):\n",
    "            # Truncates gamma to 50!\n",
    "            gammas.append(tf.minimum(gamma[:,0], 50))\n",
    "        gammas_stacked = tf.transpose(tf.stack(gammas))\n",
    "        print(\"gammas_stacked=\", gammas_stacked)\n",
    "        # Calculate powered batch [BATCH_SIZE x NUMBER_OF_SLOTS].\n",
    "        powed_batch = tf.pow(batch, gammas_stacked)+EPS\n",
    "        print(\"powed_batch=\", powed_batch)\n",
    "\n",
    "        # \"Normalization\" [BATCH_SIZE x NUMBER_OF_SLOTS].\n",
    "        sharpened_batch = (powed_batch) / (tf.reduce_sum(powed_batch, axis=1, keep_dims=True))\n",
    "        print(\"sharpened_batch=\",sharpened_batch)\n",
    "\n",
    "        return sharpened_batch\n",
    "\n",
    "\n",
    "NUMBER_OF_SLOTS = 3\n",
    "# test sharpening\n",
    "v = tf.placeholder(tf.float32, shape=[None, NUMBER_OF_SLOTS])\n",
    "#v = tf.constant([[0.2, 0.3, 0.4]], dtype=tf.float32)\n",
    "print(\"v=\",v)\n",
    "g = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "#g = tf.constant([1.0], dtype=tf.float32)\n",
    "print(\"g=\",g)\n",
    "\n",
    "sharp_v = build_sharpening(v, g)\n",
    "\n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "\n",
    "my_v = [[0.2, 0.3, 0.4],[0.0, 0.3, 0.9],[0.3, 0.3, 0.3],[0.1, 0.2, 0.7]]\n",
    "init_g = np.transpose([50.0, 1.0, 10.0, 50.0])\n",
    "my_g = np.reshape(init_g, [4,1])\n",
    "my_feed_dict={v: my_v, g: my_g}\n",
    "\n",
    "# Execute graph.\n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(initialize_model)\n",
    "print(\"v =\\n\",sess.run([v], feed_dict=my_feed_dict))\n",
    "print(\"sharp_v =\\n\",sess.run([sharp_v], feed_dict=my_feed_dict))\n",
    "# [7,1,2,3,4,5,6]\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Memory\n",
    "## Preserved memory (content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write_weights= Tensor(\"Const:0\", shape=(2, 4), dtype=float32)\n",
      "erase_vector= Tensor(\"Const_1:0\", shape=(2, 2), dtype=float32)\n",
      "memory= Tensor(\"stack:0\", shape=(2, 3, 4), dtype=float32)\n",
      "prev_memory_SxN= Tensor(\"content_preservation/Squeeze:0\", shape=(3, 4), dtype=float32)\n",
      "write_1xN= Tensor(\"content_preservation/Slice:0\", shape=(1, 4), dtype=float32)\n",
      "erase_Cx1= Tensor(\"content_preservation/transpose:0\", shape=(2, 1), dtype=float32)\n",
      "Tensor(\"content_preservation/sub:0\", shape=(2, 4), dtype=float32)\n",
      "prev_memory_SxN= Tensor(\"content_preservation/Squeeze_1:0\", shape=(3, 4), dtype=float32)\n",
      "write_1xN= Tensor(\"content_preservation/Slice_2:0\", shape=(1, 4), dtype=float32)\n",
      "erase_Cx1= Tensor(\"content_preservation/transpose_1:0\", shape=(2, 1), dtype=float32)\n",
      "Tensor(\"content_preservation/sub_1:0\", shape=(2, 4), dtype=float32)\n",
      "write_weights_BxN =\n",
      " [[ 1.          0.5         0.          1.        ]\n",
      " [ 0.2         0.69999999  0.1         0.        ]]\n",
      "erase_vector_BxC =\n",
      " [[ 1.   1. ]\n",
      " [ 0.1  0.1]]\n",
      "prev_memory_BxSxN =\n",
      " [[[ 0.2         0.2         0.30000001  0.1       ]\n",
      "  [ 0.30000001  0.30000001  0.30000001  0.2       ]\n",
      "  [ 0.40000001  0.40000001  0.30000001  0.69999999]]\n",
      "\n",
      " [[ 0.1         0.          0.89999998  0.80000001]\n",
      "  [ 0.2         0.60000002  0.          0.1       ]\n",
      "  [ 0.69999999  0.80000001  0.1         0.1       ]]]\n",
      "preserved_memory_mask_BxSxN =\n",
      " [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.          0.5         1.          0.        ]\n",
      "  [ 0.          0.5         1.          0.        ]]\n",
      "\n",
      " [[ 1.          1.          1.          1.        ]\n",
      "  [ 0.98000002  0.93000001  0.99000001  1.        ]\n",
      "  [ 0.98000002  0.93000001  0.99000001  1.        ]]]\n",
      "preserved_memory_BxSxN =\n",
      " [[[ 0.2         0.2         0.30000001  0.1       ]\n",
      "  [ 0.          0.15000001  0.30000001  0.        ]\n",
      "  [ 0.          0.2         0.30000001  0.        ]]\n",
      "\n",
      " [[ 0.1         0.          0.89999998  0.80000001]\n",
      "  [ 0.19600001  0.55800003  0.          0.1       ]\n",
      "  [ 0.68599999  0.74400002  0.099       0.1       ]]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "ADDRESS_SIZE = 1\n",
    "CONTENT_SIZE = 2\n",
    "SLOT_SIZE = ADDRESS_SIZE + CONTENT_SIZE\n",
    "NUMBER_OF_SLOTS=4\n",
    "\n",
    "def build_memory_preservation_iterative(write_weights_BxN_, erase_vector_BxC_, prev_memory_BxSxN_):\n",
    "    \"\"\"Computes how much memory will be preserved using weights and erase vector as params.\n",
    "    Version that iterates through samples in the batch.\n",
    "    Args:\n",
    "        write_weights_BxN: a 2-D Tensor [BATCH_SIZE x NUMBER_OF_SLOTS] \n",
    "        erase_vector_BxC: a 2-D Tensor [BATCH_SIZE x CONTENT_SIZE]\n",
    "        prev_memory_BxSxN: a 3-D !! Tensor [BATCH_SIZE x SLOT_SIZE x NUMBER_OF_SLOTS]\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"memory_preservation\"):\n",
    "        \n",
    "        #batch_size = int(memory_batch.get_shape()[0])\n",
    "\n",
    "        # Decompose memory into a list of BATCH_SIZE memories of size [SLOT_SIZE x NUMBER_OF_SLOTS].\n",
    "        #prev_memory_B_SxN = tf.unstack(prev_memory_BxSxN_, axis=0)\n",
    "        preserved_memory_B_SxN = list()\n",
    "        preserved_memory_mask_B_SxN = list()\n",
    "\n",
    "        # Iterate through samples in batch.\n",
    "        for b in range(BATCH_SIZE):\n",
    "\n",
    "            # Get part of the memory for given sample - [SLOT_SIZE x NUMBER_OF_SLOTS]\n",
    "            prev_memory_SxN = tf.squeeze(tf.slice(prev_memory_BxSxN_, [b, 0, 0], [1, SLOT_SIZE, NUMBER_OF_SLOTS],\n",
    "                                          name=\"memory_slot\"))\n",
    "            print(\"prev_memory_SxN=\",prev_memory_SxN)\n",
    "            \n",
    "            # Get erase vector.\n",
    "            write_1xN = tf.slice(write_weights_BxN_, [b,0], [1, NUMBER_OF_SLOTS])\n",
    "            print(\"write_1xN=\",write_1xN)\n",
    "            erase_Cx1 = tf.transpose(tf.slice(erase_vector_BxC_, [b,0], [1, CONTENT_SIZE]))\n",
    "            print(\"erase_Cx1=\",erase_Cx1)\n",
    "            preserved_content_mask_CxN = tf.ones([CONTENT_SIZE, NUMBER_OF_SLOTS]) - tf.matmul(erase_Cx1, write_1xN)\n",
    "            print(preserved_content_mask_CxN)\n",
    "            \n",
    "            preserved_memory_mask_SxN = tf.concat(\n",
    "                [tf.ones([ADDRESS_SIZE, NUMBER_OF_SLOTS], tf.float32), preserved_content_mask_CxN],\n",
    "                axis=0)\n",
    "            \n",
    "            preserved_memory_B_SxN.append(preserved_memory_mask_SxN * prev_memory_SxN)\n",
    "            preserved_memory_mask_B_SxN.append(preserved_memory_mask_SxN)\n",
    "        # 3. Preservation gate p [CONTENT_SIZE x NUMBER_OF_SLOTS]\n",
    "        #preserved_content_BxCxN = tf.ones([BATCH_SIZE, CONTENT_SIZE, NUMBER_OF_SLOTS]) - \n",
    "        #    tf.matmul(tf.transpose(erase_vector_BxC), write_weights_BxN)\n",
    "        # How much memory will \"preserve\" [SLOT_SIZE x NUMBER_OF_SLOTS] \n",
    "        #preserved_memory_SxN = prev_memory_SxN * tf.concat(\n",
    "        #    [tf.ones([ADDRESS_SIZE, NUMBER_OF_SLOTS], tf.float32), preserved_content_CxN],\n",
    "        #    axis=0)\n",
    "\n",
    "\n",
    "        #        print (preserved_content_BxCxN)\n",
    "        preserved_memory_BxSxN = tf.stack(preserved_memory_B_SxN)\n",
    "        preserved_memory_mask_BxSxN = tf.stack(preserved_memory_mask_B_SxN)\n",
    "        return preserved_memory_BxSxN, preserved_memory_mask_BxSxN\n",
    "\n",
    "\n",
    "# Write weights: [2 samples x 4 slots]\n",
    "write_weights = tf.constant([[1.0, 0.5, 0.0, 1.0],[0.2, 0.7, 0.1, 0.0]], dtype=tf.float32)\n",
    "print(\"write_weights=\",write_weights)\n",
    "# erase_vector: [2 samples x 2 content_size]\n",
    "erase_vector= tf.constant([[1.0, 1.0],[0.1, 0.1]], dtype=tf.float32)\n",
    "print(\"erase_vector=\",erase_vector)\n",
    "\n",
    "# Memory: [2 samples x 4 slots x 3 slot_size]\n",
    "memory_0 = tf.transpose(tf.constant([[0.2, 0.3, 0.4],[0.2, 0.3, 0.4],[0.3, 0.3, 0.3],[0.1, 0.2, 0.7]], dtype=tf.float32))\n",
    "memory_1 = tf.transpose(tf.constant([[0.1, 0.2, 0.7],[0.0, 0.6, 0.8],[0.9, 0.0, 0.1],[0.8, 0.1, 0.1]], dtype=tf.float32))\n",
    "memory = tf.stack([memory_0, memory_1])\n",
    "print(\"memory=\",memory)\n",
    "\n",
    "pres_mem, pres_mem_mask = build_memory_preservation_iterative(write_weights, erase_vector, memory)\n",
    "\n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "    \n",
    "# Execute graph.\n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(initialize_model)\n",
    "write_weights_BxN, erase_vector_BxC, prev_memory_BxSxN, preserved_memory_BxSxN, preserved_memory_mask_BxSxN = \\\n",
    "    sess.run([write_weights, erase_vector, memory, pres_mem, pres_mem_mask])\n",
    "print(\"write_weights_BxN =\\n\",write_weights_BxN)\n",
    "print(\"erase_vector_BxC =\\n\",erase_vector_BxC)\n",
    "print(\"prev_memory_BxSxN =\\n\",prev_memory_BxSxN)\n",
    "print(\"preserved_memory_mask_BxSxN =\\n\",preserved_memory_mask_BxSxN)\n",
    "print(\"preserved_memory_BxSxN =\\n\",preserved_memory_BxSxN)\n",
    "# [7,1,2,3,4,5,6]\n",
    "sess.close()\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write_weights= Tensor(\"Const:0\", shape=(2, 4), dtype=float32)\n",
      "erase_vector= Tensor(\"Const_1:0\", shape=(2, 2), dtype=float32)\n",
      "memory= Tensor(\"stack:0\", shape=(2, 3, 4), dtype=float32)\n",
      "write_weights_Bx1xN= Tensor(\"memory_preservation/ExpandDims:0\", shape=(2, 1, 4), dtype=float32)\n",
      "erase_BxCx1= Tensor(\"memory_preservation/ExpandDims_1:0\", shape=(2, 2, 1), dtype=float32)\n",
      "erase_content_mask_BxCxN= Tensor(\"memory_preservation/MatMul:0\", shape=(2, 2, 4), dtype=float32)\n",
      "Tensor(\"memory_preservation/sub:0\", shape=(2, 2, 4), dtype=float32)\n",
      "preserved_address_mask_BxAxN= Tensor(\"memory_preservation/ones:0\", shape=(2, 1, 4), dtype=float32)\n",
      "preserved_memory_mask_BxSxN= Tensor(\"memory_preservation/concat:0\", shape=(2, 3, 4), dtype=float32)\n",
      "preserved_memory_BxSxN= Tensor(\"memory_preservation/Mul:0\", shape=(2, 3, 4), dtype=float32)\n",
      "write_weights_BxN =\n",
      " [[ 1.          0.5         0.          1.        ]\n",
      " [ 0.2         0.69999999  0.1         0.        ]]\n",
      "erase_vector_BxC =\n",
      " [[ 1.   1. ]\n",
      " [ 0.1  0.1]]\n",
      "prev_memory_BxSxN =\n",
      " [[[ 0.2         0.2         0.30000001  0.1       ]\n",
      "  [ 0.30000001  0.30000001  0.30000001  0.2       ]\n",
      "  [ 0.40000001  0.40000001  0.30000001  0.69999999]]\n",
      "\n",
      " [[ 0.1         0.          0.89999998  0.80000001]\n",
      "  [ 0.2         0.60000002  0.          0.1       ]\n",
      "  [ 0.69999999  0.80000001  0.1         0.1       ]]]\n",
      "preserved_memory_mask_BxSxN =\n",
      " [[[ 1.          1.          1.          1.        ]\n",
      "  [ 0.          0.5         1.          0.        ]\n",
      "  [ 0.          0.5         1.          0.        ]]\n",
      "\n",
      " [[ 1.          1.          1.          1.        ]\n",
      "  [ 0.98000002  0.93000001  0.99000001  1.        ]\n",
      "  [ 0.98000002  0.93000001  0.99000001  1.        ]]]\n",
      "preserved_memory_BxSxN =\n",
      " [[[ 0.2         0.2         0.30000001  0.1       ]\n",
      "  [ 0.          0.15000001  0.30000001  0.        ]\n",
      "  [ 0.          0.2         0.30000001  0.        ]]\n",
      "\n",
      " [[ 0.1         0.          0.89999998  0.80000001]\n",
      "  [ 0.19600001  0.55800003  0.          0.1       ]\n",
      "  [ 0.68599999  0.74400002  0.099       0.1       ]]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "ADDRESS_SIZE = 1\n",
    "CONTENT_SIZE = 2\n",
    "SLOT_SIZE = ADDRESS_SIZE + CONTENT_SIZE\n",
    "NUMBER_OF_SLOTS=4\n",
    "\n",
    "def build_memory_preservation(write_weights_BxN_, erase_vector_BxC_, prev_memory_BxSxN_):\n",
    "    \"\"\"Computes how much memory will be preserved using weights and erase vector as params.\n",
    "    Batched version, i.e. all computations are computed without iteration through the batch samples.\n",
    "    Args:\n",
    "        write_weights_BxN: a 2-D Tensor [BATCH_SIZE x NUMBER_OF_SLOTS] \n",
    "        erase_vector_BxC: a 2-D Tensor [BATCH_SIZE x CONTENT_SIZE]\n",
    "        prev_memory_BxSxN: a 3-D !! Tensor [BATCH_SIZE x SLOT_SIZE x NUMBER_OF_SLOTS]\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"memory_preservation\"):\n",
    "        \n",
    "        # Expand dimensions of weights and erase vectors to 3D.\n",
    "        write_weights_Bx1xN = tf.expand_dims(write_weights_BxN_, axis=1)\n",
    "        print(\"write_weights_Bx1xN=\",write_weights_Bx1xN)\n",
    "        erase_BxCx1 = tf.expand_dims(erase_vector_BxC_, axis=2)\n",
    "        print(\"erase_BxCx1=\",erase_BxCx1)\n",
    "        \n",
    "        # Calculate the erase content mask.\n",
    "        erase_content_mask_BxCxN = tf.matmul(erase_BxCx1, write_weights_Bx1xN)\n",
    "        print(\"erase_content_mask_BxCxN=\",erase_content_mask_BxCxN)\n",
    "        \n",
    "        # Calculate the preserved mask.\n",
    "        preserved_content_mask_BxCxN = tf.ones_like(erase_content_mask_BxCxN) - erase_content_mask_BxCxN\n",
    "        print(preserved_content_mask_BxCxN)\n",
    "        \n",
    "        # Create the preserved address mask.\n",
    "        preserved_address_mask_BxAxN = tf.ones([int(prev_memory_BxSxN_.get_shape()[0]), ADDRESS_SIZE, NUMBER_OF_SLOTS], tf.float32)\n",
    "        print(\"preserved_address_mask_BxAxN=\",preserved_address_mask_BxAxN)\n",
    "\n",
    "        # Concatenate the latter two.\n",
    "        preserved_memory_mask_BxSxN = tf.concat(\n",
    "            [preserved_address_mask_BxAxN, preserved_content_mask_BxCxN],\n",
    "            axis=1)\n",
    "        print(\"preserved_memory_mask_BxSxN=\",preserved_memory_mask_BxSxN)\n",
    "\n",
    "        # Finally, calculate the preserved memory part.\n",
    "        preserved_memory_BxSxN = tf.multiply(preserved_memory_mask_BxSxN, prev_memory_BxSxN_)\n",
    "        print(\"preserved_memory_BxSxN=\",preserved_memory_BxSxN)\n",
    "        \n",
    "        return preserved_memory_BxSxN, preserved_memory_mask_BxSxN\n",
    "\n",
    "\n",
    "# Write weights: [2 samples x 4 slots]\n",
    "write_weights = tf.constant([[1.0, 0.5, 0.0, 1.0],[0.2, 0.7, 0.1, 0.0]], dtype=tf.float32)\n",
    "print(\"write_weights=\",write_weights)\n",
    "# erase_vector: [2 samples x 2 content_size]\n",
    "erase_vector= tf.constant([[1.0, 1.0],[0.1, 0.1]], dtype=tf.float32)\n",
    "print(\"erase_vector=\",erase_vector)\n",
    "\n",
    "# Memory: [2 samples x 4 slots x 3 slot_size]\n",
    "memory_0 = tf.transpose(tf.constant([[0.2, 0.3, 0.4],[0.2, 0.3, 0.4],[0.3, 0.3, 0.3],[0.1, 0.2, 0.7]], dtype=tf.float32))\n",
    "memory_1 = tf.transpose(tf.constant([[0.1, 0.2, 0.7],[0.0, 0.6, 0.8],[0.9, 0.0, 0.1],[0.8, 0.1, 0.1]], dtype=tf.float32))\n",
    "memory = tf.stack([memory_0, memory_1])\n",
    "print(\"memory=\",memory)\n",
    "\n",
    "pres_mem, pres_mem_mask = build_memory_preservation(write_weights, erase_vector, memory)\n",
    "\n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "    \n",
    "# Execute graph.\n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(initialize_model)\n",
    "write_weights_BxN, erase_vector_BxC, prev_memory_BxSxN, preserved_memory_BxSxN, preserved_memory_mask_BxSxN = \\\n",
    "    sess.run([write_weights, erase_vector, memory, pres_mem, pres_mem_mask])\n",
    "print(\"write_weights_BxN =\\n\",write_weights_BxN)\n",
    "print(\"erase_vector_BxC =\\n\",erase_vector_BxC)\n",
    "print(\"prev_memory_BxSxN =\\n\",prev_memory_BxSxN)\n",
    "print(\"preserved_memory_mask_BxSxN =\\n\",preserved_memory_mask_BxSxN)\n",
    "print(\"preserved_memory_BxSxN =\\n\",preserved_memory_BxSxN)\n",
    "\n",
    "# Close session.\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write_weights= Tensor(\"Const:0\", shape=(2, 4), dtype=float32)\n",
      "add_vector= Tensor(\"Const_1:0\", shape=(2, 2), dtype=float32)\n",
      "memory= Tensor(\"stack:0\", shape=(2, 3, 4), dtype=float32)\n",
      "write_weights_Bx1xN= Tensor(\"content_preservation/ExpandDims:0\", shape=(2, 1, 4), dtype=float32)\n",
      "add_vector_BxCx1= Tensor(\"content_preservation/ExpandDims_1:0\", shape=(2, 2, 1), dtype=float32)\n",
      "content_update_BxCxN= Tensor(\"content_preservation/MatMul:0\", shape=(2, 2, 4), dtype=float32)\n",
      "address_update_BxAxN= Tensor(\"content_preservation/zeros:0\", shape=(2, 1, 4), dtype=float32)\n",
      "memory_update_BxSxN= Tensor(\"content_preservation/concat:0\", shape=(2, 3, 4), dtype=float32)\n",
      "write_weights_BxN =\n",
      " [[ 1.          0.5         0.          1.        ]\n",
      " [ 0.2         0.69999999  0.1         0.        ]]\n",
      "add_vector_BxC =\n",
      " [[ 1.   1. ]\n",
      " [ 0.1  0.1]]\n",
      "memory_update_BxSxN =\n",
      " [[[ 0.    0.    0.    0.  ]\n",
      "  [ 1.    0.5   0.    1.  ]\n",
      "  [ 1.    0.5   0.    1.  ]]\n",
      "\n",
      " [[ 0.    0.    0.    0.  ]\n",
      "  [ 0.02  0.07  0.01  0.  ]\n",
      "  [ 0.02  0.07  0.01  0.  ]]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "ADDRESS_SIZE = 1\n",
    "CONTENT_SIZE = 2\n",
    "SLOT_SIZE = ADDRESS_SIZE + CONTENT_SIZE\n",
    "NUMBER_OF_SLOTS=4\n",
    "\n",
    "def build_memory_update(write_weights_BxN_, add_vector_BxC_):\n",
    "    \"\"\"Computes the update that will be added to the memory.\n",
    "    Assumes that memory is a 3D tensor  [BATCH_SIZE x SLOT_SIZE x NUMBER_OF_SLOTS]\n",
    "    Batched version, i.e. all computations are computed without iteration through the batch samples.\n",
    "    Args:\n",
    "        write_weights_BxN: a 2-D Tensor [BATCH_SIZE x NUMBER_OF_SLOTS] \n",
    "        add_vector_BxC_: a 2-D Tensor [BATCH_SIZE x CONTENT_SIZE]\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"memory_update\"):\n",
    "        \n",
    "        # Expand dimensions of weights and erase vectors to 3D.\n",
    "        write_weights_Bx1xN = tf.expand_dims(write_weights_BxN_, axis=1)\n",
    "        print(\"write_weights_Bx1xN=\",write_weights_Bx1xN)\n",
    "        add_vector_BxCx1 = tf.expand_dims(add_vector_BxC_, axis=2)\n",
    "        print(\"add_vector_BxCx1=\",add_vector_BxCx1)\n",
    "        \n",
    "        # Calculate the content update.\n",
    "        content_update_BxCxN = tf.matmul(add_vector_BxCx1, write_weights_Bx1xN)\n",
    "        print(\"content_update_BxCxN=\",content_update_BxCxN)\n",
    "        \n",
    "        # Create the addres part - all zeros, so it won't change.        \n",
    "        address_update_BxAxN = tf.zeros([int(write_weights_Bx1xN.get_shape()[0]), ADDRESS_SIZE, NUMBER_OF_SLOTS], tf.float32)\n",
    "        print(\"address_update_BxAxN=\",address_update_BxAxN)\n",
    "\n",
    "        # Concatenate the latter two.\n",
    "        memory_update_BxSxN = tf.concat(\n",
    "            [address_update_BxAxN, content_update_BxCxN],\n",
    "            axis=1)\n",
    "        print(\"memory_update_BxSxN=\",memory_update_BxSxN)\n",
    "\n",
    "        return memory_update_BxSxN\n",
    "\n",
    "\n",
    "# Write weights: [2 samples x 4 slots]\n",
    "write_weights = tf.constant([[1.0, 0.5, 0.0, 1.0],[0.2, 0.7, 0.1, 0.0]], dtype=tf.float32)\n",
    "print(\"write_weights=\",write_weights)\n",
    "# erase_vector: [2 samples x 2 content_size]\n",
    "add_vector= tf.constant([[1.0, 1.0],[0.1, 0.1]], dtype=tf.float32)\n",
    "print(\"add_vector=\",add_vector)\n",
    "\n",
    "# Memory: [2 samples x 4 slots x 3 slot_size]\n",
    "memory_0 = tf.transpose(tf.constant([[0.2, 0.3, 0.4],[0.2, 0.3, 0.4],[0.3, 0.3, 0.3],[0.1, 0.2, 0.7]], dtype=tf.float32))\n",
    "memory_1 = tf.transpose(tf.constant([[0.1, 0.2, 0.7],[0.0, 0.6, 0.8],[0.9, 0.0, 0.1],[0.8, 0.1, 0.1]], dtype=tf.float32))\n",
    "memory = tf.stack([memory_0, memory_1])\n",
    "print(\"memory=\",memory)\n",
    "\n",
    "mem_update = build_memory_update(write_weights, add_vector)\n",
    "\n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "    \n",
    "# Execute graph.\n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(initialize_model)\n",
    "write_weights_BxN, add_vector_BxC, prev_memory_BxSxN, memory_update_BxSxN = \\\n",
    "    sess.run([write_weights, add_vector, memory, mem_update])\n",
    "print(\"write_weights_BxN =\\n\",write_weights_BxN)\n",
    "print(\"add_vector_BxC =\\n\",add_vector_BxC)\n",
    "#print(\"prev_memory_BxSxN =\\n\",prev_memory_BxSxN)\n",
    "print(\"memory_update_BxSxN =\\n\",memory_update_BxSxN)\n",
    "\n",
    "# Close session.\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_weights= Tensor(\"Const:0\", shape=(2, 4), dtype=float32)\n",
      "memory= Tensor(\"stack:0\", shape=(2, 3, 4), dtype=float32)\n",
      "read_weights_Bx1xN= Tensor(\"memory_output/ExpandDims:0\", shape=(2, 1, 4), dtype=float32)\n",
      "prev_content_BxNxC= Tensor(\"memory_output/transpose:0\", shape=(2, 4, 2), dtype=float32)\n",
      "output_Bx1xC= Tensor(\"memory_output/output_Bx1xC:0\", shape=(2, 1, 2), dtype=float32)\n",
      "output_BxC= Tensor(\"memory_output/output_BxC:0\", shape=(2, 2), dtype=float32)\n",
      "read_weights_BxN =\n",
      " [[  1.00000001e-01   9.99999978e-03   1.00000005e-03   9.99999975e-05]\n",
      " [  1.00000001e-01   1.00000001e-01   1.00000001e-01   1.00000001e-01]]\n",
      "prev_memory_BxSxN =\n",
      " [[[ 0.2         0.2         0.30000001  0.1       ]\n",
      "  [ 0.30000001  0.30000001  0.30000001  0.2       ]\n",
      "  [ 0.40000001  0.40000001  0.30000001  0.69999999]]\n",
      "\n",
      " [[ 0.1         0.          0.89999998  0.80000001]\n",
      "  [ 0.2         0.60000002  0.          0.1       ]\n",
      "  [ 0.69999999  0.80000001  0.1         0.1       ]]]\n",
      "output_BxC =\n",
      " [[ 0.03332     0.04437   ]\n",
      " [ 0.09        0.17000002]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "ADDRESS_SIZE = 1\n",
    "CONTENT_SIZE = 2\n",
    "SLOT_SIZE = ADDRESS_SIZE + CONTENT_SIZE\n",
    "NUMBER_OF_SLOTS=4\n",
    "\n",
    "def build_memory_output(read_weights_BxN_, prev_memory_BxSxN_):\n",
    "    \"\"\"Creates ops computing the memory output.\n",
    "    Assumes that memory is a 3D tensor  [BATCH_SIZE x SLOT_SIZE x NUMBER_OF_SLOTS]\n",
    "    Batched version, i.e. all computations are computed without iteration through the batch samples.\n",
    "    Args:\n",
    "        read_weights_BxN: a 2-D Tensor [BATCH_SIZE x NUMBER_OF_SLOTS] \n",
    "        prev_memory_BxSxN_: a 3-D !! Tensor [BATCH_SIZE x SLOT_SIZE x NUMBER_OF_SLOTS]\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"memory_output\"):\n",
    "        \n",
    "        # Expand dimensions of weights to 3D  [BATCH_SIZE x 1 x NUMBER_OF_SLOTS]\n",
    "        read_weights_Bx1xN = tf.expand_dims(read_weights_BxN_, axis=1)\n",
    "        print(\"read_weights_Bx1xN=\",read_weights_Bx1xN)\n",
    "        \n",
    "        # Get the content  [BATCH_SIZE x NUMBER_OF_SLOTS x CONTENT_SIZE]\n",
    "        prev_content_BxCxN = tf.slice (prev_memory_BxSxN_, [0, ADDRESS_SIZE, 0], [-1, CONTENT_SIZE, NUMBER_OF_SLOTS])\n",
    "        prev_content_BxNxC = tf.transpose(prev_content_BxCxN, perm=[0, 2, 1])\n",
    "        print(\"prev_content_BxNxC=\",prev_content_BxNxC)\n",
    "        \n",
    "        # Calculate output [BATCH_SIZE x 1 x CONTENT_SIZE]\n",
    "        output_Bx1xC = tf.matmul(read_weights_Bx1xN, prev_content_BxNxC, name=\"output_Bx1xC\")\n",
    "        print(\"output_Bx1xC=\",output_Bx1xC)\n",
    "        \n",
    "        # Squeeze the output to [BATCH_SIZE x CONTENT_SIZE] \n",
    "        output_BxC = tf.reshape(output_Bx1xC, [-1, CONTENT_SIZE], name=\"output_BxC\")\n",
    "        print(\"output_BxC=\",output_BxC)\n",
    "\n",
    "        return output_BxC\n",
    "\n",
    "\n",
    "# read_weights weights: [2 samples x 4 slots]\n",
    "read_weights = tf.constant([[0.1, 0.01, 0.001, 0.0001],[0.1, 0.1, 0.1, 0.1]], dtype=tf.float32)\n",
    "print(\"read_weights=\",read_weights)\n",
    "\n",
    "\n",
    "# Memory: [2 samples x 4 slots x 3 slot_size]\n",
    "memory_0 = tf.transpose(tf.constant([[0.2, 0.3, 0.4],[0.2, 0.3, 0.4],[0.3, 0.3, 0.3],[0.1, 0.2, 0.7]], dtype=tf.float32))\n",
    "memory_1 = tf.transpose(tf.constant([[0.1, 0.2, 0.7],[0.0, 0.6, 0.8],[0.9, 0.0, 0.1],[0.8, 0.1, 0.1]], dtype=tf.float32))\n",
    "memory = tf.stack([memory_0, memory_1])\n",
    "print(\"memory=\",memory)\n",
    "\n",
    "mem_output = build_memory_output(read_weights, memory)\n",
    "\n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "    \n",
    "# Execute graph.\n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(initialize_model)\n",
    "read_weights_BxN, prev_memory_BxSxN, output_BxC = \\\n",
    "    sess.run([read_weights, memory, mem_output])\n",
    "print(\"read_weights_BxN =\\n\",read_weights_BxN)\n",
    "print(\"prev_memory_BxSxN =\\n\",prev_memory_BxSxN)\n",
    "print(\"output_BxC =\\n\",output_BxC)\n",
    "\n",
    "# Close session.\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul:0\", shape=(2, 4, 4), dtype=int32)\n",
      "a =\n",
      " [array([[[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]],\n",
      "\n",
      "       [[13, 14, 15],\n",
      "        [16, 17, 18],\n",
      "        [19, 20, 21],\n",
      "        [22, 23, 23]]], dtype=int32)]\n",
      "b =\n",
      " [array([[[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]],\n",
      "\n",
      "       [[13, 14, 15, 16],\n",
      "        [17, 18, 19, 20],\n",
      "        [21, 22, 23, 23]]], dtype=int32)]\n",
      "c =\n",
      " [array([[[  38,   44,   50,   56],\n",
      "        [  83,   98,  113,  128],\n",
      "        [ 128,  152,  176,  200],\n",
      "        [ 173,  206,  239,  272]],\n",
      "\n",
      "       [[ 722,  764,  806,  833],\n",
      "        [ 875,  926,  977, 1010],\n",
      "        [1028, 1088, 1148, 1187],\n",
      "        [1160, 1228, 1296, 1341]]], dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# 3-D tensor `a`\n",
    "a = tf.constant(np.arange(1, 24, dtype=np.int32),\n",
    "                shape=[2, 4, 3])\n",
    "#=> [[[ 1.  2.  3.]\n",
    "#    [ 4.  5.  6.]],\n",
    "#    [[ 7.  8.  9.]\n",
    "#    [10. 11. 12.]]]\n",
    "\n",
    "# 3-D tensor `b`\n",
    "b = tf.constant(np.arange(1, 24, dtype=np.int32),\n",
    "                shape=[2, 3, 4])\n",
    "#=> [[[13. 14.]\n",
    "#    [15. 16.]\n",
    "#    [17. 18.]],\n",
    "#   [[19. 20.]\n",
    "#    [21. 22.]\n",
    "#    [23. 24.]]]\n",
    "c = tf.matmul(a, b)\n",
    "print(c)\n",
    "#=> [[[ 94 100]\n",
    "#    [229 244]],\n",
    "#    [[508 532]\n",
    "#    [697 730]]]\n",
    "\n",
    "# Execute graph.\n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "#sess.run(initialize_model)\n",
    "print(\"a =\\n\",sess.run([a]))\n",
    "print(\"b =\\n\",sess.run([b]))\n",
    "print(\"c =\\n\",sess.run([c]))\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parts= [0, 1, 2, 3, 4]\n",
      "out tensors:\n",
      " [<tf.Tensor 'DynamicPartition_12:0' shape=(?, 2) dtype=int32>, <tf.Tensor 'DynamicPartition_12:1' shape=(?, 2) dtype=int32>, <tf.Tensor 'DynamicPartition_12:2' shape=(?, 2) dtype=int32>, <tf.Tensor 'DynamicPartition_12:3' shape=(?, 2) dtype=int32>, <tf.Tensor 'DynamicPartition_12:4' shape=(?, 2) dtype=int32>, <tf.Tensor 'DynamicPartition_12:5' shape=(?, 2) dtype=int32>, <tf.Tensor 'DynamicPartition_12:6' shape=(?, 2) dtype=int32>, <tf.Tensor 'DynamicPartition_12:7' shape=(?, 2) dtype=int32>, <tf.Tensor 'DynamicPartition_12:8' shape=(?, 2) dtype=int32>, <tf.Tensor 'DynamicPartition_12:9' shape=(?, 2) dtype=int32>, <tf.Tensor 'DynamicPartition_12:10' shape=(?, 2) dtype=int32>, <tf.Tensor 'DynamicPartition_12:11' shape=(?, 2) dtype=int32>, <tf.Tensor 'DynamicPartition_12:12' shape=(?, 2) dtype=int32>, <tf.Tensor 'DynamicPartition_12:13' shape=(?, 2) dtype=int32>, <tf.Tensor 'DynamicPartition_12:14' shape=(?, 2) dtype=int32>, <tf.Tensor 'DynamicPartition_12:15' shape=(?, 2) dtype=int32>, <tf.Tensor 'DynamicPartition_12:16' shape=(?, 2) dtype=int32>, <tf.Tensor 'DynamicPartition_12:17' shape=(?, 2) dtype=int32>, <tf.Tensor 'DynamicPartition_12:18' shape=(?, 2) dtype=int32>, <tf.Tensor 'DynamicPartition_12:19' shape=(?, 2) dtype=int32>]\n",
      "input data:\n",
      " [[6 8]\n",
      " [3 9]\n",
      " [1 4]\n",
      " [6 0]\n",
      " [1 4]]\n",
      "sess.run result:\n",
      " [array([[6, 8]], dtype=int32), array([[3, 9]], dtype=int32), array([[1, 4]], dtype=int32), array([[6, 0]], dtype=int32), array([[1, 4]], dtype=int32), array([], shape=(0, 2), dtype=int32), array([], shape=(0, 2), dtype=int32), array([], shape=(0, 2), dtype=int32), array([], shape=(0, 2), dtype=int32), array([], shape=(0, 2), dtype=int32), array([], shape=(0, 2), dtype=int32), array([], shape=(0, 2), dtype=int32), array([], shape=(0, 2), dtype=int32), array([], shape=(0, 2), dtype=int32), array([], shape=(0, 2), dtype=int32), array([], shape=(0, 2), dtype=int32), array([], shape=(0, 2), dtype=int32), array([], shape=(0, 2), dtype=int32), array([], shape=(0, 2), dtype=int32), array([], shape=(0, 2), dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x = tf.placeholder(tf.int32, shape=[None, 2])\n",
    "data = np.random.randint(10, size=(5,2))\n",
    "\n",
    "parts = list(range(len(data)))\n",
    "print(\"parts=\",parts)\n",
    "out = tf.dynamic_partition(x, parts, 20)\n",
    "\n",
    "sess = tf.Session()\n",
    "print ('out tensors:\\n', out)\n",
    "print\n",
    "print ('input data:\\n', data)\n",
    "print\n",
    "print ('sess.run result:\\n', sess.run(out, {x: data}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
