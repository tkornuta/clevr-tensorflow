{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  out_dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  out_labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return out_dataset, out_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train_data', 'total_image_size', 'valid_data', 'image_size', 'num_labels', 'train_labels', 'valid_labels', 'test_data', 'test_labels'])\n",
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Define a dataset structure that will contain all sets.\n",
    "datasets = {\n",
    "    \"image_size\": 28,\n",
    "    \"num_labels\": 10,\n",
    "    \"total_image_size\": 28*28\n",
    "}\n",
    "datasets[\"train_data\"], datasets[\"train_labels\"] = reformat(train_dataset, train_labels)\n",
    "datasets[\"valid_data\"], datasets[\"valid_labels\"] = reformat(valid_dataset, valid_labels)\n",
    "datasets[\"test_data\"], datasets[\"test_labels\"] = reformat(test_dataset, test_labels)\n",
    "\n",
    "print(datasets.keys())\n",
    "print('Training set', datasets[\"train_data\"].shape, datasets[\"train_labels\"].shape)\n",
    "print('Validation set', datasets[\"valid_data\"].shape, datasets[\"valid_labels\"].shape)\n",
    "print('Test set', datasets[\"test_data\"].shape, datasets[\"test_labels\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that runs a graph.\n",
    "# graph_info_ Structure that contains the computational graph\n",
    "# dataset_ Dataset containing train, test and validation datasets (plus some additional data).\n",
    "# batch_size_ Size of batch. \n",
    "# number_of_steps_ Number of steps to be performed.\n",
    "def run_graph(graph_info_, dataset_, batch_size_, number_of_steps_):\n",
    "    with tf.Session(graph=graph_info_[\"graph\"]) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Initialized\")\n",
    "        for step in range(number_of_steps_+1):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size_) % (dataset_[\"train_labels\"].shape[0] - batch_size_)\n",
    "            \n",
    "            # Generate a minibatch.\n",
    "            batch_data = dataset_[\"train_data\"][offset:(offset + batch_size_), :]\n",
    "            batch_labels = dataset_[\"train_labels\"][offset:(offset + batch_size_), :]\n",
    "            #print('Batch set', batch_data.shape, batch_labels.shape)\n",
    "            \n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {graph_info_[\"tf_train_data\"] : batch_data, graph_info_[\"tf_train_labels\"] : batch_labels}\n",
    "            # Targets to be evaluated.\n",
    "            targets = [graph_info_[\"optimizer\"], graph_info_[\"loss\"], graph_info_[\"train\"]]\n",
    "            # Run the session and evaluate the \"targets\": returns loss and predictions.\n",
    "            _, l, predictions = session.run(targets, feed_dict=feed_dict)\n",
    "            if (step % 500 == 0):\n",
    "                print(\"Minibatch loss at step\", step, \":\", l)\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(graph_info_[\"valid\"].eval(), dataset_[\"valid_labels\"]))\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(graph_info_[\"test\"].eval(), dataset_[\"test_labels\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression model with SGD and L2 regularization\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic regression model with SGD and L2 regularization.\n",
    "def build_logistic_regression_model(datasets_, batch_size_, alpha_, l2_beta_):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Input data. For the training data, we use a placeholder that will be fed\n",
    "        # at run time with a training minibatch.\n",
    "        tf_train_data = tf.placeholder(tf.float32, shape=(batch_size_, datasets_[\"total_image_size\"]))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size_, datasets_[\"num_labels\"]))\n",
    "        # For validation and test data - simply create constants, as they won't change.\n",
    "        tf_valid_data = tf.constant(datasets_[\"valid_data\"])\n",
    "        tf_test_data = tf.constant(datasets_[\"test_data\"])\n",
    "\n",
    "        # Variables.\n",
    "        weights = tf.Variable(tf.truncated_normal([datasets_[\"total_image_size\"], datasets_[\"num_labels\"]]))\n",
    "        biases = tf.Variable(tf.zeros([datasets_[\"num_labels\"]]))\n",
    "  \n",
    "        # Graph \"leading\" to logits.\n",
    "        logits = tf.matmul(tf_train_data, weights) + biases\n",
    "        \n",
    "        # Loss fuction - built on top of the already defined logits.\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) \\\n",
    "            + l2_beta_*tf.nn.l2_loss(weights)\n",
    "     \n",
    "        info = {\n",
    "            \"graph\": graph,\n",
    "            \"batch_size\": batch_size_,\n",
    "            \"tf_train_data\": tf_train_data,\n",
    "            \"tf_train_labels\": tf_train_labels,\n",
    "            \"loss\": loss,\n",
    "            # Optimizer.\n",
    "            \"optimizer\": tf.train.GradientDescentOptimizer(alpha_).minimize(loss),\n",
    "            # Predictions for the training, validation, and test data.\n",
    "            \"train\": tf.nn.softmax(logits),\n",
    "            \"valid\": tf.nn.softmax(tf.matmul(tf_valid_data, weights) + biases),\n",
    "            \"test\": tf.nn.softmax(tf.matmul(tf_test_data, weights) + biases)\n",
    "        }\n",
    "\n",
    "    return info    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 54.1814\n",
      "Minibatch accuracy: 3.1%\n",
      "Validation accuracy: 8.0%\n",
      "Minibatch loss at step 500 : 1.11716\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 1000 : 0.738345\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 1500 : 0.847092\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 2000 : 0.802329\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 2500 : 0.774675\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 3000 : 0.789026\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.2%\n",
      "Test accuracy: 87.4%\n"
     ]
    }
   ],
   "source": [
    "# Build graph.\n",
    "logistic_regression_graph = build_logistic_regression_model(datasets_=datasets, batch_size_=128, alpha_=0.5, l2_beta_=0.01)\n",
    "# Run graph.\n",
    "run_graph(logistic_regression_graph, dataset_=datasets, batch_size_=128, number_of_steps_=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-layer neural net with SGD and L2 regularization\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2-layer neural net with SGD and L2 regularization.\n",
    "def build_nn_2layers_model(datasets_, batch_size_, alpha_, l2_beta_):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Input data. For the training data, we use a placeholder that will be fed\n",
    "        # at run time with a training minibatch.\n",
    "        tf_train_data = tf.placeholder(tf.float32, shape=(batch_size_, datasets_[\"total_image_size\"]))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size_, datasets_[\"num_labels\"]))\n",
    "        # For validation and test data - simply create constants, as they won't change.\n",
    "        tf_valid_data = tf.constant(datasets_[\"valid_data\"])\n",
    "        tf_test_data = tf.constant(datasets_[\"test_data\"])\n",
    "\n",
    "        # Input -> hidden layer.\n",
    "        weights1 = tf.Variable(tf.truncated_normal([datasets_[\"total_image_size\"], 1024]))\n",
    "        biases1 = tf.Variable(tf.zeros([1024])) \n",
    "        logits1 = tf.matmul(tf_train_data, weights1) + biases1\n",
    "        hidden1 = tf.nn.relu(logits1)\n",
    "\n",
    "        # hidden -> output layer.\n",
    "        weights2 = tf.Variable(tf.truncated_normal([1024, num_labels]))\n",
    "        biases2 = tf.Variable(tf.zeros([num_labels])) \n",
    "\n",
    "        # Graph \"leading\" to logits.\n",
    "        logits2 = tf.matmul(hidden1, weights2) + biases2  \n",
    "\n",
    "        # Loss fuction - built on top of the already defined logits.\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits2)) \\\n",
    "            + l2_beta_*tf.nn.l2_loss(weights1) \\\n",
    "            + l2_beta_*tf.nn.l2_loss(weights2)\n",
    "     \n",
    "        info = {\n",
    "            \"graph\": graph,\n",
    "            \"batch_size\": batch_size_,\n",
    "            \"tf_train_data\": tf_train_data,\n",
    "            \"tf_train_labels\": tf_train_labels,\n",
    "            \"loss\": loss,\n",
    "            # Optimizer.\n",
    "            \"optimizer\": tf.train.GradientDescentOptimizer(alpha_).minimize(loss),\n",
    "            # Predictions for the training, validation, and test data.\n",
    "            \"train\": tf.nn.softmax(logits2),\n",
    "            \"valid\": tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_data, weights1) + biases1), weights2) + biases2),\n",
    "            \"test\": tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_data, weights1) + biases1), weights2) + biases2)\n",
    "        }\n",
    "\n",
    "    return info  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 3480.83\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 27.6%\n",
      "Minibatch loss at step 500 : 21.4426\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 1000 : 0.891277\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 1500 : 0.772096\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 2000 : 0.718489\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 2500 : 0.753798\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 3000 : 0.744555\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.9%\n",
      "Test accuracy: 89.8%\n"
     ]
    }
   ],
   "source": [
    "# Build graph.\n",
    "nn_2layers_graph = build_nn_2layers_model(datasets_=datasets, batch_size_=128, alpha_=0.5, l2_beta_=0.01)\n",
    "# Run graph.\n",
    "run_graph(nn_2layers_graph, dataset_=datasets, batch_size_=128, number_of_steps_=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 784)\n",
      "(256, 10)\n",
      "Initialized\n",
      "Minibatch loss at step 0 : 3488.21\n",
      "Minibatch accuracy: 14.8%\n",
      "Validation accuracy: 22.9%\n",
      "Minibatch loss at step 500 : 21.0058\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.1%\n",
      "Minibatch loss at step 1000 : 0.341622\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.3%\n",
      "Minibatch loss at step 1500 : 0.188954\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.5%\n",
      "Minibatch loss at step 2000 : 0.181841\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.6%\n",
      "Minibatch loss at step 2500 : 0.178564\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.6%\n",
      "Minibatch loss at step 3000 : 0.176522\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.5%\n",
      "Test accuracy: 78.7%\n"
     ]
    }
   ],
   "source": [
    "# Create \"reduced\" dataset.\n",
    "slicedsets = datasets.copy() # Shallow copy\n",
    "slice_size = 2 * 128\n",
    "slicedsets[\"train_data\"] = slicedsets[\"train_data\"][:slice_size, :]\n",
    "slicedsets[\"train_labels\"] = slicedsets[\"train_labels\"][:slice_size]\n",
    "\n",
    "\n",
    "print(slicedsets[\"train_data\"].shape)\n",
    "print(slicedsets[\"train_labels\"].shape)\n",
    "\n",
    "# Build graph.\n",
    "nn_2layers_graph = build_nn_2layers_model(datasets_=slicedsets, batch_size_=128, alpha_=0.5, l2_beta_=0.01)\n",
    "# Run graph.\n",
    "run_graph(nn_2layers_graph, dataset_=slicedsets, batch_size_=128, number_of_steps_=3000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2-layer neural net with dropout, L2 regularization, optimized with Adam.\n",
    "def build_nn_2layers_dropout_model(datasets_, batch_size_, alpha_, l2_beta_, dropout_keep_prob_):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Input data. For the training data, we use a placeholder that will be fed\n",
    "        # at run time with a training minibatch.\n",
    "        tf_train_data = tf.placeholder(tf.float32, shape=(batch_size_, datasets_[\"total_image_size\"]))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size_, datasets_[\"num_labels\"]))\n",
    "        # For validation and test data - simply create constants, as they won't change.\n",
    "        tf_valid_data = tf.constant(datasets_[\"valid_data\"])\n",
    "        tf_test_data = tf.constant(datasets_[\"test_data\"])\n",
    "\n",
    "        # Input -> hidden layer.\n",
    "        weights1 = tf.Variable(tf.truncated_normal([datasets_[\"total_image_size\"], 1024]))\n",
    "        biases1 = tf.Variable(tf.zeros([1024])) \n",
    "        logits1 = tf.matmul(tf_train_data, weights1) + biases1\n",
    "        hidden1 = tf.nn.relu(logits1)\n",
    "        hidden1_dropout = tf.nn.dropout(hidden1, dropout_keep_prob_)\n",
    "\n",
    "        # hidden -> output layer.\n",
    "        weights2 = tf.Variable(tf.truncated_normal([1024, num_labels]))\n",
    "        biases2 = tf.Variable(tf.zeros([num_labels])) \n",
    "\n",
    "        # Graph \"leading\" to logits.\n",
    "        logits2 = tf.matmul(hidden1_dropout, weights2) + biases2  \n",
    "\n",
    "        # Loss fuction - built on top of the already defined logits.\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits2)) \\\n",
    "            + l2_beta_*tf.nn.l2_loss(weights1) \\\n",
    "            + l2_beta_*tf.nn.l2_loss(weights2)\n",
    "     \n",
    "        info = {\n",
    "            \"graph\": graph,\n",
    "            \"batch_size\": batch_size_,\n",
    "            \"tf_train_data\": tf_train_data,\n",
    "            \"tf_train_labels\": tf_train_labels,\n",
    "            \"loss\": loss,\n",
    "            # Optimizer.\n",
    "            \"optimizer\": tf.train.AdamOptimizer(alpha_).minimize(loss),\n",
    "            # Predictions for the training, validation, and test data.\n",
    "            \"train\": tf.nn.softmax(logits2),\n",
    "            \"valid\": tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_data, weights1) + biases1), weights2) + biases2),\n",
    "            \"test\": tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_data, weights1) + biases1), weights2) + biases2)\n",
    "        }\n",
    "\n",
    "    return info  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 3555.08\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 16.2%\n",
      "Minibatch loss at step 500 : 2403.77\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 1000 : 1659.18\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 1500 : 1092.61\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 2000 : 698.914\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 2500 : 424.604\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 3000 : 246.152\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 83.3%\n",
      "Test accuracy: 89.9%\n"
     ]
    }
   ],
   "source": [
    "# Build graph.\n",
    "nn_2layers_graph = build_nn_2layers_dropout_model(datasets_=datasets, batch_size_=128, \n",
    "          alpha_=0.001, l2_beta_=0.01, dropout_keep_prob_=0.5)\n",
    "# Run graph.\n",
    "run_graph(nn_2layers_graph, dataset_=datasets, batch_size_=128, number_of_steps_=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates a hidden layer.\n",
    "def construct_hidden_layer(input_size_, output_size_, dropout_keep_prob_, l2_beta_, train_inputs_, valid_inputs_, test_inputs_):\n",
    "    # Allocate parameteres.\n",
    "    weights = tf.Variable(tf.truncated_normal([input_size_, output_size_]))\n",
    "    biases = tf.Variable(tf.zeros([output_size_]))\n",
    "    # Train flow: input -> output .\n",
    "    train_hidden = tf.nn.relu(tf.matmul(train_inputs_, weights) + biases)\n",
    "    train_hidden_dropout = tf.nn.dropout(train_hidden, dropout_keep_prob_)\n",
    "    # Validation flow: input -> output.\n",
    "    valid_hidden = tf.nn.relu(tf.matmul(valid_inputs_, weights) + biases)\n",
    "    # Test flow: input -> output.\n",
    "    test_hidden = tf.nn.relu(tf.matmul(test_inputs_, weights) + biases)\n",
    "    # L2 regularization \"cost\".\n",
    "    cost_hidden = l2_beta_ * tf.nn.l2_loss(weights);\n",
    "    # Return created objects.\n",
    "    return [train_hidden_dropout, valid_hidden, test_hidden, cost_hidden]\n",
    "\n",
    "\n",
    "# Creates an output layer.\n",
    "def construct_output_layer(input_size_, output_size_, l2_beta_, train_inputs_, valid_inputs_, test_inputs_, train_labels_):\n",
    "    # Allocate parameteres.\n",
    "    weights = tf.Variable(tf.truncated_normal([input_size_, output_size_]))\n",
    "    biases = tf.Variable(tf.zeros([output_size_]))\n",
    "    # Train flow: input -> logits .\n",
    "    train_logits = tf.matmul(train_inputs_, weights) + biases  \n",
    "    train_output = tf.nn.softmax(train_logits)\n",
    "    # Validation flow: input -> output.\n",
    "    valid_output = tf.nn.softmax(tf.matmul(valid_inputs_, weights) + biases)\n",
    "    # Test flow: input -> output.\n",
    "    test_output = tf.nn.softmax(tf.matmul(test_inputs_, weights) + biases)\n",
    "    # Loss.\n",
    "    loss_output = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=train_labels_, logits=train_logits)) #\\\n",
    "        #+ l2_beta_ * tf.nn.l2_loss(weights);\n",
    "    # Return created objects.\n",
    "    return [train_output, valid_output, test_output, loss_output]\n",
    "\n",
    "# Builds a deep neural net with dropout, L2 regularization, optimized with Adam.\n",
    "def build_dnn_model(datasets_, batch_size_, alpha_, l2_beta_, dropout_keep_prob_):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Input data. For the training data, we use a placeholder that will be fed\n",
    "        # at run time with a training minibatch.\n",
    "        tf_train_data = tf.placeholder(tf.float32, shape=(batch_size_, datasets_[\"total_image_size\"]))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size_, datasets_[\"num_labels\"]))\n",
    "        # For validation and test data - simply create constants, as they won't change.\n",
    "        tf_valid_data = tf.constant(datasets_[\"valid_data\"])\n",
    "        tf_test_data = tf.constant(datasets_[\"test_data\"])\n",
    "\n",
    "        # Input -> hidden layer1.\n",
    "        [train_hidden1, valid_hidden1, test_hidden1, loss_hidden1] = construct_hidden_layer(\n",
    "            input_size_=datasets_[\"total_image_size\"], output_size_=4096, dropout_keep_prob_=dropout_keep_prob_, \n",
    "            l2_beta_=l2_beta_, train_inputs_=tf_train_data, valid_inputs_=tf_valid_data, test_inputs_=tf_test_data)\n",
    "\n",
    "        # Hidden layer1 -> hidden layer2.\n",
    "        [train_hidden2, valid_hidden2, test_hidden2, loss_hidden2] = construct_hidden_layer(\n",
    "            input_size_=4096, output_size_=2048, dropout_keep_prob_=dropout_keep_prob_, \n",
    "            l2_beta_=l2_beta_, train_inputs_=train_hidden1, valid_inputs_=valid_hidden1, test_inputs_=test_hidden1)\n",
    "\n",
    "        # hidden layer2 -> hidden layer3.\n",
    "        [train_hidden3, valid_hidden3, test_hidden3, loss_hidden3] = construct_hidden_layer(\n",
    "            input_size_=2048, output_size_=1024, dropout_keep_prob_=dropout_keep_prob_, \n",
    "            l2_beta_=l2_beta_, train_inputs_=train_hidden2, valid_inputs_=valid_hidden2, test_inputs_=test_hidden2)\n",
    "\n",
    "        # hidden layer3 -> hidden layer4.\n",
    "        #[train_hidden4, valid_hidden4, test_hidden4, loss_hidden4] = construct_hidden_layer(\n",
    "        #    input_size_=256, output_size_=128, dropout_keep_prob_=dropout_keep_prob_, \n",
    "        #    l2_beta_=l2_beta_, train_inputs_=train_hidden3, valid_inputs_=valid_hidden3, test_inputs_=test_hidden3)\n",
    "\n",
    "        # Hidden layer4 -> output layer.\n",
    "        [train_output, valid_output, test_output, loss_output] = construct_output_layer(\n",
    "            input_size_=1024, output_size_=datasets_[\"num_labels\"], l2_beta_=l2_beta_, \n",
    "            train_inputs_=train_hidden3, valid_inputs_=valid_hidden3, test_inputs_=test_hidden3,\n",
    "            train_labels_=tf_train_labels)\n",
    "\n",
    "        # Loss fuction - sum losses.\n",
    "        #loss = loss_hidden1 + loss_hidden2 + loss_hidden3 + loss_hidden4 + loss_output\n",
    "        loss = loss_output\n",
    "\n",
    "        # Learning rate decay.\n",
    "        global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "        decay_rate=0.99\n",
    "        decay_steps=20000\n",
    "        learning_rate = tf.train.exponential_decay(alpha_, global_step, decay_steps, decay_rate, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(alpha_).minimize(loss, global_step=global_step)\n",
    "        \n",
    "        info = {\n",
    "            \"graph\": graph,\n",
    "            \"tf_train_data\": tf_train_data,\n",
    "            \"tf_train_labels\": tf_train_labels,\n",
    "            \"loss\": loss,\n",
    "            # Optimizer.\n",
    "            \"optimizer\": optimizer, #tf.train.AdamOptimizer(alpha_).minimize(loss),\n",
    "            # Predictions for the training, validation, and test data.\n",
    "            \"train\": train_output,\n",
    "            \"valid\": valid_output,\n",
    "            \"test\": test_output\n",
    "        }\n",
    "        \n",
    "    return info  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 1.01303e+06\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 12.5%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1b7f62026c4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdnn_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0malpha_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_beta_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Run graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrun_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnn_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_steps_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-b1efe622a9bf>\u001b[0m in \u001b[0;36mrun_graph\u001b[0;34m(graph_info_, dataset_, batch_size_, number_of_steps_)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgraph_info_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_info_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_info_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# Run the session and evaluate the \"targets\": returns loss and predictions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Minibatch loss at step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\":\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/tensorflow/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Build graph.\n",
    "dnn_graph = build_dnn_model(datasets_=datasets, batch_size_=256, \\\n",
    "          alpha_=0.001, l2_beta_=0.01, dropout_keep_prob_=0.5)\n",
    "# Run graph.\n",
    "run_graph(dnn_graph, dataset_=datasets, batch_size_=256, number_of_steps_=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
