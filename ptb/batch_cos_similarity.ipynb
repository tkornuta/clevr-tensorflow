{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to perform operation on arrays.\n",
    "elems = np.array([1, 2, 3, 4, 5, 6])\n",
    "squares = tf.map_fn(lambda x: x * x, elems)\n",
    "# squares == [1, 4, 9, 16, 25, 36]\n",
    "\n",
    "sess=tf.Session()\n",
    "print(sess.run(squares))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elems = (np.array([1, 2, 3]), np.array([-1, 1, -1]))\n",
    "alternate = tf.map_fn(lambda x: x[0] * x[1], elems, dtype=tf.int64)\n",
    "# alternate == [-1, 2, -3]\n",
    "\n",
    "sess=tf.Session()\n",
    "print(sess.run(alternate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Broadcasting ability of TF\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.constant([[0, 1,2],[2, 3,2],[4, 5,2],[6, 7,2]], dtype=tf.float32)\n",
    "y = tf.constant([[0, 1,2],[-2, -1,2]], dtype=tf.float32)\n",
    "\n",
    "x_vect_len = int(x.shape[1])\n",
    "print(\"len=\",x_vect_len)\n",
    "x_ = tf.expand_dims(x, 0)\n",
    "y_ = tf.expand_dims(y, 1)\n",
    "z = tf.reshape(tf.add(x_, y_), [-1, x_vect_len])\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(z)\n",
    "print(x_.eval())\n",
    "print(y_.eval())\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# place holders \n",
    "batch = tf.placeholder(tf.float32, shape=[3,3], name=\"batch\")\n",
    "memory = tf.placeholder(tf.float32, shape=[2,3], name=\"memory\")\n",
    "# Cos similarity\n",
    "norm_batch = tf.nn.l2_normalize(batch,0) \n",
    "norm_memory = tf.nn.l2_normalize(memory,0)\n",
    "\n",
    "nb_ = tf.expand_dims(norm_batch, 0)\n",
    "nm_ = tf.expand_dims(norm_memory, 1)\n",
    "cos_similarity = tf.reshape(tf.add(nb_,nm_), [-1, 2])\n",
    "#cos_similarity = tf.reshape(tf.reduce_sum(tf.multiply(nb_,nm_)), [-1, 2])\n",
    "\n",
    "                            \n",
    "sess=tf.InteractiveSession()\n",
    "cos_sim=sess.run(cos_similarity,feed_dict={batch:[[1,2,3],[3,3,3],[1,2,6]],memory:[[2,4,6],[1,2,4]]})\n",
    "\n",
    "print(cos_sim)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Size of the hidden state 64\n",
    "HIDDEN_SIZE = 3\n",
    "\n",
    "# Size of the MANN memory.\n",
    "MEMORY_SIZE = 5\n",
    "\n",
    "# A batch size of 100\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# A single recurrent layer of number of units = sequences of length\n",
    "# e.g. 200 bytes\n",
    "SEQ_LENGTH = 10\n",
    "\n",
    "EPS = 1e-15\n",
    "\n",
    "# place holders \n",
    "batch = tf.placeholder(tf.float32, shape=None, name=\"batch\")\n",
    "#memory = tf.placeholder(tf.float32, shape=[None], name=\"memory\")\n",
    "memory = tf.Variable(tf.zeros([MEMORY_SIZE, HIDDEN_SIZE]), trainable=False, name=\"memory\")\n",
    "\n",
    "memory_set = memory.assign([[1, 0, 1],[0.1, 0.2 , 0.4],[ 0,0,0],[-0.3,0.2,0.3],[0, 1, 0]])\n",
    "\n",
    "# Create BATCH_SIZE placeholders for similarity - each MEMORY_SIZE x 1,  \n",
    "# 0. Placeholders for inputs.\n",
    "with tf.name_scope(\"similarity\"):\n",
    "  # Define similarity buffers.\n",
    "  similarity = list()\n",
    "  #for b in range(BATCH_SIZE):\n",
    "    # Collect placeholders for similarity.\n",
    "    #similarity.append(tf.placeholder(tf.float32, shape=[MEMORY_SIZE, 1], name=\"Similarity\"))\n",
    " \n",
    "  # Normalize\n",
    "  norm_batch = tf.nn.l2_normalize(batch,1) \n",
    "  norm_memory = tf.nn.l2_normalize(memory,1)\n",
    "  print(norm_batch[0])\n",
    "  # Define similarity buffers.\n",
    "  numerator_batch = list()\n",
    "  denominator_batch = list()\n",
    "  similarity_batch = list()\n",
    "\n",
    "  for b in range(BATCH_SIZE):\n",
    "    similarity_batch.append(tf.map_fn(lambda x: tf.reduce_sum(tf.multiply(norm_batch[b],x)), norm_memory))\n",
    "    # Read weight based on similarity.\n",
    "  read_weight = tf.nn.softmax(similarity_batch)\n",
    "    \n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(memory_set)\n",
    "print(\"memory =\",memory.eval())\n",
    "\n",
    "tmp_batch = [[1, 0, 1],[-0.3,0.3,0.3]]\n",
    "\n",
    "print(\"Batch=\",batch.eval(feed_dict={batch:tmp_batch}))\n",
    "print(\"Norm batch=\",norm_batch.eval(feed_dict={batch:tmp_batch}))\n",
    "\n",
    "\n",
    "num, den, sim, rw=sess.run([numerator_batch, denominator_batch, similarity_batch, read_weight],\n",
    "                           feed_dict={batch:tmp_batch})\n",
    "print(\"norm memory =\",norm_memory.eval())\n",
    "\n",
    "print(\"numerator_batch=\",num)\n",
    "print(\"denominator_batch=\\n\",den,\"\\n\")\n",
    "\n",
    "print(\"similarity=\",sim)\n",
    "print(\"rw=\", rw)\n",
    "\n",
    "\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory =\n",
      " [[ 1.          0.1         0.         -0.30000001  0.        ]\n",
      " [ 0.          0.2         0.30000001  0.2         1.        ]\n",
      " [ 1.          0.40000001  0.          0.30000001  0.        ]]\n",
      "\n",
      "Batch=\n",
      " [[ 1.          0.          1.        ]\n",
      " [-0.30000001  0.30000001 -0.30000001]]\n",
      "\n",
      "=====i= 0\n",
      "Memory after update=\n",
      " [[ 1.70000005  0.80000001  0.69999999  0.39999998  0.69999999]\n",
      " [ 0.30000001  0.5         0.60000002  0.5         1.29999995]\n",
      " [ 1.70000005  1.10000002  0.69999999  1.          0.69999999]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.45834252  0.17094871  0.12408432  0.12254006  0.12408432]\n",
      " [ 0.06612217  0.16460168  0.22554328  0.21601786  0.32771501]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]]\n",
      "\n",
      "=====i= 1\n",
      "Memory after update=\n",
      " [[ 2.83850598  1.6215682   1.45642138  1.15773475  1.42576981]\n",
      " [ 0.61983669  0.84938049  0.96766305  0.86480534  1.69831443]\n",
      " [ 2.83850598  1.92156827  1.45642138  1.75773478  1.42576981]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.3082411   0.19331703  0.16681786  0.16480623  0.16681786]\n",
      " [ 0.11691221  0.18372267  0.21472025  0.20928739  0.27535748]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 1.45834255  1.17094874  1.12408435  1.12254012  1.12408435]\n",
      " [ 1.06612217  1.16460168  1.22554326  1.21601784  1.32771504]]\n",
      "\n",
      "=====i= 2\n",
      "Memory after update=\n",
      " [[ 3.8116734   2.4597683   2.25882316  1.95975471  2.20998049]\n",
      " [ 0.9549104   1.20449734  1.33207917  1.22759151  2.08092165]\n",
      " [ 3.8116734   2.75976849  2.25882316  2.55975485  2.20998049]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.28032011  0.19504654  0.1756929   0.17505865  0.17388187]\n",
      " [ 0.13348469  0.18976013  0.21267265  0.20804965  0.25603285]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 1.30824113  1.19331706  1.1668179   1.16480625  1.1668179 ]\n",
      " [ 1.11691225  1.18372273  1.21472025  1.20928741  1.27535748]]\n",
      "\n",
      "=====i= 3\n",
      "Memory after update=\n",
      " [[ 4.75194836  3.29788685  3.07071424  2.77239847  3.00705242]\n",
      " [ 1.29495585  1.56142545  1.69588101  1.59000635  2.45773149]\n",
      " [ 4.75194836  3.59788704  3.07071424  3.37239861  3.00705242]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.26110452  0.19683355  0.18148671  0.1811531   0.17942217]\n",
      " [ 0.14607492  0.1925696   0.2106211   0.20694679  0.24378762]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 1.28032017  1.19504654  1.17569292  1.1750586   1.17388189]\n",
      " [ 1.13348472  1.18976009  1.21267271  1.20804965  1.25603282]]\n",
      "\n",
      "=====i= 4\n",
      "Memory after update=\n",
      " [[ 5.66923046  4.13694954  3.88901472  3.59146738  3.81333828]\n",
      " [ 1.63877833  1.91919637  2.05906725  1.95209038  2.83086777]\n",
      " [ 5.66923046  4.43694973  3.88901472  4.19146776  3.81333828]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.24984834  0.19770151  0.18490621  0.18473326  0.18281062]\n",
      " [ 0.15430407  0.1942344   0.20921806  0.20615837  0.23608516]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 1.26110458  1.19683361  1.18148673  1.18115306  1.17942214]\n",
      " [ 1.14607489  1.19256961  1.21062112  1.20694685  1.24378765]]\n",
      "\n",
      "=====i= 5\n",
      "Memory after update=\n",
      " [[ 6.57278776  4.97638083  4.71115541  4.41435337  4.6253233 ]\n",
      " [ 1.98506951  2.27746677  2.42183256  2.3139379   3.2016933 ]\n",
      " [ 6.57278776  5.27638102  4.71115541  5.01435375  4.6253233 ]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.24438407  0.19811215  0.18662569  0.18655826  0.18431993]\n",
      " [ 0.1586715   0.19516931  0.20856136  0.20589703  0.23170078]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 1.24984837  1.19770145  1.18490624  1.18473327  1.18281066]\n",
      " [ 1.15430403  1.19423437  1.20921803  1.2061584   1.23608518]]\n",
      "\n",
      "=====i= 6\n",
      "Memory after update=\n",
      " [[ 7.46957016  5.81594229  5.53521252  5.23914242  5.44013309]\n",
      " [ 2.33267093  2.63601756  2.78440094  2.6757071   3.57120347]\n",
      " [ 7.46957016  6.11594248  5.53521252  5.8391428   5.44013309]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.23683202  0.19858207  0.1888614   0.18882374  0.18690078]\n",
      " [ 0.16468211  0.1960482   0.20734993  0.20506135  0.22685839]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 1.24438405  1.19811213  1.18662572  1.18655825  1.18431997]\n",
      " [ 1.1586715   1.19516933  1.20856142  1.20589709  1.23170078]]\n",
      "\n",
      "=====i= 7\n",
      "Memory after update=\n",
      " [[ 8.35699749  6.65570974  6.36186886  6.06644773  6.25897646]\n",
      " [ 2.6820755   2.99483204  3.14660597  3.03722548  3.93926096]\n",
      " [ 8.35699749  6.95570993  6.36186886  6.66644812  6.25897646]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.23278633  0.19882216  0.19009356  0.19008808  0.18820989]\n",
      " [ 0.16812037  0.19659042  0.20670955  0.20467487  0.22390476]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 1.23683202  1.19858205  1.18886137  1.1888237   1.18690073]\n",
      " [ 1.16468215  1.19604826  1.2073499   1.20506132  1.22685838]]\n",
      "\n",
      "=====i= 8\n",
      "Memory after update=\n",
      " [[ 9.23934746  7.49555492  7.18994951  6.8951335   7.08001518]\n",
      " [ 3.03251171  3.35380912  3.50861883  3.398628    4.30643225]\n",
      " [ 9.23934746  7.79555511  7.18994951  7.49513388  7.08001518]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.2295654   0.19900148  0.19107266  0.19108778  0.18927267]\n",
      " [ 0.17092742  0.19700511  0.20617405  0.20434228  0.22155115]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 1.2327863   1.19882214  1.19009352  1.19008803  1.18820989]\n",
      " [ 1.16812038  1.19659042  1.2067095   1.20467484  1.22390473]]\n",
      "\n",
      "=====i= 9\n",
      "Memory after update=\n",
      " [[ 10.11763477   8.33545494   8.01916981   7.72491837   7.90282249]\n",
      " [  3.38379002   3.71291065   3.870471     3.75993061   4.67289782]\n",
      " [ 10.11763477   8.63545513   8.01916981   8.32491875   7.90282249]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.22483616  0.19924374  0.19251031  0.19254918  0.19086064]\n",
      " [ 0.17517303  0.19759345  0.20534755  0.20381799  0.21806794]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 1.22956538  1.19900143  1.1910727   1.19108772  1.18927264]\n",
      " [ 1.17092741  1.19700515  1.20617402  1.20434225  1.22155118]]\n"
     ]
    }
   ],
   "source": [
    "# Different approach - change memory \"orientation\"\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Size of the hidden state 64\n",
    "HIDDEN_SIZE = 3\n",
    "\n",
    "# Size of the MANN memory.\n",
    "MEMORY_SIZE = 5\n",
    "\n",
    "# A batch size of 100\n",
    "#BATCH_SIZE = 2\n",
    "\n",
    "# A single recurrent layer of number of units = sequences of length\n",
    "# e.g. 200 bytes\n",
    "SEQ_LENGTH = 10\n",
    "\n",
    "# \"Read decay\".\n",
    "GAMMA = 0.9\n",
    "\n",
    "#EPS = 1e-15\n",
    "\n",
    "# place holders \n",
    "batch = tf.placeholder(tf.float32, shape=None, name=\"Batch_h\")\n",
    "#memory = tf.placeholder(tf.float32, shape=[None], name=\"memory\")\n",
    "memory = tf.Variable(tf.zeros([HIDDEN_SIZE, MEMORY_SIZE]), trainable=False, name=\"Memory_M\")\n",
    "#alpha = tf.Variable(tf.truncated_normal(shape=[1]), name=\"Alpha\")\n",
    "alpha = tf.Variable(tf.truncated_normal(shape=[1]), name=\"Alpha\")\n",
    "\n",
    "# SET INITIAL MEMORY STATE.\n",
    "memory_set = memory.assign(tf.transpose([[1, 0, 1],\n",
    "                            [0.1, 0.2, 0.4],\n",
    "                            [ 0, 0.3, 0],\n",
    "                            [-0.3, 0.2, 0.3],\n",
    "                            [0, 1, 0]]))\n",
    "alpha_set = alpha.assign([100])\n",
    "\n",
    "# Placeholders for previous weights.\n",
    "prev_update_weights = tf.placeholder(tf.float32, shape=None, name=\"Prev_uw\")\n",
    "prev_read_weights = tf.placeholder(tf.float32, shape=None, name=\"Prev_rw\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Read_head\"):\n",
    "    # Normalize batches and memory.\n",
    "    norm_batch = tf.nn.l2_normalize(batch,1, name=\"NormalizedBatch_h\") \n",
    "    norm_memory = tf.nn.l2_normalize(memory,1, name=\"NormalizedMemory_h\")\n",
    "\n",
    "    # calculate similarity.\n",
    "    similarity = tf.tensordot(norm_batch, norm_memory, axes=1, name= \"Similarity_D\") \n",
    "    # Read weights based on similarity.\n",
    "    read_weights = tf.nn.softmax(similarity, name=\"Read_weights_rw\")\n",
    "    # Read \"vector\" (in fact batch).\n",
    "    r = tf.tensordot(read_weights, tf.transpose(memory), axes=1, name=\"Read_vector_r\")\n",
    "\n",
    "# TODO: add dependencies, that write will be done after read.\n",
    "with tf.name_scope(\"Write_head\"):\n",
    "    write_weights = tf.add(tf.sigmoid(alpha) * prev_read_weights, 1,# (1 - tf.sigmoid(alpha)) * read_weights, \n",
    "                           name=\"Write_weights_ww\")\n",
    "    \n",
    "    calculate_mem_update = tf.tensordot(tf.transpose(batch), write_weights, axes=1)\n",
    "    memory_update_op = memory.assign(memory + calculate_mem_update)\n",
    "\n",
    "with tf.name_scope(\"Update_head\"): # This relies on prev. weights and will be used in fact in NEXT step.\n",
    "    update_weights = tf.add(GAMMA * prev_update_weights, read_weights, name=\"Update_weights_uw\")\n",
    "\n",
    "    \n",
    "    \n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "    \n",
    "# Execute graph.\n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(initialize_model)\n",
    "sess.run([memory_set, alpha_set])\n",
    "print(\"Memory =\\n\",memory.eval())\n",
    "\n",
    "tmp_batch = [[1, 0, 1],[-0.3, 0.3, -0.3]]\n",
    "# Prev UW [batch size x memory size]\n",
    "prev_uw = [[0,0,0,0,0], [0,0,0,0,0]]\n",
    "prev_rw = [[0,0,0,0,0], [0,0,0,0,0]]\n",
    "\n",
    "print(\"\\nBatch=\\n\",batch.eval(feed_dict={batch:tmp_batch}))\n",
    "#print(\"Norm batch=\",norm_batch.eval(feed_dict={batch:tmp_batch}))\n",
    "\n",
    "for i in range(10):\n",
    "    sim, r_vect, prev_rw, prev_uw, prev_ww, mu_op = sess.run(\n",
    "        [similarity, r, read_weights, update_weights, write_weights, memory_update_op],\n",
    "                               feed_dict={\n",
    "                                   batch:tmp_batch,\n",
    "                                   prev_update_weights: prev_uw,\n",
    "                                   prev_read_weights: prev_rw\n",
    "                               })\n",
    "    #print(\"Norm memory =\\n\",norm_memory.eval())\n",
    "    print (\"\\n=====i=\", i)\n",
    "    print(\"Memory after update=\\n\", mu_op)\n",
    "    #print(\"\\nSimilarity=\\n\",sim)\n",
    "    print(\"\\nprev_rw=\\n\", prev_rw)\n",
    "    #print(\"\\nr_vect=\\n\", r_vect)\n",
    "    #print(\"\\nprev_uw=\\n\", prev_uw)\n",
    "    print(\"\\nprev_ww=\\n\", prev_ww)\n",
    "\n",
    "\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
