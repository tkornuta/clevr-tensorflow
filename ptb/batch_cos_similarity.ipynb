{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How to perform operation on arrays.\n",
    "elems = np.array([1, 2, 3, 4, 5, 6])\n",
    "squares = tf.map_fn(lambda x: x * x, elems)\n",
    "# squares == [1, 4, 9, 16, 25, 36]\n",
    "\n",
    "sess=tf.Session()\n",
    "print(sess.run(squares))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "elems = (np.array([1, 2, 3]), np.array([-1, 1, -1]))\n",
    "alternate = tf.map_fn(lambda x: x[0] * x[1], elems, dtype=tf.int64)\n",
    "# alternate == [-1, 2, -3]\n",
    "\n",
    "sess=tf.Session()\n",
    "print(sess.run(alternate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"Broadcasting ability of TF\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.constant([[0, 1,2],[2, 3,2],[4, 5,2],[6, 7,2]], dtype=tf.float32)\n",
    "y = tf.constant([[0, 1,2],[-2, -1,2]], dtype=tf.float32)\n",
    "\n",
    "x_vect_len = int(x.shape[1])\n",
    "print(\"len=\",x_vect_len)\n",
    "x_ = tf.expand_dims(x, 0)\n",
    "y_ = tf.expand_dims(y, 1)\n",
    "z = tf.reshape(tf.add(x_, y_), [-1, x_vect_len])\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(z)\n",
    "print(x_.eval())\n",
    "print(y_.eval())\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# place holders \n",
    "batch = tf.placeholder(tf.float32, shape=[3,3], name=\"batch\")\n",
    "memory = tf.placeholder(tf.float32, shape=[2,3], name=\"memory\")\n",
    "# Cos similarity\n",
    "norm_batch = tf.nn.l2_normalize(batch,0) \n",
    "norm_memory = tf.nn.l2_normalize(memory,0)\n",
    "\n",
    "nb_ = tf.expand_dims(norm_batch, 0)\n",
    "nm_ = tf.expand_dims(norm_memory, 1)\n",
    "cos_similarity = tf.reshape(tf.add(nb_,nm_), [-1, 2])\n",
    "#cos_similarity = tf.reshape(tf.reduce_sum(tf.multiply(nb_,nm_)), [-1, 2])\n",
    "\n",
    "                            \n",
    "sess=tf.InteractiveSession()\n",
    "cos_sim=sess.run(cos_similarity,feed_dict={batch:[[1,2,3],[3,3,3],[1,2,6]],memory:[[2,4,6],[1,2,4]]})\n",
    "\n",
    "print(cos_sim)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Size of the hidden state 64\n",
    "HIDDEN_SIZE = 3\n",
    "\n",
    "# Size of the MANN memory.\n",
    "MEMORY_SIZE = 5\n",
    "\n",
    "# A batch size of 100\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# A single recurrent layer of number of units = sequences of length\n",
    "# e.g. 200 bytes\n",
    "SEQ_LENGTH = 10\n",
    "\n",
    "EPS = 1e-15\n",
    "\n",
    "# place holders \n",
    "batch = tf.placeholder(tf.float32, shape=None, name=\"batch\")\n",
    "#memory = tf.placeholder(tf.float32, shape=[None], name=\"memory\")\n",
    "memory = tf.Variable(tf.zeros([MEMORY_SIZE, HIDDEN_SIZE]), trainable=False, name=\"memory\")\n",
    "\n",
    "memory_set = memory.assign([[1, 0, 1],[0.1, 0.2 , 0.4],[ 0,0,0],[-0.3,0.2,0.3],[0, 1, 0]])\n",
    "\n",
    "# Create BATCH_SIZE placeholders for similarity - each MEMORY_SIZE x 1,  \n",
    "# 0. Placeholders for inputs.\n",
    "with tf.name_scope(\"similarity\"):\n",
    "  # Define similarity buffers.\n",
    "  similarity = list()\n",
    "  #for b in range(BATCH_SIZE):\n",
    "    # Collect placeholders for similarity.\n",
    "    #similarity.append(tf.placeholder(tf.float32, shape=[MEMORY_SIZE, 1], name=\"Similarity\"))\n",
    " \n",
    "  # Normalize\n",
    "  norm_batch = tf.nn.l2_normalize(batch,1) \n",
    "  norm_memory = tf.nn.l2_normalize(memory,1)\n",
    "  print(norm_batch[0])\n",
    "  # Define similarity buffers.\n",
    "  numerator_batch = list()\n",
    "  denominator_batch = list()\n",
    "  similarity_batch = list()\n",
    "\n",
    "  for b in range(BATCH_SIZE):\n",
    "    similarity_batch.append(tf.map_fn(lambda x: tf.reduce_sum(tf.multiply(norm_batch[b],x)), norm_memory))\n",
    "    # Read weight based on similarity.\n",
    "  read_weight = tf.nn.softmax(similarity_batch)\n",
    "    \n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(memory_set)\n",
    "print(\"memory =\",memory.eval())\n",
    "\n",
    "tmp_batch = [[1, 0, 1],[-0.3,0.3,0.3]]\n",
    "\n",
    "print(\"Batch=\",batch.eval(feed_dict={batch:tmp_batch}))\n",
    "print(\"Norm batch=\",norm_batch.eval(feed_dict={batch:tmp_batch}))\n",
    "\n",
    "\n",
    "num, den, sim, rw=sess.run([numerator_batch, denominator_batch, similarity_batch, read_weight],\n",
    "                           feed_dict={batch:tmp_batch})\n",
    "print(\"norm memory =\",norm_memory.eval())\n",
    "\n",
    "print(\"numerator_batch=\",num)\n",
    "print(\"denominator_batch=\\n\",den,\"\\n\")\n",
    "\n",
    "print(\"similarity=\",sim)\n",
    "print(\"rw=\", rw)\n",
    "\n",
    "\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def rest(x): return tf.gather(x, tf.range(1, tf.size(x)))\n",
    "\n",
    "x = tf.Variable([0., 1.])\n",
    "x_op = tf.assign(x, [0., 1., 2.], validate_shape=False)\n",
    "\n",
    "with tf.control_dependencies([x_op]):\n",
    "    true_fun  = lambda: tf.assign(x, rest(x_op), validate_shape=False)\n",
    "    false_fun = lambda: tf.constant([])\n",
    "    pred = tf.constant(True)\n",
    "    cond_op = tf.cond(pred, true_fun, false_fun)\n",
    "\n",
    "with tf.Session(\"\"):\n",
    "  x.initializer.run()\n",
    "  print(cond_op.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds given top elements\n",
    "import tensorflow as tf\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tmp_batch = [[1, 0, 1, 2, 3, 4, 5, 6, 1],[-0.2, 0.1, -0.4, 0.5, 0.1, 0.9, 0.8, 0.3, -0.3]]\n",
    "\n",
    "\n",
    "top_k = tf.nn.top_k(tmp_batch, 3)\n",
    "ones = tf.where\n",
    "\n",
    "\n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "    \n",
    "# Execute graph.\n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(initialize_model)\n",
    "print(sess.run([top_k, top_k]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds n smallest elements\n",
    "\n",
    "import tensorflow as tf\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "batch = tf.convert_to_tensor([[40, 30, 20, 10], [10, 20, 30, 40]])\n",
    "\n",
    "# Number of smallest elements.\n",
    "n =3\n",
    "\n",
    "# You can do it using built-in tf.nn.top_k function - find size-n top elements in each sample.\n",
    "k = batch.shape[1] - n\n",
    "top = tf.nn.top_k(batch, k)\n",
    "\n",
    "# To get boolean True/False values, you can first get the k-th value and then use tf.greater_equal:\n",
    "kth = tf.reduce_min(top.values)\n",
    "top2 = tf.greater_equal(batch, kth)\n",
    "# And finally - cast it to n smallest elements.\n",
    "smallest = tf.cast(top2, tf.int32) * -1 + 1\n",
    "\n",
    "# Execute graph.\n",
    "sess=tf.InteractiveSession()\n",
    "\n",
    "print(sess.run(top))\n",
    "\n",
    "print(sess.run(top).values)\n",
    "\n",
    "print(sess.run(top2))\n",
    "\n",
    "print(sess.run(smallest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unknown>\n",
      "Memory =\n",
      " [[ 1.          0.1         0.         -0.30000001  0.        ]\n",
      " [ 0.          0.2         0.30000001  0.2         1.        ]\n",
      " [ 1.          0.40000001  0.          0.30000001  0.        ]]\n",
      "\n",
      "Batch=\n",
      " [[ 0.1  0.   0. ]\n",
      " [ 0.   0.   0. ]]\n",
      "\n",
      "=====i= 0\n",
      "Memory after update=\n",
      " [[ 1.          0.1         0.         -0.30000001  0.        ]\n",
      " [ 0.          0.2         0.30000001  0.2         1.        ]\n",
      " [ 1.          0.40000001  0.          0.30000001  0.        ]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.40252841  0.17065592  0.15513614  0.11654346  0.15513614]\n",
      " [ 0.2         0.2         0.2         0.2         0.2       ]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "\n",
      " readed vector=\n",
      " [[ 0.38463095  0.25911686  0.50575382]\n",
      " [ 0.16        0.34000003  0.34      ]]\n",
      "\n",
      "=====i= 1\n",
      "Memory after update=\n",
      " [[ 1.02113187  0.10895908  0.00814432 -0.24637964  0.00814432]\n",
      " [ 0.          0.2         0.30000001  0.2         1.        ]\n",
      " [ 1.          0.40000001  0.          0.30000001  0.        ]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.40252841  0.17065592  0.15513614  0.11654346  0.15513614]\n",
      " [ 0.2         0.2         0.2         0.2         0.2       ]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 0.21131903  0.0895908   0.08144324  0.53620374  0.08144324]\n",
      " [ 0.10499584  0.10499584  0.10499584  0.10499584  0.10499584]]\n",
      "\n",
      " readed vector=\n",
      " [[ 0.38463095  0.25911686  0.50575382]\n",
      " [ 0.16        0.34000003  0.34      ]]\n",
      "\n",
      "=====i= 2\n",
      "Memory after update=\n",
      " [[ 1.04226375  0.11791816  0.06379073 -0.24026136  0.06379073]\n",
      " [ 0.          0.2         0.30000001  0.2         1.        ]\n",
      " [ 1.          0.40000001  0.          0.30000001  0.        ]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.40173659  0.16937338  0.15395328  0.12098338  0.15395328]\n",
      " [ 0.2         0.2         0.2         0.2         0.2       ]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 0.21131903  0.0895908   0.55646408  0.06118289  0.55646408]\n",
      " [ 0.10499584  0.10499584  0.10499584  0.10499584  0.10499584]]\n",
      "\n",
      " readed vector=\n",
      " [[ 0.40138066  0.25821063  0.50578094]\n",
      " [ 0.17999998  0.34000003  0.34      ]]\n",
      "\n",
      "=====i= 3\n",
      "Memory after update=\n",
      " [[ 1.06335413  0.174312    0.07187296 -0.18640789  0.07187296]\n",
      " [ 0.          0.2         0.30000001  0.2         1.        ]\n",
      " [ 1.          0.40000001  0.          0.30000001  0.        ]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.39401591  0.16740218  0.159218    0.12014583  0.159218  ]\n",
      " [ 0.2         0.2         0.2         0.2         0.2       ]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 0.21090335  0.56393832  0.08082227  0.53853458  0.08082227]\n",
      " [ 0.10499584  0.10499584  0.10499584  0.10499584  0.10499584]]\n",
      "\n",
      " readed vector=\n",
      " [[ 0.42185509  0.26449299  0.49702054]\n",
      " [ 0.20950042  0.34000003  0.34      ]]\n",
      "\n",
      "=====i= 4\n",
      "Memory after update=\n",
      " [[ 1.08403909  0.18310027  0.12773365 -0.18010049  0.12773365]\n",
      " [ 0.          0.2         0.30000001  0.2         1.        ]\n",
      " [ 1.          0.40000001  0.          0.30000001  0.        ]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.38813102  0.17275012  0.15736572  0.12438741  0.15736572]\n",
      " [ 0.2         0.2         0.2         0.2         0.2       ]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 0.20685014  0.08788266  0.55860698  0.06307406  0.55860698]\n",
      " [ 0.10499584  0.10499584  0.10499584  0.10499584  0.10499584]]\n",
      "\n",
      " readed vector=\n",
      " [[ 0.44226703  0.26400295  0.49454731]\n",
      " [ 0.23900083  0.34000003  0.34      ]]\n",
      "\n",
      "=====i= 5\n",
      "Memory after update=\n",
      " [[ 1.10441518  0.23967138  0.13599502 -0.12606832  0.13599502]\n",
      " [ 0.          0.2         0.30000001  0.2         1.        ]\n",
      " [ 1.          0.40000001  0.          0.30000001  0.        ]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.37972888  0.17091672  0.16273427  0.12388586  0.16273427]\n",
      " [ 0.2         0.2         0.2         0.2         0.2       ]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 0.2037607   0.56571102  0.08261373  0.54032165  0.08261373]\n",
      " [ 0.10499584  0.10499584  0.10499584  0.10499584  0.10499584]]\n",
      "\n",
      " readed vector=\n",
      " [[ 0.46219721  0.27051508  0.48526132]\n",
      " [ 0.26850125  0.34000003  0.34      ]]\n",
      "\n",
      "=====i= 6\n",
      "Memory after update=\n",
      " [[ 1.12435019  0.24864416  0.19204031 -0.11956458  0.19204031]\n",
      " [ 0.          0.2         0.30000001  0.2         1.        ]\n",
      " [ 1.          0.40000001  0.          0.30000001  0.        ]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.37310824  0.17627706  0.16112171  0.12837134  0.16112171]\n",
      " [ 0.2         0.2         0.2         0.2         0.2       ]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 0.19934976  0.08972772  0.56045294  0.0650375   0.56045294]\n",
      " [ 0.10499584  0.10499584  0.10499584  0.10499584  0.10499584]]\n",
      "\n",
      " readed vector=\n",
      " [[ 0.4819549   0.27038792  0.48213047]\n",
      " [ 0.29800165  0.34000003  0.34      ]]\n",
      "\n",
      "=====i= 7\n",
      "Memory after update=\n",
      " [[ 1.14393759  0.25789833  0.20049886 -0.06532326  0.20049886]\n",
      " [ 0.          0.2         0.30000001  0.2         1.        ]\n",
      " [ 1.          0.40000001  0.          0.30000001  0.        ]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.36452836  0.17454483  0.16643098  0.12806492  0.16643098]\n",
      " [ 0.2         0.2         0.2         0.2         0.2       ]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 0.19587405  0.09254178  0.08458554  0.54241312  0.08458554]\n",
      " [ 0.10499584  0.10499584  0.10499584  0.10499584  0.10499584]]\n",
      "\n",
      " readed vector=\n",
      " [[ 0.50186795  0.27688223  0.47276577]\n",
      " [ 0.3275021   0.34000003  0.34      ]]\n",
      "\n",
      "=====i= 8\n",
      "Memory after update=\n",
      " [[ 1.16307461  0.31456366  0.20923615 -0.05860012  0.20923615]\n",
      " [ 0.          0.2         0.30000001  0.2         1.        ]\n",
      " [ 1.          0.40000001  0.          0.30000001  0.        ]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.36180961  0.17377639  0.16571368  0.13298661  0.16571368]\n",
      " [ 0.2         0.2         0.2         0.2         0.2       ]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 0.1913698   0.56665325  0.08737279  0.06723142  0.08737279]\n",
      " [ 0.10499584  0.10499584  0.10499584  0.10499584  0.10499584]]\n",
      "\n",
      " readed vector=\n",
      " [[ 0.53702885  0.2767804   0.47121614]\n",
      " [ 0.36750206  0.34000003  0.34      ]]\n",
      "\n",
      "=====i= 9\n",
      "Memory after update=\n",
      " [[ 1.18206882  0.32368657  0.21793577 -0.00411652  0.21793577]\n",
      " [ 0.          0.2         0.30000001  0.2         1.        ]\n",
      " [ 1.          0.40000001  0.          0.30000001  0.        ]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.35626405  0.17992106  0.16529252  0.1332299   0.16529252]\n",
      " [ 0.2         0.2         0.2         0.2         0.2       ]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 0.18994251  0.09122898  0.08699623  0.54483604  0.08699623]\n",
      " [ 0.10499584  0.10499584  0.10499584  0.10499584  0.10499584]]\n",
      "\n",
      " readed vector=\n",
      " [[ 0.53232133  0.27751046  0.46820146]\n",
      " [ 0.36750206  0.34000003  0.34      ]]\n"
     ]
    }
   ],
   "source": [
    "# Different approach - change memory \"orientation\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Size of the hidden state 64\n",
    "HIDDEN_SIZE = 3\n",
    "\n",
    "# Size of the MANN memory.\n",
    "MEMORY_SIZE = 5\n",
    "\n",
    "# A batch size of 100\n",
    "#BATCH_SIZE = 2\n",
    "\n",
    "# A single recurrent layer of number of units = sequences of length\n",
    "# e.g. 200 bytes\n",
    "SEQ_LENGTH = 10\n",
    "\n",
    "# \"Read decay\".\n",
    "GAMMA = 0.1\n",
    "\n",
    "# Number of smallest elements.\n",
    "N_SMALLEST =2\n",
    "\n",
    "#EPS = 1e-15\n",
    "\n",
    "# place holders \n",
    "batch = tf.placeholder(tf.float32, shape=None, name=\"Batch_h\")\n",
    "#memory = tf.placeholder(tf.float32, shape=[None], name=\"memory\")\n",
    "memory = tf.Variable(tf.zeros([HIDDEN_SIZE, MEMORY_SIZE]), trainable=False, name=\"Memory_M\")\n",
    "#alpha = tf.Variable(tf.truncated_normal(shape=[1]), name=\"Alpha\")\n",
    "alpha = tf.Variable(tf.truncated_normal(shape=[1]), name=\"Alpha\")\n",
    "\n",
    "# SET INITIAL MEMORY STATE.\n",
    "memory_set = memory.assign(tf.transpose([[1, 0, 1],\n",
    "                            [0.1, 0.2, 0.4],\n",
    "                            [ 0, 0.3, 0],\n",
    "                            [-0.3, 0.2, 0.3],\n",
    "                            [0, 1, 0]]))\n",
    "alpha_set = alpha.assign([0.1])\n",
    "\n",
    "# Placeholders for previous weights.\n",
    "prev_update_weights = tf.placeholder(tf.float32, shape=None, name=\"Prev_uw\")\n",
    "prev_read_weights = tf.placeholder(tf.float32, shape=None, name=\"Prev_rw\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Read_head\"):\n",
    "    # Normalize batches and memory.\n",
    "    norm_batch = tf.nn.l2_normalize(batch,1, name=\"NormalizedBatch_h\") \n",
    "    norm_memory = tf.nn.l2_normalize(memory,1, name=\"NormalizedMemory_h\")\n",
    "\n",
    "    # calculate similarity.\n",
    "    similarity = tf.tensordot(norm_batch, norm_memory, axes=1, name= \"Similarity_D\") \n",
    "    # Read weights based on similarity.\n",
    "    read_weights = tf.nn.softmax(similarity, name=\"Read_weights_rw\")\n",
    "    # Read \"vector\" (in fact batch).\n",
    "    r = tf.tensordot(read_weights, tf.transpose(memory), axes=1, name=\"Read_vector_r\")\n",
    "\n",
    "# TODO: add dependencies, that write will be done after read.\n",
    "with tf.name_scope(\"Write_head\"):\n",
    "    # A \"truncation scheme to update the least-used positions\".\n",
    "    # First, find (size-n) top elements (in each \"batch sample\"/head separatelly).\n",
    "    k_number = MEMORY_SIZE - N_SMALLEST\n",
    "    print(prev_update_weights.shape)\n",
    "    top = tf.nn.top_k(prev_update_weights, k_number)\n",
    "\n",
    "    # To get boolean True/False values, you can first get the k-th value and then use tf.greater_equal:\n",
    "    kth = tf.reduce_min(top.values)\n",
    "    top2 = tf.greater_equal(prev_update_weights, kth)\n",
    "    # And finally - cast it to n smallest elements.\n",
    "    smallest_lru_weights = tf.cast(top2, tf.float32) * -1.0 + 1.0\n",
    "\n",
    "    write_weights = tf.add(tf.sigmoid(alpha) * prev_read_weights, (1.0 - tf.sigmoid(alpha)) * smallest_lru_weights, \n",
    "                           name=\"Write_weights_ww\")\n",
    "    \n",
    "with tf.name_scope(\"Memory_update\"):\n",
    "    calculated_mem_update = tf.tensordot(tf.transpose(batch), write_weights, axes=1)\n",
    "    memory_update_op = memory.assign(memory + calculated_mem_update)\n",
    "\n",
    "with tf.name_scope(\"Update_head\"): # This relies on prev. weights and will be used in fact in NEXT step.\n",
    "    update_weights = tf.add(GAMMA * prev_update_weights, read_weights + write_weights, name=\"Update_weights_uw\")\n",
    "\n",
    "    \n",
    "    \n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "    \n",
    "# Execute graph.\n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(initialize_model)\n",
    "sess.run([memory_set, alpha_set])\n",
    "print(\"Memory =\\n\",memory.eval())\n",
    "\n",
    "#tmp_batch = [[1, 0, 1],[-0.3, 0.3, -0.3]]\n",
    "tmp_batch = [[0.1, 0, 0],[0, 0, 0]]\n",
    "# Prev UW [batch size x memory size]\n",
    "prev_uw = [[0,0,0,0,0], [0,0,0,0,0]]\n",
    "prev_rw = [[0,0,0,0,0], [0,0,0,0,0]]\n",
    "\n",
    "print(\"\\nBatch=\\n\",batch.eval(feed_dict={batch:tmp_batch}))\n",
    "#print(\"Norm batch=\",norm_batch.eval(feed_dict={batch:tmp_batch}))\n",
    "\n",
    "for i in range(10):\n",
    "    sim, r_vect, prev_rw, prev_uw, prev_ww, mu_op = sess.run(\n",
    "        [similarity, r, read_weights, update_weights, write_weights, memory_update_op],\n",
    "                               feed_dict={\n",
    "                                   batch:tmp_batch,\n",
    "                                   prev_update_weights: prev_uw,\n",
    "                                   prev_read_weights: prev_rw\n",
    "                               })\n",
    "    #print(\"Norm memory =\\n\",norm_memory.eval())\n",
    "    print (\"\\n=====i=\", i)\n",
    "    print(\"Memory after update=\\n\", mu_op)\n",
    "    #print(\"\\nSimilarity=\\n\",sim)\n",
    "    print(\"\\nprev_rw=\\n\", prev_rw)\n",
    "    #print(\"\\nprev_uw=\\n\", prev_uw)\n",
    "    print(\"\\nprev_ww=\\n\", prev_ww)\n",
    "\n",
    "    print(\"\\n readed vector=\\n\", r_vect)\n",
    "\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a*b =\n",
      " [array([[ -1.,  -4.,  -9.],\n",
      "       [  4.,   9.,  16.]], dtype=float32)]\n",
      "a dot b =\n",
      " [array([[ 3.,  4.,  5.],\n",
      "       [ 4.,  5.,  6.],\n",
      "       [ 5.,  6.,  7.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Different approach - change memory \"orientation\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# place holders \n",
    "a = tf.placeholder(tf.float32, shape=None)\n",
    "b = tf.placeholder(tf.float32, shape=None)\n",
    "\n",
    "op = a * b\n",
    "op2 = tf.tensordot(tf.transpose(a), b, axes=1)\n",
    "\n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "    \n",
    "# Values\n",
    "va= [[1,2,3],[2,3,4]]\n",
    "vb= [[-1,-2,-3],[2,3,4]]\n",
    "\n",
    "# Initialize session.\n",
    "sess=tf.InteractiveSession()\n",
    "sess.run(initialize_model)\n",
    "# Execute graph.\n",
    "print(\"a*b =\\n\",sess.run([op], feed_dict={a:va, b:vb}))\n",
    "print(\"a dot b =\\n\",sess.run([op2], feed_dict={a:va, b:vb}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
