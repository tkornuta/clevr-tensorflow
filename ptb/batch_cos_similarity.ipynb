{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a*b =\n",
      " [array([[ -1.,  -4.,  -9.],\n",
      "       [  4.,   9.,  16.]], dtype=float32)]\n",
      "a dot b =\n",
      " [array([[ 3.,  4.,  5.],\n",
      "       [ 4.,  5.,  6.],\n",
      "       [ 5.,  6.,  7.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Different approach - change memory \"orientation\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# place holders \n",
    "a = tf.placeholder(tf.float32, shape=None)\n",
    "b = tf.placeholder(tf.float32, shape=None)\n",
    "\n",
    "op = a * b\n",
    "op2 = tf.tensordot(tf.transpose(a), b, axes=1)\n",
    "\n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "    \n",
    "# Values\n",
    "va= [[1,2,3],[2,3,4]]\n",
    "vb= [[-1,-2,-3],[2,3,4]]\n",
    "\n",
    "# Initialize session.\n",
    "sess=tf.InteractiveSession()\n",
    "sess.run(initialize_model)\n",
    "# Execute graph.\n",
    "print(\"a*b =\\n\",sess.run([op], feed_dict={a:va, b:vb}))\n",
    "print(\"a dot b =\\n\",sess.run([op2], feed_dict={a:va, b:vb}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.34999999  0.50999999  0.51999998  0.52999997  0.94      ]\n",
      " [ 0.31999999  0.55000001  0.72000003  0.74000001  0.82999998]\n",
      " [ 0.23        0.34999999  0.63        0.63999999  0.72000003]\n",
      " [ 0.02        0.03        0.11        0.14        0.15000001]\n",
      " [ 0.01        0.04        0.72000003  0.73000002  0.75      ]]\n"
     ]
    }
   ],
   "source": [
    "# Sorts 2D tensor, each row separatelly.\n",
    "a = tf.Variable([[0.51, 0.52, 0.53, 0.94, 0.35],\n",
    "             [0.32, 0.72, 0.83, 0.74, 0.55],\n",
    "             [0.23, 0.72, 0.63, 0.64, 0.35],\n",
    "             [0.11, 0.02, 0.03, 0.14, 0.15],\n",
    "             [0.01, 0.72, 0.73, 0.04, 0.75]],tf.float32)\n",
    "\n",
    "row_size = a.get_shape().as_list()[-1]\n",
    "top_k = tf.nn.top_k(-a, k=row_size)\n",
    "\n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "    \n",
    "# Execute graph.\n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(initialize_model)\n",
    "\n",
    "print(sess.run(-top_k.values))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  4  9 16 25 36]\n"
     ]
    }
   ],
   "source": [
    "# How to perform operation on arrays.\n",
    "elems = np.array([1, 2, 3, 4, 5, 6])\n",
    "#squares = tf.map_fn(lambda x: x * x, elems)\n",
    "squares = tf.multiply(elems, elems)\n",
    "# squares == [1, 4, 9, 16, 25, 36]\n",
    "\n",
    "sess=tf.Session()\n",
    "print(sess.run(squares))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "elems = (np.array([1, 2, 3]), np.array([-1, 1, -1]))\n",
    "alternate = tf.map_fn(lambda x: x[0] * x[1], elems, dtype=tf.int64)\n",
    "# alternate == [-1, 2, -3]\n",
    "\n",
    "sess=tf.Session()\n",
    "print(sess.run(alternate))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"Broadcasting ability of TF\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.constant([[0, 1,2],[2, 3,2],[4, 5,2],[6, 7,2]], dtype=tf.float32)\n",
    "y = tf.constant([[0, 1,2],[-2, -1,2]], dtype=tf.float32)\n",
    "\n",
    "x_vect_len = int(x.shape[1])\n",
    "print(\"len=\",x_vect_len)\n",
    "x_ = tf.expand_dims(x, 0)\n",
    "y_ = tf.expand_dims(y, 1)\n",
    "z = tf.reshape(tf.add(x_, y_), [-1, x_vect_len])\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(z)\n",
    "print(x_.eval())\n",
    "print(y_.eval())\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# place holders \n",
    "batch = tf.placeholder(tf.float32, shape=[3,3], name=\"batch\")\n",
    "memory = tf.placeholder(tf.float32, shape=[2,3], name=\"memory\")\n",
    "# Cos similarity\n",
    "norm_batch = tf.nn.l2_normalize(batch,0) \n",
    "norm_memory = tf.nn.l2_normalize(memory,0)\n",
    "\n",
    "nb_ = tf.expand_dims(norm_batch, 0)\n",
    "nm_ = tf.expand_dims(norm_memory, 1)\n",
    "cos_similarity = tf.reshape(tf.add(nb_,nm_), [-1, 2])\n",
    "#cos_similarity = tf.reshape(tf.reduce_sum(tf.multiply(nb_,nm_)), [-1, 2])\n",
    "\n",
    "                            \n",
    "sess=tf.InteractiveSession()\n",
    "cos_sim=sess.run(cos_similarity,feed_dict={batch:[[1,2,3],[3,3,3],[1,2,6]],memory:[[2,4,6],[1,2,4]]})\n",
    "\n",
    "print(cos_sim)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Size of the hidden state 64\n",
    "HIDDEN_SIZE = 3\n",
    "\n",
    "# Size of the MANN memory.\n",
    "MEMORY_SIZE = 5\n",
    "\n",
    "# A batch size of 100\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# A single recurrent layer of number of units = sequences of length\n",
    "# e.g. 200 bytes\n",
    "SEQ_LENGTH = 10\n",
    "\n",
    "EPS = 1e-15\n",
    "\n",
    "# place holders \n",
    "batch = tf.placeholder(tf.float32, shape=None, name=\"batch\")\n",
    "#memory = tf.placeholder(tf.float32, shape=[None], name=\"memory\")\n",
    "memory = tf.Variable(tf.zeros([MEMORY_SIZE, HIDDEN_SIZE]), trainable=False, name=\"memory\")\n",
    "\n",
    "memory_set = memory.assign([[1, 0, 1],[0.1, 0.2 , 0.4],[ 0,0,0],[-0.3,0.2,0.3],[0, 1, 0]])\n",
    "\n",
    "# Create BATCH_SIZE placeholders for similarity - each MEMORY_SIZE x 1,  \n",
    "with tf.name_scope(\"similarity\"):\n",
    "  # Define similarity buffers.\n",
    "  similarity = list()\n",
    "  #for b in range(BATCH_SIZE):\n",
    "    # Collect placeholders for similarity.\n",
    "    #similarity.append(tf.placeholder(tf.float32, shape=[MEMORY_SIZE, 1], name=\"Similarity\"))\n",
    " \n",
    "  # Normalize\n",
    "  norm_batch = tf.nn.l2_normalize(batch,1) \n",
    "  norm_memory = tf.nn.l2_normalize(memory,1)\n",
    "  print(norm_batch[0])\n",
    "  # Define similarity buffers.\n",
    "  numerator_batch = list()\n",
    "  denominator_batch = list()\n",
    "  similarity_batch = list()\n",
    "\n",
    "  for b in range(BATCH_SIZE):\n",
    "    similarity_batch.append(tf.map_fn(lambda x: tf.reduce_sum(tf.multiply(norm_batch[b],x)), norm_memory))\n",
    "    # Read weight based on similarity.\n",
    "  read_weight = tf.nn.softmax(similarity_batch)\n",
    "    \n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(memory_set)\n",
    "print(\"memory =\",memory.eval())\n",
    "\n",
    "tmp_batch = [[1, 0, 1],[-0.3,0.3,0.3]]\n",
    "\n",
    "print(\"Batch=\",batch.eval(feed_dict={batch:tmp_batch}))\n",
    "print(\"Norm batch=\",norm_batch.eval(feed_dict={batch:tmp_batch}))\n",
    "\n",
    "\n",
    "num, den, sim, rw=sess.run([numerator_batch, denominator_batch, similarity_batch, read_weight],\n",
    "                           feed_dict={batch:tmp_batch})\n",
    "print(\"norm memory =\",norm_memory.eval())\n",
    "\n",
    "print(\"numerator_batch=\",num)\n",
    "print(\"denominator_batch=\\n\",den,\"\\n\")\n",
    "\n",
    "print(\"similarity=\",sim)\n",
    "print(\"rw=\", rw)\n",
    "\n",
    "\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def rest(x): return tf.gather(x, tf.range(1, tf.size(x)))\n",
    "\n",
    "x = tf.Variable([0., 1.])\n",
    "x_op = tf.assign(x, [0., 1., 2.], validate_shape=False)\n",
    "\n",
    "with tf.control_dependencies([x_op]):\n",
    "    true_fun  = lambda: tf.assign(x, rest(x_op), validate_shape=False)\n",
    "    false_fun = lambda: tf.constant([])\n",
    "    pred = tf.constant(True)\n",
    "    cond_op = tf.cond(pred, true_fun, false_fun)\n",
    "\n",
    "with tf.Session(\"\"):\n",
    "  x.initializer.run()\n",
    "  print(cond_op.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finds given top elements\n",
    "import tensorflow as tf\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tmp_batch = [[1, 0, 1, 2, 3, 4, 5, 6, 1],[-0.2, 0.1, -0.4, 0.5, 0.1, 0.9, 0.8, 0.3, -0.3]]\n",
    "\n",
    "\n",
    "top_k = tf.nn.top_k(tmp_batch, 3)\n",
    "ones = tf.where\n",
    "\n",
    "\n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "    \n",
    "# Execute graph.\n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(initialize_model)\n",
    "print(sess.run([top_k, top_k]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finds n smallest elements - using additional division into list\n",
    "### (~ok, does not handle several elements with the same values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch.shape= (2, 4)\n",
      "k_number= 1\n",
      "batch= [[ 4  3  2  1]\n",
      " [10 20  1 40]]\n",
      "smallest_ones= [array([ 0.,  1.,  1.,  1.], dtype=float32), array([ 1.,  1.,  1.,  0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Finds n smallest elements\n",
    "\n",
    "import tensorflow as tf\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "batch = tf.convert_to_tensor([[4, 3, 2, 1], [10, 20, 1, 40]])\n",
    "print(\"batch.shape=\", batch.shape)\n",
    "\n",
    "# Number of smallest elements.\n",
    "n = 3\n",
    "\n",
    "# You can do it using built-in tf.nn.top_k function - find size-n top elements in each sample.\n",
    "k_number = batch.shape[1] - n\n",
    "print(\"k_number=\", k_number)\n",
    "\n",
    "smallest_ones_batch = list()\n",
    "for i_batch in range(batch.shape[0]):\n",
    "    top = tf.nn.top_k(batch[i_batch], k_number)\n",
    "    # To get boolean True/False values, you can first get the k-th value and then use tf.greater_equal:\n",
    "    kth = tf.reduce_min(top.values)\n",
    "    top2 = tf.greater_equal(batch[i_batch], kth)\n",
    "\n",
    "    # And finally - cast it to n smallest elements.\n",
    "    smallest = tf.cast(top2, tf.float32) * -1.0 + 1.0\n",
    "    \n",
    "    smallest_ones_batch.append(smallest)\n",
    "\n",
    "\n",
    "#w_prew = tf.convert_to_tensor()\n",
    "\n",
    "# Execute graph.e\n",
    "sess=tf.InteractiveSession()\n",
    "print(\"batch=\",sess.run(batch))\n",
    "\n",
    "print(\"smallest_ones=\",sess.run(smallest_ones_batch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finds n smallest elements - without the division (OK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch= [[ 4  3  2  1]\n",
      " [10 20  1 40]]\n",
      "smallest_ones= [[ 0.  1.  1.  1.]\n",
      " [ 1.  1.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Finds n smallest elements\n",
    "\n",
    "import tensorflow as tf\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "batch = tf.convert_to_tensor([[4, 3, 2, 1], [10, 20, 1, 40]])\n",
    "#batch = tf.convert_to_tensor([[ 0.1143328,   0.31078878,  0.1143328,   0.1143328,   0.23188008,  0.1143328 ],\n",
    "#    [ 0.1143328,   0.1143328,   0.31078878,  0.1143328,   0.1143328,   0.23188008],\n",
    "#    [ 0.1143328,   0.31078878,  0.1143328,   0.1143328,   0.23188008,  0.1143328 ]])\n",
    "\n",
    "#print(\"batch.shape=\", batch.shape)\n",
    "\n",
    "# Number of smallest elements.\n",
    "n = 3\n",
    "\n",
    "# You can do it using built-in tf.nn.top_k function - find size-n top elements ALONG THE LAST DIMENSION\n",
    "\n",
    "top = tf.nn.top_k(-batch, n)\n",
    "# To get boolean True/False values, you can first get the k-th value and then use tf.greater_equal:\n",
    "kth = tf.reduce_min(top.values, axis=1, keep_dims=True)\n",
    "#top2 = tf.greater_equal(batch, kth)\n",
    "\n",
    "# And finally - cast it to n smallest elements.\n",
    "#smallest_ones_batch = tf.cast(top2, tf.float32) * -1.0 + 1.0\n",
    "smallest_ones_batch = tf.cast(tf.greater_equal(-batch, kth), tf.float32)\n",
    "    \n",
    "\n",
    "\n",
    "#w_prew = tf.convert_to_tensor()\n",
    "\n",
    "# Execute graph.e\n",
    "sess=tf.InteractiveSession()\n",
    "print(\"batch=\",sess.run(batch))\n",
    "\n",
    "#print(\"top.values=\",sess.run(top.values))\n",
    "\n",
    "#print(\"kth=\",sess.run(kth))\n",
    "\n",
    "print(\"smallest_ones=\",sess.run(smallest_ones_batch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different approach - change memory \"orientation\" (BAD!)\n",
    "### Fully operational read-write-update heads, batch of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unknown>\n",
      "Memory =\n",
      " [[ 1.          0.1         0.         -0.30000001  0.        ]\n",
      " [ 0.          0.2         0.30000001  0.2         1.        ]\n",
      " [ 1.          0.40000001  0.          0.30000001  0.        ]]\n",
      "\n",
      "Batch=\n",
      " [[ 0.1  0.   0. ]\n",
      " [ 0.  -0.1  0. ]]\n",
      "\n",
      "=====i= 0\n",
      "Memory after update=\n",
      " [[ 1.          0.1         0.         -0.30000001  0.        ]\n",
      " [ 0.          0.2         0.30000001  0.2         1.        ]\n",
      " [ 1.          0.40000001  0.          0.30000001  0.        ]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.40252841  0.17065592  0.15513614  0.11654346  0.15513614]\n",
      " [ 0.26199317  0.21776541  0.19853558  0.21776541  0.10394045]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "\n",
      " readed vector=\n",
      " [[ 0.38463095  0.25911686  0.50575382]\n",
      " [ 0.21844007  0.25060728  0.41442895]]\n",
      "\n",
      "=====i= 1\n",
      "Memory after update=\n",
      " [[ 1.06863403  0.15646116  0.00814432 -0.29388171  0.00814432]\n",
      " [-0.06125618  0.14106569  0.24207522  0.14106569  0.99454331]\n",
      " [ 1.          0.40000001  0.          0.30000001  0.        ]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.40252841  0.17065592  0.15513614  0.11654346  0.15513614]\n",
      " [ 0.26199317  0.21776541  0.19853558  0.21776541  0.10394045]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 0.68633986  0.56461161  0.08144324  0.06118289  0.08144324]\n",
      " [ 0.61256176  0.58934313  0.57924789  0.58934313  0.05456657]]\n",
      "\n",
      " readed vector=\n",
      " [[ 0.38463095  0.25911686  0.50575382]\n",
      " [ 0.21844007  0.25060728  0.41442895]]\n",
      "\n",
      "=====i= 2\n",
      "Memory after update=\n",
      " [[ 1.13726807  0.21292233  0.01628865 -0.28776342  0.01628865]\n",
      " [-0.12251236  0.08213137  0.18415043  0.08213137  0.98908663]\n",
      " [ 1.          0.40000001  0.          0.30000001  0.        ]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.39773834  0.17607072  0.15422057  0.11774987  0.15422057]\n",
      " [ 0.26597121  0.21914014  0.19894296  0.21914016  0.09680546]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 0.68633986  0.56461161  0.08144324  0.06118289  0.08144324]\n",
      " [ 0.61256176  0.58934313  0.57924789  0.58934313  0.05456657]]\n",
      "\n",
      " readed vector=\n",
      " [[ 0.42049244  0.2077961   0.50349158]\n",
      " [ 0.25652018  0.18997031  0.41936931]]\n",
      "\n",
      "=====i= 3\n",
      "Memory after update=\n",
      " [[ 1.20565057  0.26966777  0.02438491 -0.28158179  0.02438491]\n",
      " [-0.18397738  0.02312489  0.12620425  0.02312489  0.98400456]\n",
      " [ 1.          0.40000001  0.          0.30000001  0.        ]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.39299172  0.18102859  0.15350956  0.11896058  0.15350956]\n",
      " [ 0.26931077  0.22036037  0.1993895   0.22036037  0.090579  ]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 0.68382514  0.56745428  0.08096258  0.06181623  0.08096258]\n",
      " [ 0.61465019  0.59006482  0.57946175  0.59006482  0.05082085]]\n",
      "\n",
      " readed vector=\n",
      " [[ 0.45625043  0.15659527  0.50109136]\n",
      " [ 0.29450974  0.12951124  0.42356303]]\n",
      "\n",
      "=====i= 4\n",
      "Memory after update=\n",
      " [[ 1.27378392  0.32667348  0.03244384 -0.27533659  0.03244384]\n",
      " [-0.24561772 -0.03594566  0.06823464 -0.03594566  0.97924936]\n",
      " [ 1.          0.40000001  0.          0.30000001  0.        ]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.38837281  0.18559593  0.15294355  0.12014421  0.15294355]\n",
      " [ 0.27181727  0.22140171  0.19991061  0.22140171  0.08546873]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 0.6813333   0.57005703  0.08058932  0.06245183  0.08058932]\n",
      " [ 0.61640334  0.59070545  0.57969618  0.59070545  0.04755209]]\n",
      "\n",
      " readed vector=\n",
      " [[ 0.49191973  0.10541767  0.49865445]\n",
      " [ 0.33203781  0.06956275  0.42679849]]\n",
      "\n",
      "=====i= 5\n",
      "Memory after update=\n",
      " [[ 1.3416748   0.38391897  0.04047306 -0.26902926  0.04047306]\n",
      " [-0.30738965 -0.09507087  0.01023766 -0.09507087  0.97476244]\n",
      " [ 1.          0.40000001  0.          0.30000001  0.        ]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.38394246  0.18979745  0.15248805  0.121284    0.15248805]\n",
      " [ 0.27335516  0.22225463  0.20053661  0.22225463  0.08159901]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 0.67890847  0.57245481  0.08029218  0.06307321  0.08029218]\n",
      " [ 0.61771923  0.59125209  0.57996976  0.59125209  0.0448693 ]]\n",
      "\n",
      " readed vector=\n",
      " [[ 0.5275622   0.05424369  0.49624664]\n",
      " [ 0.36875883  0.01047028  0.42893338]]\n",
      "\n",
      "=====i= 6\n",
      "Memory after update=\n",
      " [[ 1.40933311  0.44138503  0.04847836 -0.26266211  0.04847836]\n",
      " [-0.36924231 -0.15424086 -0.04779218 -0.15424086  0.97047865]\n",
      " [ 1.          0.40000001  0.          0.30000001  0.        ]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.37973201  0.1936619   0.15211757  0.12237101  0.15211757]\n",
      " [ 0.27390361  0.22292356  0.2012767   0.22292356  0.07897255]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 0.67658263  0.57466054  0.08005305  0.06367157  0.08005305]\n",
      " [ 0.61852658  0.5916999   0.58029836  0.5916999   0.04283778]]\n",
      "\n",
      " readed vector=\n",
      " [[ 0.56321937  0.00306462  0.49390808]\n",
      " [ 0.40444374 -0.04754213  0.42995012]]\n",
      "\n",
      "=====i= 7\n",
      "Memory after update=\n",
      " [[ 1.47677028  0.49905396  0.05646422 -0.25623789  0.05646422]\n",
      " [-0.43112376 -0.21344596 -0.10586087 -0.21344596  0.96633273]\n",
      " [ 1.          0.40000001  0.          0.30000001  0.        ]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.37575382  0.19721866  0.15181312  0.12340123  0.15181312]\n",
      " [ 0.27355355  0.22342254  0.20211545  0.22342254  0.07748588]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 0.6743722   0.5766893   0.07985856  0.06424223  0.07985856]\n",
      " [ 0.61881453  0.59205103  0.58068693  0.59205103  0.04145894]]\n",
      "\n",
      " readed vector=\n",
      " [[ 0.59891808 -0.04812098  0.49166167]\n",
      " [ 0.43901339 -0.10439046  0.42994934]]\n",
      "\n",
      "=====i= 8\n",
      "Memory after update=\n",
      " [[ 1.54399872  0.55690962  0.0644341  -0.24975958  0.0644341 ]\n",
      " [-0.49298683 -0.27267727 -0.1639736  -0.27267727  0.9622649 ]\n",
      " [ 1.          0.40000001  0.          0.30000001  0.        ]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.37844878  0.20276201  0.14851482  0.12175955  0.14851482]\n",
      " [ 0.27402708  0.22451232  0.20348559  0.22451232  0.07346272]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 0.67228377  0.57855654  0.07969873  0.06478307  0.07969873]\n",
      " [ 0.61863077  0.59231299  0.58112723  0.59231299  0.04067848]]\n",
      "\n",
      " readed vector=\n",
      " [[ 0.64564329 -0.10463326  0.49608147]\n",
      " [ 0.47482789 -0.16453382  0.43118569]]\n",
      "\n",
      "=====i= 9\n",
      "Memory after update=\n",
      " [[ 1.61136854  0.61505628  0.07223082 -0.24336746  0.07223082]\n",
      " [-0.55487478 -0.33196577 -0.22215825 -0.33196577  0.95840824]\n",
      " [ 1.          0.40000001  0.          0.30000001  0.        ]]\n",
      "\n",
      "prev_rw=\n",
      " [[ 0.36849064  0.20352109  0.15134919  0.12528992  0.15134919]\n",
      " [ 0.27085835  0.22399016  0.20394628  0.22399016  0.07721505]]\n",
      "\n",
      "prev_ww=\n",
      " [[ 0.67369854  0.58146667  0.07796719  0.06392123  0.07796719]\n",
      " [ 0.61887932  0.59288514  0.58184654  0.59288514  0.0385664 ]]\n",
      "\n",
      " readed vector=\n",
      " [[ 0.67050356 -0.15049957  0.48748606]\n",
      " [ 0.50511992 -0.21482414  0.42765146]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Size of the hidden state 64\n",
    "HIDDEN_SIZE = 3\n",
    "\n",
    "# Size of the MANN memory.\n",
    "MEMORY_SIZE = 5\n",
    "\n",
    "# A batch size of 100\n",
    "#BATCH_SIZE = 2\n",
    "\n",
    "# A single recurrent layer of number of units = sequences of length\n",
    "# e.g. 200 bytes\n",
    "SEQ_LENGTH = 10\n",
    "\n",
    "# \"Read decay\".\n",
    "GAMMA = 0.1\n",
    "\n",
    "# Number of smallest elements.\n",
    "N_SMALLEST =2\n",
    "\n",
    "#EPS = 1e-15\n",
    "\n",
    "# place holders \n",
    "batch = tf.placeholder(tf.float32, shape=None, name=\"Batch_h\")\n",
    "#memory = tf.placeholder(tf.float32, shape=[None], name=\"memory\")\n",
    "memory = tf.Variable(tf.zeros([HIDDEN_SIZE, MEMORY_SIZE]), trainable=False, name=\"Memory_M\")\n",
    "#alpha = tf.Variable(tf.truncated_normal(shape=[1]), name=\"Alpha\")\n",
    "alpha = tf.Variable(tf.truncated_normal(shape=[1]), name=\"Alpha\")\n",
    "\n",
    "# SET INITIAL MEMORY STATE.\n",
    "memory_set = memory.assign(tf.transpose([[1, 0, 1],\n",
    "                            [0.1, 0.2, 0.4],\n",
    "                            [ 0, 0.3, 0],\n",
    "                            [-0.3, 0.2, 0.3],\n",
    "                            [0, 1, 0]]))\n",
    "alpha_set = alpha.assign([0.1])\n",
    "\n",
    "# Placeholders for previous weights.\n",
    "prev_update_weights = tf.placeholder(tf.float32, shape=None, name=\"Prev_uw\")\n",
    "prev_read_weights = tf.placeholder(tf.float32, shape=None, name=\"Prev_rw\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Read_head\"):\n",
    "    # Normalize batches and memory.\n",
    "    norm_batch = tf.nn.l2_normalize(batch,1, name=\"NormalizedBatch_h\") \n",
    "    norm_memory = tf.nn.l2_normalize(memory,1, name=\"NormalizedMemory_h\")\n",
    "\n",
    "    # calculate similarity.\n",
    "    similarity = tf.tensordot(norm_batch, norm_memory, axes=1, name= \"Similarity_D\") \n",
    "    # Read weights based on similarity.\n",
    "    read_weights = tf.nn.softmax(similarity, name=\"Read_weights_rw\")\n",
    "    # Read \"vector\" (in fact batch).\n",
    "    r = tf.tensordot(read_weights, tf.transpose(memory), axes=1, name=\"Read_vector_r\")\n",
    "\n",
    "# TODO: add dependencies, that write will be done after read.\n",
    "with tf.name_scope(\"Write_head\"):\n",
    "    # A \"truncation scheme to update the least-used positions\".\n",
    "    # First, find (size-n) top elements (in each \"batch sample\"/head separatelly).\n",
    "    k_number = MEMORY_SIZE - N_SMALLEST\n",
    "    print(prev_update_weights.shape)\n",
    "    top = tf.nn.top_k(prev_update_weights, k_number)\n",
    "\n",
    "    # To get boolean True/False values, you can first get the k-th value and then use tf.greater_equal:\n",
    "    kth = tf.reduce_min(top.values)\n",
    "    top2 = tf.greater(prev_update_weights, kth)\n",
    "    # And finally - cast it to n smallest elements.\n",
    "    smallest_lru_weights = tf.cast(top2, tf.float32) * -1.0 + 1.0\n",
    "\n",
    "    write_weights = tf.add(tf.sigmoid(alpha) * prev_read_weights, (1.0 - tf.sigmoid(alpha)) * smallest_lru_weights, \n",
    "                           name=\"Write_weights_ww\")\n",
    "    \n",
    "with tf.name_scope(\"Memory_update\"):\n",
    "    calculated_mem_update = tf.tensordot(tf.transpose(batch), write_weights, axes=1)\n",
    "    memory_update_op = memory.assign(memory + calculated_mem_update)\n",
    "\n",
    "with tf.name_scope(\"Update_head\"): # This relies on prev. weights and will be used in fact in NEXT step.\n",
    "    update_weights = tf.add(GAMMA * prev_update_weights, read_weights + write_weights, name=\"Update_weights_uw\")\n",
    "\n",
    "    \n",
    "    \n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "    \n",
    "# Execute graph.\n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(initialize_model)\n",
    "sess.run([memory_set, alpha_set])\n",
    "print(\"Memory =\\n\",memory.eval())\n",
    "\n",
    "#tmp_batch = [[1, 0, 1],[-0.3, 0.3, -0.3]]\n",
    "tmp_batch = [[0.1, 0, 0],[0, -0.1, 0]]\n",
    "# Prev UW [batch size x memory size]\n",
    "prev_uw = [[0,0,0,0,0], [0,0,0,0,0]]\n",
    "prev_rw = [[0,0,0,0,0], [0,0,0,0,0]]\n",
    "\n",
    "print(\"\\nBatch=\\n\",batch.eval(feed_dict={batch:tmp_batch}))\n",
    "#print(\"Norm batch=\",norm_batch.eval(feed_dict={batch:tmp_batch}))\n",
    "\n",
    "for i in range(10):\n",
    "    sim, r_vect, prev_rw, prev_uw, prev_ww, mu_op = sess.run(\n",
    "        [similarity, r, read_weights, update_weights, write_weights, memory_update_op],\n",
    "                               feed_dict={\n",
    "                                   batch:tmp_batch,\n",
    "                                   prev_update_weights: prev_uw,\n",
    "                                   prev_read_weights: prev_rw\n",
    "                               })\n",
    "    #print(\"Norm memory =\\n\",norm_memory.eval())\n",
    "    print (\"\\n=====i=\", i)\n",
    "    print(\"Memory after update=\\n\", mu_op)\n",
    "    #print(\"\\nSimilarity=\\n\",sim)\n",
    "    print(\"\\nprev_rw=\\n\", prev_rw)\n",
    "    #print(\"\\nprev_uw=\\n\", prev_uw)\n",
    "    print(\"\\nprev_ww=\\n\", prev_ww)\n",
    "\n",
    "    print(\"\\n readed vector=\\n\", r_vect)\n",
    "\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different approach - change memory \"orientation\"\n",
    "### Read-write-update heads, step towards batch of \"sequences of vectors\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Size of the hidden state 64\n",
    "HIDDEN_SIZE = 3\n",
    "\n",
    "# Size of the MANN memory.\n",
    "MEMORY_SIZE = 5\n",
    "\n",
    "# A batch size of 100\n",
    "#BATCH_SIZE = 2\n",
    "\n",
    "# A single recurrent layer of number of units = sequences of length\n",
    "# e.g. 200 bytes\n",
    "SEQ_LENGTH = 10\n",
    "\n",
    "# \"Read decay\".\n",
    "GAMMA = 0.1\n",
    "\n",
    "# Number of smallest elements.\n",
    "N_SMALLEST =2\n",
    "\n",
    "#EPS = 1e-15\n",
    "\n",
    "# place holders \n",
    "batch = tf.placeholder(tf.float32, shape=None, name=\"Batch_h\")\n",
    "#memory = tf.placeholder(tf.float32, shape=[None], name=\"memory\")\n",
    "memory = tf.Variable(tf.zeros([HIDDEN_SIZE, MEMORY_SIZE]), trainable=False, name=\"Memory_M\")\n",
    "#alpha = tf.Variable(tf.truncated_normal(shape=[1]), name=\"Alpha\")\n",
    "alpha = tf.Variable(tf.truncated_normal(shape=[1]), name=\"Alpha\")\n",
    "\n",
    "# SET INITIAL MEMORY STATE.\n",
    "memory_set = memory.assign(tf.transpose([[1, 0, 1],\n",
    "                            [0.1, 0.2, 0.4],\n",
    "                            [ 0, 0.3, 0],\n",
    "                            [-0.3, 0.2, 0.3],\n",
    "                            [0, 1, 0]]))\n",
    "alpha_set = alpha.assign([0.1])\n",
    "\n",
    "# Placeholders for previous weights.\n",
    "prev_update_weights = tf.placeholder(tf.float32, shape=None, name=\"Prev_uw\")\n",
    "prev_read_weights = tf.placeholder(tf.float32, shape=None, name=\"Prev_rw\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Read_head\"):\n",
    "    # Normalize batches and memory.\n",
    "    norm_batch = tf.nn.l2_normalize(batch,1, name=\"NormalizedBatch_h\") \n",
    "    norm_memory = tf.nn.l2_normalize(memory,1, name=\"NormalizedMemory_h\")\n",
    "\n",
    "    # calculate similarity.\n",
    "    similarity = tf.tensordot(norm_batch, norm_memory, axes=1, name= \"Similarity_D\") \n",
    "    # Read weights based on similarity.\n",
    "    read_weights = tf.nn.softmax(similarity, name=\"Read_weights_rw\")\n",
    "    # Read \"vector\" (in fact batch).\n",
    "    r = tf.tensordot(read_weights, tf.transpose(memory), axes=1, name=\"Read_vector_r\")\n",
    "\n",
    "# TODO: add dependencies, that write will be done after read.\n",
    "with tf.name_scope(\"Write_head\"):\n",
    "    # A \"truncation scheme to update the least-used positions\".\n",
    "    # First, find (size-n) top elements (in each \"batch sample\"/head separatelly).\n",
    "    k_number = MEMORY_SIZE - N_SMALLEST\n",
    "    print(prev_update_weights.shape)\n",
    "    top = tf.nn.top_k(prev_update_weights, k_number)\n",
    "\n",
    "    # To get boolean True/False values, you can first get the k-th value and then use tf.greater_equal:\n",
    "    kth = tf.reduce_min(top.values)\n",
    "    top2 = tf.greater_equal(prev_update_weights, kth)\n",
    "    # And finally - cast it to n smallest elements.\n",
    "    smallest_lru_weights = tf.cast(top2, tf.float32) * -1.0 + 1.0\n",
    "\n",
    "    write_weights = tf.add(tf.sigmoid(alpha) * prev_read_weights, (1.0 - tf.sigmoid(alpha)) * smallest_lru_weights, \n",
    "                           name=\"Write_weights_ww\")\n",
    "    \n",
    "with tf.name_scope(\"Memory_update\"):\n",
    "    calculated_mem_update = tf.tensordot(tf.transpose(batch), write_weights, axes=1)\n",
    "    memory_update_op = memory.assign(memory + calculated_mem_update)\n",
    "\n",
    "with tf.name_scope(\"Update_head\"): # This relies on prev. weights and will be used in fact in NEXT step.\n",
    "    update_weights = tf.add(GAMMA * prev_update_weights, read_weights + write_weights, name=\"Update_weights_uw\")\n",
    "\n",
    "    \n",
    "    \n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "    \n",
    "# Execute graph.\n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(initialize_model)\n",
    "sess.run([memory_set, alpha_set])\n",
    "print(\"Memory =\\n\",memory.eval())\n",
    "\n",
    "#tmp_batch = [[1, 0, 1],[-0.3, 0.3, -0.3]]\n",
    "tmp_batch = [[0.1, 0, 0],[0, 0, 0]]\n",
    "# Prev UW [batch size x memory size]\n",
    "prev_uw = [[0,0,0,0,0], [0,0,0,0,0]]\n",
    "prev_rw = [[0,0,0,0,0], [0,0,0,0,0]]\n",
    "\n",
    "print(\"\\nBatch=\\n\",batch.eval(feed_dict={batch:tmp_batch}))\n",
    "#print(\"Norm batch=\",norm_batch.eval(feed_dict={batch:tmp_batch}))\n",
    "\n",
    "for i in range(10):\n",
    "    sim, r_vect, prev_rw, prev_uw, prev_ww, mu_op = sess.run(\n",
    "        [similarity, r, read_weights, update_weights, write_weights, memory_update_op],\n",
    "                               feed_dict={\n",
    "                                   batch:tmp_batch,\n",
    "                                   prev_update_weights: prev_uw,\n",
    "                                   prev_read_weights: prev_rw\n",
    "                               })\n",
    "    #print(\"Norm memory =\\n\",norm_memory.eval())\n",
    "    print (\"\\n=====i=\", i)\n",
    "    print(\"Memory after update=\\n\", mu_op)\n",
    "    #print(\"\\nSimilarity=\\n\",sim)\n",
    "    print(\"\\nprev_rw=\\n\", prev_rw)\n",
    "    #print(\"\\nprev_uw=\\n\", prev_uw)\n",
    "    print(\"\\nprev_ww=\\n\", prev_ww)\n",
    "\n",
    "    print(\"\\n readed vector=\\n\", r_vect)\n",
    "\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
