{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import tarfile\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import shutil \n",
    "import random\n",
    "\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "# Dirs - must be absolute paths!\n",
    "LOG_DIR = '/tmp/tf/ptb_char_lstm/20char/'\n",
    "# Local dir where PTB files will be stored.\n",
    "PTB_DIR = '/home/tkornuta/data/ptb/'\n",
    "\n",
    "# Filenames.\n",
    "TRAIN = \"ptb.train.txt\"\n",
    "VALID = \"ptb.valid.txt\"\n",
    "TEST = \"ptb.test.txt\"\n",
    "\n",
    "# Size of the hidden state 64\n",
    "HIDDEN_SIZE = 64\n",
    "\n",
    "# A batch size of 100\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# A single recurrent layer of number of units = sequences of length\n",
    "# e.g. 200 bytes\n",
    "SEQ_LENGTH = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check/maybe download PTB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified /home/tkornuta/data/ptb/simple-examples.tgz ( 34869662 )\n"
     ]
    }
   ],
   "source": [
    "def maybe_download_ptb(path, \n",
    "                       filename='simple-examples.tgz', \n",
    "                       url='http://www.fit.vutbr.cz/~imikolov/rnnlm/', \n",
    "                       expected_bytes =34869662):\n",
    "  # Eventually create the PTB dir.\n",
    "  if not tf.gfile.Exists(path):\n",
    "    tf.gfile.MakeDirs(path)\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  _filename = path+filename\n",
    "  if not os.path.exists(_filename):\n",
    "    print('Downloading %s...' % filename)\n",
    "    _filename, _ = urlretrieve(url+filename, _filename)\n",
    "  statinfo = os.stat(_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', (_filename), '(', statinfo.st_size, ')')\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + _filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download_ptb(PTB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract dataset-related files from the PTB archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_ptb(path, filename='simple-examples.tgz', files=[\"ptb.train.txt\", \"ptb.valid.txt\", \"ptb.test.txt\", \n",
    "                                       \"ptb.char.train.txt\", \"ptb.char.valid.txt\", \"ptb.char.test.txt\"]):\n",
    "    \"\"\"Extracts files from PTB archive.\"\"\"\n",
    "    # Extract\n",
    "    tar = tarfile.open(path+filename)\n",
    "    tar.extractall(path)\n",
    "    tar.close()\n",
    "    # Copy files\n",
    "    for file in files:\n",
    "        shutil.copyfile(PTB_DIR+\"simple-examples/data/\"+file, PTB_DIR+file)\n",
    "    # Delete directory\n",
    "    shutil.rmtree(PTB_DIR+\"simple-examples/\")        \n",
    "\n",
    "extract_ptb(PTB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train, valid and test texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5101618  aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memote\n",
      "399782  consumers may want to move their telephones a little closer to \n",
      "449945  no it was n't black monday \n",
      " but while the new york stock excha\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename, path):\n",
    "    with open(path+filename, 'r') as myfile:\n",
    "        data=myfile.read()# .replace('\\n', '')\n",
    "        return data\n",
    "\n",
    "train_text = read_data(TRAIN, PTB_DIR)\n",
    "train_size=len(train_text)\n",
    "print(train_size, train_text[:100])\n",
    "\n",
    "valid_text = read_data(VALID, PTB_DIR)\n",
    "valid_size=len(valid_text)\n",
    "print(valid_size, valid_text[:64])\n",
    "\n",
    "test_text = read_data(TEST, PTB_DIR)\n",
    "test_size=len(test_text)\n",
    "print(test_size, test_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size =  59\n",
      "65\n",
      "33 1 58 26 0 0\n",
      "a A\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 59 # [A-Z] + [a-z] + ' ' +few 'in between; + punctuation\n",
    "first_letter = ord(string.ascii_uppercase[0]) # ascii_uppercase before lowercase! \n",
    "print(\"vocabulary size = \", vocabulary_size)\n",
    "print(first_letter)\n",
    "\n",
    "def char2id(char):\n",
    "  \"\"\" Converts char to id (int) with one-hot encoding handling of unexpected characters\"\"\"\n",
    "  if char in string.ascii_letters:# or char in string.punctuation or char in string.digits:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    # print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  \"\"\" Converts single id (int) to character\"\"\"\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "#print(len(string.punctuation))\n",
    "#for i in string.ascii_letters:\n",
    "#    print (i, char2id(i))\n",
    "\n",
    "\n",
    "print(char2id('a'), char2id('A'), char2id('z'), char2id('Z'), char2id(' '), char2id('Ã¯'))\n",
    "print(id2char(char2id('a')), id2char(char2id('A')))\n",
    "#print(id2char(65), id2char(33), id2char(90), id2char(58), id2char(0))\n",
    "#bankno\n",
    "sample = np.zeros(shape=(1, vocabulary_size), dtype=np.float)\n",
    "sample[0, char2id(' ')] = 1.0\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper class for batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, seq_length, vocab_size):\n",
    "    \"\"\"\n",
    "    Initializes the batch generator object. Stores the variables and first \"letter batch\".\n",
    "    text is text to be processed\n",
    "    batch_size is size of batch (number of samples)\n",
    "    seq_length represents the length of sequence\n",
    "    vocab_size is number of words in vocabulary (assumes one-hot encoding)\n",
    "    \"\"\"\n",
    "    # Store input parameters.\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._seq_length = seq_length\n",
    "    self._vocab_size = vocab_size\n",
    "    # Divide text into segments depending on number of batches, each segment determines a cursor position for a batch.\n",
    "    segment = self._text_size // batch_size\n",
    "    # Set initial cursor position.\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    # Store first \"letter batch\".\n",
    "    self._last_letter_batch = self._next_letter_batch()\n",
    "  \n",
    "  def _next_letter_batch(self):\n",
    "    \"\"\"\n",
    "    Returns a batch containing of encoded single letters depending on the current batch \n",
    "    cursor positions in the data.\n",
    "    Returned \"letter batch\" is of size batch_size x vocab_size\n",
    "    \"\"\"\n",
    "    letter_batch = np.zeros(shape=(self._batch_size, self._vocab_size), dtype=np.float)\n",
    "    # Iterate through \"samples\"\n",
    "    for b in range(self._batch_size):\n",
    "      # Set 1 in position pointed out by one-hot char encoding.\n",
    "      letter_batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return letter_batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    # First add last letter from previous batch (the \"additional one\").\n",
    "    batches = [self._last_letter_batch]\n",
    "    for step in range(self._seq_length):\n",
    "      batches.append(self._next_letter_batch())\n",
    "    # Store last \"letter batch\" for next batch.\n",
    "    self._last_letter_batch = batches[-1]\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "(100, 59)\n"
     ]
    }
   ],
   "source": [
    "# Trick - override first 10 chars\n",
    "#list1 = list(train_text)\n",
    "#for i in range(2):\n",
    "#    list1[i] = 'z'\n",
    "#train_text = ''.join(list1)\n",
    "#print(\"Train set =\", train_text[0:100])\n",
    "\n",
    "# Create objects for training, validation and testing batch generation.\n",
    "train_batches = BatchGenerator(train_text, BATCH_SIZE, SEQ_LENGTH, vocabulary_size)\n",
    "\n",
    "# Get first training batch.\n",
    "batch = train_batches.next()\n",
    "print(len(batch))\n",
    "print(batch[0].shape)\n",
    "#print(\"Batch = \", batch)\n",
    "#print(batches2string(batch))\n",
    "#print(\"batch len = num of enrollings\",len(batch))\n",
    "#for i in range(num_unrollings):\n",
    "#    print(\"i = \", i, \"letter=\", batches2string(batch)[0][i][0], \"bits = \", batch[i][0])\n",
    "\n",
    "\n",
    "# For validation  - process the whole text as one big batch.\n",
    "VALID_BATCH_SIZE = int(np.floor(valid_size/SEQ_LENGTH))\n",
    "valid_batches = BatchGenerator(valid_text, VALID_BATCH_SIZE, SEQ_LENGTH, vocabulary_size)\n",
    "valid_batch = valid_batches.next()\n",
    "#print (VALID_BATCH_SIZE)\n",
    "#print(len(valid_batch))\n",
    "#print(valid_batch[0].shape)\n",
    "\n",
    "# For texting  - process the whole text as one big batch.\n",
    "TEST_BATCH_SIZE = int(np.floor(test_size/SEQ_LENGTH))\n",
    "test_batches = BatchGenerator(test_text, TEST_BATCH_SIZE, SEQ_LENGTH, vocabulary_size)\n",
    "# Get single batch! \n",
    "test_batch = test_batches.next()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function defining the LSTM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state, name):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        # Calculate gates activations.\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib, name=\"Input_gate\")\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb, name=\"Forget_gate\")\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob, name=\"Output_gate\")\n",
    "\n",
    "        update = tf.add(tf.matmul(i, cx), tf.matmul(o, cm) + cb, name=\"Update\")\n",
    "        state = tf.add(forget_gate * state, input_gate * tf.tanh(update), name=\"State_update\")\n",
    "        output = output_gate * tf.tanh(state)\n",
    "        return output, state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Definition of tensor graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_buffer shape = <unknown>\n",
      "Seq length  = 20\n",
      "Batch shape = <unknown>\n",
      "20\n",
      "<unknown>\n",
      "<unknown>\n"
     ]
    }
   ],
   "source": [
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 0. Shared variables ops.\n",
    "with tf.name_scope(\"Shared_Variables\"):\n",
    "  # Define parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1), name=\"ix\")\n",
    "  im = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"im\")\n",
    "  ib = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"ib\")\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1), name=\"fx\")\n",
    "  fm = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"fm\")\n",
    "  fb = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"fb\")\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1), name=\"cx\")\n",
    "  cm = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"cm\")\n",
    "  cb = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"cb\")\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1), name=\"ox\")\n",
    "  om = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"om\")\n",
    "  ob = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"ob\")\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, vocabulary_size], -0.1, 0.1), name=\"w\")\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]), name=\"b\")\n",
    "\n",
    "  # Placeholders for previous (the oldest) state and output.\n",
    "  prev_output = tf.placeholder(tf.float32, shape=None, name=\"prev_output\")\n",
    "  prev_state = tf.placeholder(tf.float32, shape=None, name=\"prev_state\")\n",
    "\n",
    "with tf.name_scope(\"Training\"): \n",
    "    # 0. Placeholders for inputs.\n",
    "    with tf.name_scope(\"Input_data\"):\n",
    "      # Define input data buffers.\n",
    "      input_buffer = list()\n",
    "      for _ in range(SEQ_LENGTH + 1):\n",
    "        # Collect placeholders for inputs/labels.\n",
    "        input_buffer.append(tf.placeholder(tf.float32, shape=None, name=\"Input_data\"))\n",
    "      print (\"input_buffer shape =\", input_buffer[0].shape)\n",
    "      # Collection of training inputs.\n",
    "      train_inputs = input_buffer[:SEQ_LENGTH]\n",
    "      # Labels are pointing to the same placeholders!\n",
    "      # Labels are inputs shifted by one time step.\n",
    "      train_labels = input_buffer[1:]  \n",
    "      print (\"Seq length  =\", len(train_inputs))\n",
    "      print (\"Batch shape =\", train_inputs[0].shape)\n",
    "      # Concatenate targets into 2D tensor.\n",
    "      targets = tf.concat(train_labels, 0)\n",
    "\n",
    "     # 2. Training LSTM ops.\n",
    "    with tf.name_scope(\"LSTM\"):\n",
    "      # Unrolled LSTM loop.\n",
    "      # Build outpus of size SEQ_LENGTH.\n",
    "      outputs = list()\n",
    "      output = prev_output\n",
    "      state = prev_state\n",
    "      for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state, \"cell\")\n",
    "        outputs.append(output)\n",
    "      print (len(outputs))\n",
    "      print (outputs[0].shape)\n",
    "      print (tf.concat(outputs, 0).shape)\n",
    "\n",
    "    # Fully connected layer on top => classification.\n",
    "    # In fact we will create lots of FC layers (one for each output layer), with shared weights.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b, name = \"Final_FC\")\n",
    "\n",
    "    # 2. Loss ops.\n",
    "    with tf.name_scope(\"Loss\"):\n",
    "        # Loss function(s) - one for every output generated by every lstm cell.\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=targets, logits=logits))\n",
    "        # Add loss summary.\n",
    "        loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    # 3. Training ops.  \n",
    "    with tf.name_scope(\"Optimization\"):\n",
    "      # Optimizer-related variables.\n",
    "      global_step = tf.Variable(0)\n",
    "      learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "      optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "      gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "      gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "      optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # 4. Predictions ops.  \n",
    "    with tf.name_scope(\"Evaluation\") as scope:\n",
    "      # Predictions.\n",
    "      train_prediction = tf.nn.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgraph responsible for generation of sample texts, char by char."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Sample_generation\") as scope:\n",
    "  # Create graphs for sampling and validation evaluation: batch 1, \"no unrolling\".\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size], name=\"Input_data\")\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"Output_data\")\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"Hidden_state\")\n",
    "\n",
    "  # Node responsible for resetting the state and output.\n",
    "  reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, HIDDEN_SIZE])),\n",
    "      saved_sample_state.assign(tf.zeros([1, HIDDEN_SIZE])))\n",
    "  # Single LSTM cell.\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state, \"cell\")\n",
    "  # Output depends on the hidden state.\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b, name=\"logits\"), name=\"outputs\")\n",
    "\n",
    "# Merge all summaries.\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "# 4. Init global variable.\n",
    "init = tf.global_variables_initializer() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for language generation (letter sampling etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_feed_dict(dataset):\n",
    "  \"\"\"Creates a dictionaries for different sets: maps data onto Tensor placeholders.\"\"\"\n",
    "  feed_dict = dict()\n",
    "  if dataset==\"train\":\n",
    "    # Get next batch and create a feed dict.\n",
    "    next_batch = train_batches.next()\n",
    "    for i in range(SEQ_LENGTH + 1):\n",
    "        feed_dict[input_buffer[i]] = next_batch[i]\n",
    "    # Reset previous state and output\n",
    "    feed_dict[prev_output] = np.zeros([BATCH_SIZE, HIDDEN_SIZE])\n",
    "    feed_dict[prev_state] = np.zeros([BATCH_SIZE, HIDDEN_SIZE])\n",
    "        \n",
    "  elif dataset==\"valid\":\n",
    "    for i in range(SEQ_LENGTH + 1):\n",
    "        feed_dict[input_buffer[i]] = valid_batch[i]\n",
    "    # Reset previous state and output\n",
    "    feed_dict[prev_output] = np.zeros([VALID_BATCH_SIZE, HIDDEN_SIZE])\n",
    "    feed_dict[prev_state] = np.zeros([VALID_BATCH_SIZE, HIDDEN_SIZE])\n",
    "    \n",
    "  else: # test\n",
    "    for i in range(SEQ_LENGTH + 1):\n",
    "        feed_dict[input_buffer[i]] = test_batch[i]\n",
    "    # Reset previous state and output\n",
    "    feed_dict[prev_output] = np.zeros([TEST_BATCH_SIZE, HIDDEN_SIZE])\n",
    "    feed_dict[prev_state] = np.zeros([TEST_BATCH_SIZE, HIDDEN_SIZE])\n",
    "    \n",
    "  return feed_dict # {prev_output: train_output_zeros, prev_state: train_state_zeros }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Eventually clear the log dir.\n",
    "if tf.gfile.Exists(LOG_DIR):\n",
    "  tf.gfile.DeleteRecursively(LOG_DIR)\n",
    "# Create (new) log dir.\n",
    "tf.gfile.MakeDirs(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Number of iterations per epoch = 2550\n",
      "Training set BPC at step 0: 4.07763 learning rate: 10.000000\n",
      "================================================================================\n",
      "jco ^Y_Z R VKoHeGcUX ukywaDdqjn hOB wypSPr`qB  eWg jnceh Hstt_chJuip]  yWKIP Fo\\\n",
      "Vqelne^_e ce^fffiiy^eCp x^  iLU[ykgRfkfK  elfGUAn  _LrN  EiyvamovefGF mjlh\\WPevp\n",
      "HttVgjBoilUrBu lP wgaH JoT n  lSuyHXC^Ynito HJhVtswhyelN m BB  e XnXfXmO Zy fs`C\n",
      "FvuipoPeDv  DiCdZqcpVvoFWKcCkm[wTvZdYe Na[S Ue uxItn  EkOghLEu  gmpMtoGaOiT`Kg  \n",
      "_lOTU [io [bWwpI^v fcTIGuckDsz[ui tQzEuqVQGas rYkCyHaub_QafDa`FsMMhFY aWWh hNJgn\n",
      "================================================================================\n",
      "Validation set BPC: 3.42081\n",
      "Training set BPC at step 10: 3.40826 learning rate: 10.000000\n",
      "Training set BPC at step 20: 2.92294 learning rate: 10.000000\n",
      "Training set BPC at step 30: 2.83488 learning rate: 10.000000\n",
      "Training set BPC at step 40: 2.78968 learning rate: 10.000000\n",
      "Training set BPC at step 50: 2.63227 learning rate: 10.000000\n",
      "Training set BPC at step 60: 2.55843 learning rate: 10.000000\n",
      "Training set BPC at step 70: 2.52271 learning rate: 10.000000\n",
      "Training set BPC at step 80: 2.42334 learning rate: 10.000000\n",
      "Training set BPC at step 90: 2.41920 learning rate: 10.000000\n",
      "Training set BPC at step 100: 2.33583 learning rate: 10.000000\n",
      "================================================================================\n",
      "Matilader jagigy thalalTgc  harseu   N the pithe athr torev doretes tualbincs  u\n",
      "Rapels tha qed  Nsthosm s ons  ios thice incico  ung orofers   ialg ppragejtlif \n",
      "xer scithecint urkgs thatt silae  catilh sacd thiis paxIcs otoer talrstonaprecyv\n",
      "Dsate toprares ack s fimer wererioy redUes na f thitk caca t orhy  he te thlvest\n",
      "Klepston a nitou thidudey un  Aug tus  copoderuttyin  tallots mo ne munki the we\n",
      "================================================================================\n",
      "Validation set BPC: 2.31410\n",
      "Training set BPC at step 110: 2.48762 learning rate: 10.000000\n",
      "Training set BPC at step 120: 2.23948 learning rate: 10.000000\n",
      "Training set BPC at step 130: 2.27775 learning rate: 10.000000\n",
      "Training set BPC at step 140: 2.22706 learning rate: 10.000000\n",
      "Training set BPC at step 150: 2.27237 learning rate: 10.000000\n",
      "Training set BPC at step 160: 2.21003 learning rate: 10.000000\n",
      "Training set BPC at step 170: 2.13696 learning rate: 10.000000\n",
      "Training set BPC at step 180: 2.17216 learning rate: 10.000000\n",
      "Training set BPC at step 190: 2.11007 learning rate: 10.000000\n",
      "Training set BPC at step 200: 2.11432 learning rate: 10.000000\n",
      "================================================================================\n",
      "zpedcald in lentu hat dise hot putorties ingul co c stont Tent N of thast gurk s\n",
      "Yed  unk  in whin    jote tre heth N N   N spend ya  ink  now the clad  in by mu\n",
      "Tit it hos thans pusicis incrersle al ud nowacs  unk  to s  unk  restort a butin\n",
      "]ed of  ho agsenk  unk  to N  unk  clap pomongs pr agopect of that buses and as \n",
      "X rues  no  unk  mo bervee bouner sompentino radw in tham a N N wescomaater iv i\n",
      "================================================================================\n",
      "Validation set BPC: 2.12191\n",
      "Training set BPC at step 210: 2.08439 learning rate: 10.000000\n",
      "Training set BPC at step 220: 2.11092 learning rate: 10.000000\n",
      "Training set BPC at step 230: 2.11344 learning rate: 10.000000\n",
      "Training set BPC at step 240: 2.08236 learning rate: 10.000000\n",
      "Training set BPC at step 250: 2.07468 learning rate: 10.000000\n",
      "Training set BPC at step 260: 2.00930 learning rate: 10.000000\n",
      "Training set BPC at step 270: 2.05387 learning rate: 10.000000\n",
      "Training set BPC at step 280: 1.95242 learning rate: 10.000000\n",
      "Training set BPC at step 290: 1.97721 learning rate: 10.000000\n",
      "Training set BPC at step 300: 2.05276 learning rate: 10.000000\n",
      "================================================================================\n",
      "Tndergenkllss aboleders isthorXimir fom promiclant offoldidlditillies thear erfr\n",
      "trosts poupviondy than ilshomporis wexh arsensextayelliess to sompencteds buthed\n",
      "uded a sodeds joshidinederteshingllitibelhios theimentsirping the coillon N the \n",
      "lliegedentiols and N asgifasing prionted thatsyagamievion now tharthats promedue\n",
      "beck biFaghis qucker andibre notheddnssysedgents anthais waiksatollyowtler puedi\n",
      "================================================================================\n",
      "Validation set BPC: 2.06322\n",
      "Training set BPC at step 310: 2.03471 learning rate: 10.000000\n",
      "Training set BPC at step 320: 1.99259 learning rate: 10.000000\n",
      "Training set BPC at step 330: 1.96029 learning rate: 10.000000\n",
      "Training set BPC at step 340: 1.96607 learning rate: 10.000000\n",
      "Training set BPC at step 350: 1.94573 learning rate: 10.000000\n",
      "Training set BPC at step 360: 1.93513 learning rate: 10.000000\n",
      "Training set BPC at step 370: 1.96815 learning rate: 10.000000\n",
      "Training set BPC at step 380: 2.02345 learning rate: 10.000000\n",
      "Training set BPC at step 390: 1.93843 learning rate: 10.000000\n",
      "Training set BPC at step 400: 1.97517 learning rate: 10.000000\n",
      "================================================================================\n",
      "Aedment fow anduchrdifu coppenatay a fow hased fecasane of indeld to saJ and min\n",
      "Cdacth andhe sores mo isheding   at andass   thavee dushtem for a costante finul\n",
      "isyers nebevetfred teconusegnan my to man o the heach batemal encaei     unk  su\n",
      "Vesia coma tht ree stromegornd inits rarge   cusitaty it leply abots te reweleg \n",
      "Mey digett   is of cose chamvivivingation ancuute mank N lobiars ceweve futerset\n",
      "================================================================================\n",
      "Validation set BPC: 1.92680\n",
      "Training set BPC at step 410: 1.88499 learning rate: 10.000000\n",
      "Training set BPC at step 420: 1.87110 learning rate: 10.000000\n",
      "Training set BPC at step 430: 1.97970 learning rate: 10.000000\n",
      "Training set BPC at step 440: 1.93076 learning rate: 10.000000\n",
      "Training set BPC at step 450: 1.88028 learning rate: 10.000000\n",
      "Training set BPC at step 460: 1.90176 learning rate: 10.000000\n",
      "Training set BPC at step 470: 1.85140 learning rate: 10.000000\n",
      "Training set BPC at step 480: 1.94066 learning rate: 10.000000\n",
      "Training set BPC at step 490: 1.91095 learning rate: 10.000000\n",
      "Training set BPC at step 500: 1.81800 learning rate: 10.000000\n",
      "================================================================================\n",
      "N awe gondad forars wowe ip investiest N on morech a farge   N a lite of has   s\n",
      "Rnatident candest of jompic vader foutAer browieltions the jaingind say jointer \n",
      "Fence officent it in uts lagnenge orbent wan whice uchunatial panice to busin is\n",
      "Yiveter rere unke hated conre mivet rine veal restance  unk  wolis   sumk gion e\n",
      "Dinat jan\\er   ppercosing  unk  eall was aygay finaizsty fuesso site foranter u \n",
      "================================================================================\n",
      "Validation set BPC: 1.84361\n",
      "Training set BPC at step 510: 1.91017 learning rate: 10.000000\n",
      "Training set BPC at step 520: 1.87962 learning rate: 10.000000\n",
      "Training set BPC at step 530: 1.73632 learning rate: 10.000000\n",
      "Training set BPC at step 540: 1.78965 learning rate: 10.000000\n",
      "Training set BPC at step 550: 1.81369 learning rate: 10.000000\n",
      "Training set BPC at step 560: 1.85578 learning rate: 10.000000\n",
      "Training set BPC at step 570: 1.80713 learning rate: 10.000000\n",
      "Training set BPC at step 580: 1.82274 learning rate: 10.000000\n",
      "Training set BPC at step 590: 1.82307 learning rate: 10.000000\n",
      "Training set BPC at step 600: 1.84294 learning rate: 10.000000\n",
      "================================================================================\n",
      "nd secolling saist ary stwowle loter have racting exeurtch lame redort in pullio\n",
      "this calarny wich Ruch and wele   weZtions heass alduic plave casibich  unk  wis\n",
      "y the  ugk    N litcluritica infure bill lege prs  farters  smesieated theys com\n",
      " N of out are which to e preating afdaver with hupo porxaid propitially realdits\n",
      "Z offertay bohamanies rotalws yenk  sen lije as paituter  unk   unk  ploch sump \n",
      "================================================================================\n",
      "Validation set BPC: 1.80384\n",
      "Training set BPC at step 610: 1.83354 learning rate: 10.000000\n",
      "Training set BPC at step 620: 1.77661 learning rate: 10.000000\n",
      "Training set BPC at step 630: 1.77700 learning rate: 10.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set BPC at step 640: 1.77400 learning rate: 10.000000\n",
      "Training set BPC at step 650: 1.76828 learning rate: 10.000000\n",
      "Training set BPC at step 660: 1.77279 learning rate: 10.000000\n",
      "Training set BPC at step 670: 1.80810 learning rate: 10.000000\n",
      "Training set BPC at step 680: 1.74194 learning rate: 10.000000\n",
      "Training set BPC at step 690: 1.74860 learning rate: 10.000000\n",
      "Training set BPC at step 700: 1.80321 learning rate: 10.000000\n",
      "================================================================================\n",
      "Lition in recust   callying rustle anverso was depagint ingomers   dusissivinysh\n",
      "l N lited N liclided the  unk  dextiwningay compent op heplan profidamed conts s\n",
      "Tamtray in   s syslut jainalds other infusedningrafted says  unk  withs dettin  \n",
      "D cench an   mhilly the costacy and vounk   condicte plosigatt puspeding exectit\n",
      "o alvect from that it   poliesliday and say preral  unk  pooce than aver hack   \n",
      "================================================================================\n",
      "Validation set BPC: 1.76656\n",
      "Training set BPC at step 710: 1.71809 learning rate: 10.000000\n",
      "Training set BPC at step 720: 1.73249 learning rate: 10.000000\n",
      "Training set BPC at step 730: 1.74934 learning rate: 10.000000\n",
      "Training set BPC at step 740: 1.73634 learning rate: 10.000000\n",
      "Training set BPC at step 750: 1.75466 learning rate: 10.000000\n",
      "Training set BPC at step 760: 1.70567 learning rate: 10.000000\n",
      "Training set BPC at step 770: 1.70526 learning rate: 10.000000\n",
      "Training set BPC at step 780: 1.76202 learning rate: 10.000000\n",
      "Training set BPC at step 790: 1.68909 learning rate: 10.000000\n",
      "Training set BPC at step 800: 1.79874 learning rate: 10.000000\n",
      "================================================================================\n",
      "Lricis was been by two sues expcaiming stleysale was earner N crater hope compan\n",
      "Qenth adqapo ligages to  unk  as fited cass flimies new a saare and chablacth co\n",
      "Gy to  unk  the mavore u spate is dois   N mill of t million said not and   whil\n",
      "igner fation withor in the said bangh N N indiciral appeaded to k  sce ane newen\n",
      "suars is of saye heneudic he who steck  he  unk  of more hower   a with the furs\n",
      "================================================================================\n",
      "Validation set BPC: 1.72975\n",
      "Training set BPC at step 810: 1.71111 learning rate: 10.000000\n",
      "Training set BPC at step 820: 1.71039 learning rate: 10.000000\n",
      "Training set BPC at step 830: 1.67585 learning rate: 10.000000\n",
      "Training set BPC at step 840: 1.73402 learning rate: 10.000000\n",
      "Training set BPC at step 850: 1.74375 learning rate: 10.000000\n",
      "Training set BPC at step 860: 1.71146 learning rate: 10.000000\n",
      "Training set BPC at step 870: 1.74848 learning rate: 10.000000\n",
      "Training set BPC at step 880: 1.71133 learning rate: 10.000000\n",
      "Training set BPC at step 890: 1.65256 learning rate: 10.000000\n",
      "Training set BPC at step 900: 1.70089 learning rate: 10.000000\n",
      "================================================================================\n",
      "Kle sharnonth iztermon adomportizies  unk  and ans ectere to coure gived lerndy \n",
      "the cents to neat that bold york   the   mana egobitates than atticl loars he hu\n",
      "thes qoot the lave to mexthemice neepausem zurebold in the effined by statces te\n",
      "Ye beits and  unk  vedust one mr  geel caref leatecuiated ther of at olly is pur\n",
      "Vsters could be  unk  itforkiamong who reeverate sumpliem beens unitive door car\n",
      "================================================================================\n",
      "Validation set BPC: 1.71294\n",
      "Training set BPC at step 910: 1.73529 learning rate: 10.000000\n",
      "Training set BPC at step 920: 1.74719 learning rate: 10.000000\n",
      "Training set BPC at step 930: 1.75394 learning rate: 10.000000\n",
      "Training set BPC at step 940: 1.71000 learning rate: 10.000000\n",
      "Training set BPC at step 950: 1.75181 learning rate: 10.000000\n",
      "Training set BPC at step 960: 1.67650 learning rate: 10.000000\n",
      "Training set BPC at step 970: 1.72374 learning rate: 10.000000\n",
      "Training set BPC at step 980: 1.71452 learning rate: 10.000000\n",
      "Training set BPC at step 990: 1.70725 learning rate: 10.000000\n",
      "Training set BPC at step 1000: 1.70593 learning rate: 10.000000\n",
      "================================================================================\n",
      "ormer   sompenirations infor deren that a fambesle na nitions a shresiccase  unk\n",
      "^ anal were  unk  bays tradirgcomal compits businest  s better equich  unk  assu\n",
      "_ stwls the contrast way s fremmany like to share c tell in N to same investy  u\n",
      "lbardind fins   bis againung gucobbillions spart a nott expinutical of to trange\n",
      "Aich inttence by with norrien thise ressocp  scaumed trad toviy we subsed mr  th\n",
      "================================================================================\n",
      "Validation set BPC: 1.69250\n",
      "Training set BPC at step 1010: 1.72383 learning rate: 10.000000\n",
      "Training set BPC at step 1020: 1.71689 learning rate: 10.000000\n",
      "Training set BPC at step 1030: 1.79291 learning rate: 10.000000\n",
      "Training set BPC at step 1040: 1.65946 learning rate: 10.000000\n",
      "Training set BPC at step 1050: 1.65229 learning rate: 10.000000\n",
      "Training set BPC at step 1060: 1.61435 learning rate: 10.000000\n",
      "Training set BPC at step 1070: 1.68092 learning rate: 10.000000\n",
      "Training set BPC at step 1080: 1.67961 learning rate: 10.000000\n",
      "Training set BPC at step 1090: 1.66575 learning rate: 10.000000\n",
      "Training set BPC at step 1100: 1.68675 learning rate: 10.000000\n",
      "================================================================================\n",
      "Ject for madert ply  unk  coveinment indestines in out and mmofivernations the d\n",
      "Terfollospeds   anules whoperh orsent out  unk  statement anlysrate prositional \n",
      "xition to caladerparystees tewhas burlwerederagit wholed  s onexcidely to buyh a\n",
      "erees bureing the deccests to a hish on dires   the diousteradieges the vearfers\n",
      "mith investruccake resing ogeign goard settromes as at the jurthted the N N last\n",
      "================================================================================\n",
      "Validation set BPC: 1.68020\n",
      "Training set BPC at step 1110: 1.65559 learning rate: 10.000000\n",
      "Training set BPC at step 1120: 1.63528 learning rate: 10.000000\n",
      "Training set BPC at step 1130: 1.72951 learning rate: 10.000000\n",
      "Training set BPC at step 1140: 1.60237 learning rate: 10.000000\n",
      "Training set BPC at step 1150: 1.68915 learning rate: 10.000000\n",
      "Training set BPC at step 1160: 1.64208 learning rate: 10.000000\n",
      "Training set BPC at step 1170: 1.67500 learning rate: 10.000000\n",
      "Training set BPC at step 1180: 1.61308 learning rate: 10.000000\n",
      "Training set BPC at step 1190: 1.74527 learning rate: 10.000000\n",
      "Training set BPC at step 1200: 1.69725 learning rate: 10.000000\n",
      "================================================================================\n",
      "becue is be an after of sord is wear  unk  kors and cousts ordese produging comp\n",
      "Sy the lonsoming are thet lertabor ser couls reganted cormon but the  unk  beanv\n",
      "jo boukr stares without for to dederzandn rot whether probloss to news for stake\n",
      "vestions to ray was   low fuel aser of these  unk  he dy   in moverairailain of \n",
      "e banket were issuef sidules  unk  be and veproligation of besuged a year  umk  \n",
      "================================================================================\n",
      "Validation set BPC: 1.65943\n",
      "Training set BPC at step 1210: 1.67741 learning rate: 10.000000\n",
      "Training set BPC at step 1220: 1.68285 learning rate: 10.000000\n",
      "Training set BPC at step 1230: 1.63593 learning rate: 10.000000\n",
      "Training set BPC at step 1240: 1.62806 learning rate: 10.000000\n",
      "Training set BPC at step 1250: 1.65062 learning rate: 10.000000\n",
      "Training set BPC at step 1260: 1.62289 learning rate: 10.000000\n",
      "Training set BPC at step 1270: 1.65781 learning rate: 10.000000\n",
      "Training set BPC at step 1280: 1.65161 learning rate: 10.000000\n",
      "Training set BPC at step 1290: 1.64050 learning rate: 10.000000\n",
      "Training set BPC at step 1300: 1.63832 learning rate: 10.000000\n",
      "================================================================================\n",
      "O who makingsoy to N  unk  and his mor   weeders companima   co  the industly th\n",
      "\\th prodgctif  unk    te marking to u s to a someonical down to sopene expenment\n",
      "just youk n t fom the airlion who has a   whene respure infore qurence  unk  co \n",
      "U of  unk  the lastoving thewget loogs to suppo toms ret a gairs about has mearn\n",
      "E fince expled informomes dodal muchay houson torkbnajion is high disalls blorns\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set BPC: 1.64320\n",
      "Training set BPC at step 1310: 1.63667 learning rate: 10.000000\n",
      "Training set BPC at step 1320: 1.59861 learning rate: 10.000000\n",
      "Training set BPC at step 1330: 1.61632 learning rate: 10.000000\n",
      "Training set BPC at step 1340: 1.63322 learning rate: 10.000000\n",
      "Training set BPC at step 1350: 1.68898 learning rate: 10.000000\n",
      "Training set BPC at step 1360: 1.64631 learning rate: 10.000000\n",
      "Training set BPC at step 1370: 1.59415 learning rate: 10.000000\n",
      "Training set BPC at step 1380: 1.64951 learning rate: 10.000000\n",
      "Training set BPC at step 1390: 1.63444 learning rate: 10.000000\n",
      "Training set BPC at step 1400: 1.64114 learning rate: 10.000000\n",
      "================================================================================\n",
      "Qaman months  unk  indexate there priss for  unk  cabila an adversants devally v\n",
      "[ating incorts other discorcenvices   a bois prevery chenany but   tharen pemmer\n",
      "pexfes coredrecurdif in to  unk    lat earnele achuesment   N billions sated no \n",
      "S to earned N bongs  unk  brationay an expert the  unk  to  unk  pridis was dusi\n",
      "Futions in resald exence N N dearle zegraw s fre will  unk  confedpall fatual  u\n",
      "================================================================================\n",
      "Validation set BPC: 1.63192\n",
      "Training set BPC at step 1410: 1.65647 learning rate: 10.000000\n",
      "Training set BPC at step 1420: 1.60207 learning rate: 10.000000\n",
      "Training set BPC at step 1430: 1.61496 learning rate: 10.000000\n",
      "Training set BPC at step 1440: 1.59224 learning rate: 10.000000\n",
      "Training set BPC at step 1450: 1.67154 learning rate: 10.000000\n",
      "Training set BPC at step 1460: 1.52277 learning rate: 10.000000\n",
      "Training set BPC at step 1470: 1.61416 learning rate: 10.000000\n",
      "Training set BPC at step 1480: 1.58007 learning rate: 10.000000\n",
      "Training set BPC at step 1490: 1.57903 learning rate: 10.000000\n",
      "Training set BPC at step 1500: 1.64494 learning rate: 10.000000\n",
      "================================================================================\n",
      "vice to deficants guosonaltent infsuring destillion perpacial sheal fundhe tombi\n",
      "E spequely be from a jush were pecearue on short betefits mipler relues and net \n",
      "Jush intexsided the statandion one finany funds of besting offueads hoent said c\n",
      "zens w d    it under the at the  unk  of defices ise  unk srates brakes apleate \n",
      "jorter the pence for tokviety insestors for bame dividena mong betimit really   \n",
      "================================================================================\n",
      "Validation set BPC: 1.62665\n",
      "Training set BPC at step 1510: 1.65205 learning rate: 10.000000\n",
      "Training set BPC at step 1520: 1.64137 learning rate: 10.000000\n",
      "Training set BPC at step 1530: 1.62256 learning rate: 10.000000\n",
      "Training set BPC at step 1540: 1.62246 learning rate: 10.000000\n",
      "Training set BPC at step 1550: 1.58689 learning rate: 10.000000\n",
      "Training set BPC at step 1560: 1.64918 learning rate: 10.000000\n",
      "Training set BPC at step 1570: 1.63035 learning rate: 10.000000\n",
      "Training set BPC at step 1580: 1.55465 learning rate: 10.000000\n",
      "Training set BPC at step 1590: 1.56389 learning rate: 10.000000\n",
      "Training set BPC at step 1600: 1.57580 learning rate: 10.000000\n",
      "================================================================================\n",
      "gened street wornd most that leveld   are restoluriating henorth gerelt tumploop\n",
      "overs operate service the at the feveref havents between state that atter  unk  \n",
      "ped bills volum   m  may and someaster there far put of then the feels close cap\n",
      "R there anvasted inain thinded relader offers    unk  haves yorkers ordant jarty\n",
      "`o N tollewalls a  unk  to here and there ansuilizate   do reseluets the seevelu\n",
      "================================================================================\n",
      "Validation set BPC: 1.61726\n",
      "Training set BPC at step 1610: 1.64953 learning rate: 10.000000\n",
      "Training set BPC at step 1620: 1.61568 learning rate: 10.000000\n",
      "Training set BPC at step 1630: 1.58944 learning rate: 10.000000\n",
      "Training set BPC at step 1640: 1.58276 learning rate: 10.000000\n",
      "Training set BPC at step 1650: 1.68172 learning rate: 10.000000\n",
      "Training set BPC at step 1660: 1.57185 learning rate: 10.000000\n",
      "Training set BPC at step 1670: 1.60291 learning rate: 10.000000\n",
      "Training set BPC at step 1680: 1.53451 learning rate: 10.000000\n",
      "Training set BPC at step 1690: 1.62125 learning rate: 10.000000\n",
      "Training set BPC at step 1700: 1.62207 learning rate: 10.000000\n",
      "================================================================================\n",
      "Cren since of ats  unk  be but ngnemitter clablerch than stovecual mr  stop trad\n",
      "V^ence maker  unk  the stmilsh consumsing to cussitors markani uco  unk  in dust\n",
      "billion suts a N N deth  unk  million arvide the was earlies tu nablay eet anter\n",
      "king it a u s   unk  dibal until   staticl praters in rive hiscommend other tu s\n",
      "t to forue mafustrial canted   the with about three ko it  unk  arecorr restocia\n",
      "================================================================================\n",
      "Validation set BPC: 1.60328\n",
      "Training set BPC at step 1710: 1.67252 learning rate: 10.000000\n",
      "Training set BPC at step 1720: 1.63950 learning rate: 10.000000\n",
      "Training set BPC at step 1730: 1.63102 learning rate: 10.000000\n",
      "Training set BPC at step 1740: 1.57464 learning rate: 10.000000\n",
      "Training set BPC at step 1750: 1.61864 learning rate: 10.000000\n",
      "Training set BPC at step 1760: 1.63093 learning rate: 10.000000\n",
      "Training set BPC at step 1770: 1.60584 learning rate: 10.000000\n",
      "Training set BPC at step 1780: 1.54866 learning rate: 10.000000\n",
      "Training set BPC at step 1790: 1.62293 learning rate: 10.000000\n",
      "Training set BPC at step 1800: 1.54826 learning rate: 10.000000\n",
      "================================================================================\n",
      "Cing will octormes he   N chathormathost is rate as unfter the discal frew more \n",
      "Ileas   mugy is while advertional of bay the ifsurion an excince to reacy a new \n",
      "ll notheig the eefen its   with for millinglrome bextines   the sumporting beedw\n",
      "` is n t stakes problem build notervieus of perlinupo     by one investors they \n",
      "\\   with to the yeared net my reprices anyer commsury ko rehanvit retuin u   s n\n",
      "================================================================================\n",
      "Validation set BPC: 1.59141\n",
      "Training set BPC at step 1810: 1.62256 learning rate: 10.000000\n",
      "Training set BPC at step 1820: 1.56823 learning rate: 10.000000\n",
      "Training set BPC at step 1830: 1.59601 learning rate: 10.000000\n",
      "Training set BPC at step 1840: 1.58179 learning rate: 10.000000\n",
      "Training set BPC at step 1850: 1.57172 learning rate: 10.000000\n",
      "Training set BPC at step 1860: 1.63847 learning rate: 10.000000\n",
      "Training set BPC at step 1870: 1.61889 learning rate: 10.000000\n",
      "Training set BPC at step 1880: 1.57044 learning rate: 10.000000\n",
      "Training set BPC at step 1890: 1.61284 learning rate: 10.000000\n",
      "Training set BPC at step 1900: 1.62055 learning rate: 10.000000\n",
      "================================================================================\n",
      "Qitirs trass as gound have porglewy intorally to seganment befords said no poble\n",
      "Iort into beinver  unk  in the lawor mage   new look delasing  unk   ro tapionif\n",
      "promest notyer reseirgh company this   whill in will a ps   it to armarring s ea\n",
      "Y court of the unal but sevory trader toospon down three the ngrer everman theoo\n",
      "Ve brongc  turnically pacces tradewing leading mark but is N yield the vivitions\n",
      "================================================================================\n",
      "Validation set BPC: 1.58462\n",
      "Training set BPC at step 1910: 1.59017 learning rate: 10.000000\n",
      "Training set BPC at step 1920: 1.58834 learning rate: 10.000000\n",
      "Training set BPC at step 1930: 1.55274 learning rate: 10.000000\n",
      "Training set BPC at step 1940: 1.57424 learning rate: 10.000000\n",
      "Training set BPC at step 1950: 1.57363 learning rate: 10.000000\n",
      "Training set BPC at step 1960: 1.63205 learning rate: 10.000000\n",
      "Training set BPC at step 1970: 1.54930 learning rate: 10.000000\n",
      "Training set BPC at step 1980: 1.63035 learning rate: 10.000000\n",
      "Training set BPC at step 1990: 1.56646 learning rate: 10.000000\n",
      "Training set BPC at step 2000: 1.55661 learning rate: 10.000000\n",
      "================================================================================\n",
      " one producked anound of as the covers becortanie worrther   the bay a prodocial\n",
      "zed thelike of time and secial of are issued simprage areanter whiphee inclains \n",
      "he it was statulys to eas markets insteetionly buyer was quartice attarly corp c\n",
      "uting to N N instoral dideal from the contreatouling figsh in the frompatet thre\n",
      "]entaged short  years the consiscue arezed has the bused killion are terllertal \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set BPC: 1.58632\n",
      "Training set BPC at step 2010: 1.65046 learning rate: 10.000000\n",
      "Training set BPC at step 2020: 1.54430 learning rate: 10.000000\n",
      "Training set BPC at step 2030: 1.55681 learning rate: 10.000000\n",
      "Training set BPC at step 2040: 1.61538 learning rate: 10.000000\n",
      "Training set BPC at step 2050: 1.52029 learning rate: 10.000000\n",
      "Training set BPC at step 2060: 1.53551 learning rate: 10.000000\n",
      "Training set BPC at step 2070: 1.54732 learning rate: 10.000000\n",
      "Training set BPC at step 2080: 1.58058 learning rate: 10.000000\n",
      "Training set BPC at step 2090: 1.62267 learning rate: 10.000000\n",
      "Training set BPC at step 2100: 1.59905 learning rate: 10.000000\n",
      "================================================================================\n",
      "aralls semination    unk  ays  stock and quartero ling   seate   cheenst as year\n",
      " employs   fourations   N   by post that   N billion becausee langus plan purrit\n",
      "Yue off parts sidge and among to one burgin a shipted about ray he be  unk  sybl\n",
      "T  s mestlucts workgn adsencortion to it fuss   and  unk    two  unk  for three \n",
      "Host an lestation a share in state   long theye shick base senater wasken   me c\n",
      "================================================================================\n",
      "Validation set BPC: 1.57862\n",
      "Training set BPC at step 2110: 1.60606 learning rate: 10.000000\n",
      "Training set BPC at step 2120: 1.62830 learning rate: 10.000000\n",
      "Training set BPC at step 2130: 1.61758 learning rate: 10.000000\n",
      "Training set BPC at step 2140: 1.60188 learning rate: 10.000000\n",
      "Training set BPC at step 2150: 1.56600 learning rate: 10.000000\n",
      "Training set BPC at step 2160: 1.60187 learning rate: 10.000000\n",
      "Training set BPC at step 2170: 1.55827 learning rate: 10.000000\n",
      "Training set BPC at step 2180: 1.60999 learning rate: 10.000000\n",
      "Training set BPC at step 2190: 1.54316 learning rate: 10.000000\n",
      "Training set BPC at step 2200: 1.67976 learning rate: 10.000000\n",
      "================================================================================\n",
      "R mr  goot he yee  unk  bairoganks for cunalerations durnems to a longadyonce re\n",
      "be for the hove n t have r wlerner work some has u s   unk  currons  and compech\n",
      "Eteld forechanshipter  s robext outrows  unk  for is rudged one is stock which s\n",
      "unk  acrunk reback onal off   units cradls casinally guilding  unk  mr  fort ont\n",
      "nt obleauiraty werk and that at N as takion taxcis corp new usees while the week\n",
      "================================================================================\n",
      "Validation set BPC: 1.57676\n",
      "Training set BPC at step 2210: 1.59157 learning rate: 10.000000\n",
      "Training set BPC at step 2220: 1.50543 learning rate: 10.000000\n",
      "Training set BPC at step 2230: 1.56002 learning rate: 10.000000\n",
      "Training set BPC at step 2240: 1.54462 learning rate: 10.000000\n",
      "Training set BPC at step 2250: 1.58436 learning rate: 10.000000\n",
      "Training set BPC at step 2260: 1.54648 learning rate: 10.000000\n",
      "Training set BPC at step 2270: 1.56119 learning rate: 10.000000\n",
      "Training set BPC at step 2280: 1.55916 learning rate: 10.000000\n",
      "Training set BPC at step 2290: 1.54099 learning rate: 10.000000\n",
      "Training set BPC at step 2300: 1.57001 learning rate: 10.000000\n",
      "================================================================================\n",
      "llicution N   volt wonld ome to  unk  to fasiriat for pursomicums   N venued bil\n",
      "Zmends all andws are canceion oblatet detilie by supplions the suppers federiman\n",
      "Z   inte a N monofniy in  unk  subsemosoun N a plo edeam at industrially concert\n",
      "Kort  unk  down desume veitatics in samples to becalif from used   his gebray   \n",
      "veryment are somp  schedulations fallocd enta weet on meamining one and  s  s ci\n",
      "================================================================================\n",
      "Validation set BPC: 1.56943\n",
      "Training set BPC at step 2310: 1.58583 learning rate: 10.000000\n",
      "Training set BPC at step 2320: 1.51982 learning rate: 10.000000\n",
      "Training set BPC at step 2330: 1.53985 learning rate: 10.000000\n",
      "Training set BPC at step 2340: 1.59510 learning rate: 10.000000\n",
      "Training set BPC at step 2350: 1.58921 learning rate: 10.000000\n",
      "Training set BPC at step 2360: 1.54654 learning rate: 10.000000\n",
      "Training set BPC at step 2370: 1.60984 learning rate: 10.000000\n",
      "Training set BPC at step 2380: 1.53545 learning rate: 10.000000\n",
      "Training set BPC at step 2390: 1.62756 learning rate: 10.000000\n",
      "Training set BPC at step 2400: 1.56982 learning rate: 10.000000\n",
      "================================================================================\n",
      "S N   no the  unk  announced nores third woming their to rillerily bads from N N\n",
      "y   edelilative and N damagement and the crovomment   be long repurted abericely\n",
      "Ee of the  unk  face be profiel merril will n t quintey it equelly goldan for fa\n",
      "N thet in about an is transing puarced seg w unk  flanal ariles quather receigal\n",
      "J can of the reclais by are for N N trades in the  unk  announced the stock rose\n",
      "================================================================================\n",
      "Validation set BPC: 1.56100\n",
      "Training set BPC at step 2410: 1.46822 learning rate: 10.000000\n",
      "Training set BPC at step 2420: 1.52903 learning rate: 10.000000\n",
      "Training set BPC at step 2430: 1.57277 learning rate: 10.000000\n",
      "Training set BPC at step 2440: 1.51632 learning rate: 10.000000\n",
      "Training set BPC at step 2450: 1.52577 learning rate: 10.000000\n",
      "Training set BPC at step 2460: 1.57172 learning rate: 10.000000\n",
      "Training set BPC at step 2470: 1.63287 learning rate: 10.000000\n",
      "Training set BPC at step 2480: 1.58761 learning rate: 10.000000\n",
      "Training set BPC at step 2490: 1.49690 learning rate: 10.000000\n",
      "Training set BPC at step 2500: 1.56049 learning rate: 10.000000\n",
      "================================================================================\n",
      "and under madnatial load unsereficay from the stocks for right group collitonspe\n",
      "]eing aver transy avnued that changest  with hubjik never who  unk  N N N N loan\n",
      "L wenker mass cortrogeted N N N to gins fegle  shargin   cut at storagar of the \n",
      "l N and junk had deworiks plans its enious   busales   the addo is would dis   u\n",
      "V the  unk    the  unk  workd stock an  emploctled buy the first were examilorat\n",
      "================================================================================\n",
      "Validation set BPC: 1.56114\n",
      "Training set BPC at step 2510: 1.52711 learning rate: 10.000000\n",
      "Training set BPC at step 2520: 1.58439 learning rate: 10.000000\n",
      "Training set BPC at step 2530: 1.54937 learning rate: 10.000000\n",
      "Training set BPC at step 2540: 1.53630 learning rate: 10.000000\n",
      "Calculating BPC on test dataset\n",
      "Final test set BPC: 1.52736\n"
     ]
    }
   ],
   "source": [
    "# How often the test loss on validation batch will be computed. \n",
    "summary_frequency = 100\n",
    "\n",
    "# Create session.\n",
    "sess = tf.InteractiveSession()\n",
    "# Create summary writers, point them to LOG_DIR.\n",
    "train_writer = tf.summary.FileWriter(LOG_DIR + '/train', sess.graph)\n",
    "valid_writer = tf.summary.FileWriter(LOG_DIR + '/valid')\n",
    "test_writer = tf.summary.FileWriter(LOG_DIR + '/test')\n",
    "\n",
    "# Initialize global variables.\n",
    "tf.global_variables_initializer().run()\n",
    "print('Initialized')\n",
    "\n",
    "num_steps =  train_size // (BATCH_SIZE*SEQ_LENGTH) #70001\n",
    "print(\"Number of iterations per epoch =\", num_steps)\n",
    "for step in range(num_steps):\n",
    "    # Run training graph.\n",
    "    batch = train_batches.next()\n",
    "    summary, _, t_loss, lr = sess.run([merged_summaries, optimizer, loss, learning_rate], \n",
    "                                      feed_dict=create_feed_dict(\"train\"))\n",
    "    # Add summary.\n",
    "    train_writer.add_summary(summary, step*SEQ_LENGTH)\n",
    "    train_writer.flush()\n",
    "\n",
    "    # Every (100) steps collect statistics.\n",
    "    if step % summary_frequency == 0:\n",
    "      # Print loss from last batch.\n",
    "      print('Training set BPC at step %d: %0.5f learning rate: %f' % (step, t_loss, lr))\n",
    "    \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate sample text...\n",
    "        print('=' * 80)\n",
    "        # consisting of 5 lines...\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          # Reset LSTM hidden state.\n",
    "          reset_sample_state.run()\n",
    "          # with 79 characters in each.\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "        \n",
    "        # Validation set BPC.\n",
    "        v_summary, v_loss = sess.run([merged_summaries, loss], feed_dict=create_feed_dict(\"valid\"))\n",
    "        print(\"Validation set BPC: %.5f\" % v_loss)\n",
    "        valid_writer.add_summary(v_summary, step*SEQ_LENGTH)\n",
    "        valid_writer.flush()\n",
    "    # End of statistics collection\n",
    "\n",
    "# Test set BPC.\n",
    "print(\"Calculating BPC on test dataset\")\n",
    "t_summary, t_loss = sess.run([merged_summaries, loss], feed_dict=create_feed_dict(\"test\"))\n",
    "print(\"Final test set BPC: %.5f\" % t_loss)\n",
    "test_writer.add_summary(t_summary, step*SEQ_LENGTH)\n",
    "test_writer.flush()\n",
    "    \n",
    "# Close writers and session.\n",
    "train_writer.close()\n",
    "valid_writer.close()\n",
    "test_writer.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
