{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import tarfile\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import shutil \n",
    "import random\n",
    "\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "# Dirs - must be absolute paths!\n",
    "LOG_DIR = '/tmp/tf/ptb_char_lstm/'\n",
    "# Local dir where PTB files will be stored.\n",
    "PTB_DIR = '/home/tkornuta/data/ptb/'\n",
    "\n",
    "# Filenames.\n",
    "TRAIN = \"ptb.train.txt\"\n",
    "VALID = \"ptb.valid.txt\"\n",
    "TEST = \"ptb.test.txt\"\n",
    "\n",
    "# A single recurrent layer of 2000 units\n",
    "#NUM_UNITS = 100\n",
    "# Size of the hidden state 64\n",
    "HIDDEN_SIZE = 64\n",
    "\n",
    "# A batch size of 100\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Sequences of length 200 bytes\n",
    "SEQ_LENGTH = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check/maybe download PTB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified /home/tkornuta/data/ptb/simple-examples.tgz ( 34869662 )\n"
     ]
    }
   ],
   "source": [
    "def maybe_download_ptb(path, \n",
    "                       filename='simple-examples.tgz', \n",
    "                       url='http://www.fit.vutbr.cz/~imikolov/rnnlm/', \n",
    "                       expected_bytes =34869662):\n",
    "  # Eventually create the PTB dir.\n",
    "  if not tf.gfile.Exists(path):\n",
    "    tf.gfile.MakeDirs(path)\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  _filename = path+filename\n",
    "  if not os.path.exists(_filename):\n",
    "    print('Downloading %s...' % filename)\n",
    "    _filename, _ = urlretrieve(url+filename, _filename)\n",
    "  statinfo = os.stat(_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', (_filename), '(', statinfo.st_size, ')')\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + _filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download_ptb(PTB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract dataset-related files from the PTB archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_ptb(path, filename='simple-examples.tgz', files=[\"ptb.train.txt\", \"ptb.valid.txt\", \"ptb.test.txt\", \n",
    "                                       \"ptb.char.train.txt\", \"ptb.char.valid.txt\", \"ptb.char.test.txt\"]):\n",
    "    \"\"\"Extracts files from PTB archive.\"\"\"\n",
    "    # Extract\n",
    "    tar = tarfile.open(path+filename)\n",
    "    tar.extractall(path)\n",
    "    tar.close()\n",
    "    # Copy files\n",
    "    for file in files:\n",
    "        shutil.copyfile(PTB_DIR+\"simple-examples/data/\"+file, PTB_DIR+file)\n",
    "    # Delete directory\n",
    "    shutil.rmtree(PTB_DIR+\"simple-examples/\")        \n",
    "\n",
    "extract_ptb(PTB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train, valid and test texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5101618  aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memote\n",
      "399782  consumers may want to move their telephones a little closer to \n",
      "449945  no it was n't black monday \n",
      " but while the new york stock excha\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename, path):\n",
    "    with open(path+filename, 'r') as myfile:\n",
    "        data=myfile.read()# .replace('\\n', '')\n",
    "        return data\n",
    "\n",
    "train_text = read_data(TRAIN, PTB_DIR)\n",
    "train_size=len(train_text)\n",
    "print(train_size, train_text[:100])\n",
    "\n",
    "valid_text = read_data(VALID, PTB_DIR)\n",
    "valid_size=len(valid_text)\n",
    "print(valid_size, valid_text[:64])\n",
    "\n",
    "test_text = read_data(TEST, PTB_DIR)\n",
    "test_size=len(test_text)\n",
    "print(test_size, test_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size =  59\n",
      "65\n",
      "33 1 58 26 0 0\n",
      "a A\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 59 # [A-Z] + [a-z] + ' ' +few 'in between; + punctuation\n",
    "first_letter = ord(string.ascii_uppercase[0]) # ascii_uppercase before lowercase! \n",
    "print(\"vocabulary size = \", vocabulary_size)\n",
    "print(first_letter)\n",
    "\n",
    "def char2id(char):\n",
    "  \"\"\" Converts char to id (int) with one-hot encoding handling of unexpected characters\"\"\"\n",
    "  if char in string.ascii_letters:# or char in string.punctuation or char in string.digits:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    # print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  \"\"\" Converts single id (int) to character\"\"\"\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "#print(len(string.punctuation))\n",
    "#for i in string.ascii_letters:\n",
    "#    print (i, char2id(i))\n",
    "\n",
    "\n",
    "print(char2id('a'), char2id('A'), char2id('z'), char2id('Z'), char2id(' '), char2id('Ã¯'))\n",
    "print(id2char(char2id('a')), id2char(char2id('A')))\n",
    "#print(id2char(65), id2char(33), id2char(90), id2char(58), id2char(0))\n",
    "#bankno\n",
    "sample = np.zeros(shape=(1, vocabulary_size), dtype=np.float)\n",
    "sample[0, char2id(' ')] = 1.0\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper class for batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, seq_length, vocab_size):\n",
    "    \"\"\"\n",
    "    Initializes the batch generator object. Stores the variables and first \"letter batch\".\n",
    "    text is text to be processed\n",
    "    batch_size is size of batch (number of samples)\n",
    "    seq_length represents the length of sequence\n",
    "    vocab_size is number of words in vocabulary (assumes one-hot encoding)\n",
    "    \"\"\"\n",
    "    # Store input parameters.\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._seq_length = seq_length\n",
    "    self._vocab_size = vocab_size\n",
    "    # Divide text into segments depending on number of batches, each segment determines a cursor position for a batch.\n",
    "    segment = self._text_size // batch_size\n",
    "    # Set initial cursor position.\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    # Store first \"letter batch\".\n",
    "    self._last_letter_batch = self._next_letter_batch()\n",
    "  \n",
    "  def _next_letter_batch(self):\n",
    "    \"\"\"\n",
    "    Returns a batch containing of encoded single letters depending on the current batch \n",
    "    cursor positions in the data.\n",
    "    Returned \"letter batch\" is of size batch_size x vocab_size\n",
    "    \"\"\"\n",
    "    letter_batch = np.zeros(shape=(self._batch_size, self._vocab_size), dtype=np.float)\n",
    "    # Iterate through \"samples\"\n",
    "    for b in range(self._batch_size):\n",
    "      # Set 1 in position pointed out by one-hot char encoding.\n",
    "      letter_batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return letter_batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    # First add last letter from previous batch (the \"additional one\").\n",
    "    batches = [self._last_letter_batch]\n",
    "    for step in range(self._seq_length):\n",
    "      batches.append(self._next_letter_batch())\n",
    "    # Store last \"letter batch\" for next batch.\n",
    "    self._last_letter_batch = batches[-1]\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set =  aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memote\n",
      "[' aer b', 'ere de', 'nts co', 'has ju', 'n N ac', 'upon r', ' tradi', 'ed the', 'rge of', 'nk  th', 's goin', 'levels', 'ays  u', 'ng los', 'rain  ', 'l bidd', 'lieves', 'elters', 'nk  my', 'ransac', ' of fi', 'ay its', 'commis', 'assed ', 'ld los', 'panies', ' mr  r', 'd mr  ', 've use', ' headq', ' sir j', 'ral ga', 'd the ', 'point ', ' which', 'learly', 'ested ', ' unk  ', 'e was ', 'o  unk', 'act th', 'idend ', 'e chil', 'mortga', 'icago ', 'd curr', 'rns ab', 'ysts a', 'r incl', 'er mes', 'millio', 'ars wo', 'nk  be', 'unkin ', 'g the ', 'nounce', 'p to t', 'has a ', 'ey say', ' outsi', '   jus', 'to rea', 'e fund', 'onth f', 'th an ', 'tegy b', 'd thei', 's guil', 'sumer ', 'd whic', 'ns of ', 'ritish', 'aurant', 'k    t', 'ensati', 'rd and', 'such t', 'before', 'east e', 'ailure', 'olving', 'attemp', 'ts   t', 'third ', 'e diff', 'tself ', 'l for ', 'er  s ', 'ery tr', 'i can ', 'he kra', 'med ca', 'ican s', 'oncili', ' thoma', 'alance', 'uring ', 'ularit', 'bt sec', 'the tr']\n"
     ]
    }
   ],
   "source": [
    "# Trick - override first 10 chars\n",
    "#list1 = list(train_text)\n",
    "#for i in range(2):\n",
    "#    list1[i] = 'z'\n",
    "#train_text = ''.join(list1)\n",
    "print(\"Train set =\", train_text[0:100])\n",
    "\n",
    "# Create objects for training, validation and testing batch generation.\n",
    "train_batches = BatchGenerator(train_text, BATCH_SIZE, SEQ_LENGTH, vocabulary_size)\n",
    "# The latter two - threat whole text as a batch.\n",
    "valid_batches = BatchGenerator(valid_text, valid_size, 1, vocabulary_size)\n",
    "test_batches = BatchGenerator(test_text, test_size, 1, vocabulary_size)\n",
    "\n",
    "# Get first training batch.\n",
    "batch = train_batches.next()\n",
    "#print(\"Batch = \", batch)\n",
    "print(batches2string(batch))\n",
    "#print(\"batch len = num of enrollings\",len(batch))\n",
    "#for i in range(num_unrollings):\n",
    "#    print(\"i = \", i, \"letter=\", batches2string(batch)[0][i][0], \"bits = \", batch[i][0])\n",
    "\n",
    "# Generate input-target pairs for validation set.\n",
    "valid_batch = valid_batches.next()[0]\n",
    "valid_batch_input = valid_batch[:valid_size-1]\n",
    "valid_batch_target = valid_batch[1:valid_size]\n",
    "\n",
    "# Generate input-target pairs for validation set.\n",
    "test_batch = test_batches.next()[0]\n",
    "test_batch_input = test_batch[:test_size-1]\n",
    "test_batch_target = test_batch[1:test_size]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function defining the LSTM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state, name):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        # Calculate gates activations.\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib, name=\"Input_gate\")\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb, name=\"Forget_gate\")\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob, name=\"Output_gate\")\n",
    "\n",
    "        update = tf.add(tf.matmul(i, cx), tf.matmul(o, cm) + cb, name=\"Update\")\n",
    "        state = tf.add(forget_gate * state, input_gate * tf.tanh(update), name=\"State_update\")\n",
    "        output = output_gate * tf.tanh(state)\n",
    "        return output, state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Definition of tensor graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(100, 59)\n",
      "5\n",
      "(100, 64)\n",
      "(500, 64)\n"
     ]
    }
   ],
   "source": [
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 0. Shared variables ops.\n",
    "with tf.name_scope(\"Shared_Variables\"):\n",
    "  # Define parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1), name=\"ix\")\n",
    "  im = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"im\")\n",
    "  ib = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"ib\")\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1), name=\"fx\")\n",
    "  fm = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"fm\")\n",
    "  fb = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"fb\")\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1), name=\"cx\")\n",
    "  cm = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"cm\")\n",
    "  cb = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"cb\")\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1), name=\"ox\")\n",
    "  om = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"om\")\n",
    "  ob = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"ob\")\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([BATCH_SIZE, HIDDEN_SIZE]), trainable=False, name=\"saved_output\")\n",
    "  saved_state = tf.Variable(tf.zeros([BATCH_SIZE, HIDDEN_SIZE]), trainable=False, name=\"saved_state\")\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, vocabulary_size], -0.1, 0.1), name=\"w\")\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]), name=\"b\")\n",
    "  \n",
    "with tf.name_scope(\"Training\"): \n",
    "    # 0. Placeholders for inputs.\n",
    "    with tf.name_scope(\"Input_data\"):\n",
    "      # Define input data buffers.\n",
    "      train_data = list()\n",
    "      for _ in range(SEQ_LENGTH + 1):\n",
    "        # Collect placeholders for inputs/labels.\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[BATCH_SIZE,vocabulary_size], name=\"Input_data\"))\n",
    "      # Collection of training inputs.\n",
    "      train_inputs = train_data[:SEQ_LENGTH]\n",
    "      # Labels are pointing to the same placeholders!\n",
    "      # Labels are inputs shifted by one time step.\n",
    "      train_labels = train_data[1:]  \n",
    "      print (len(train_inputs))\n",
    "      print (train_inputs[0].shape)\n",
    "      # Concatenate targets into 2D tensor.\n",
    "      targets = tf.concat(train_labels, 0)\n",
    "\n",
    "\n",
    "     # 2. Training LSTM ops.\n",
    "    with tf.name_scope(\"LSTM\"):\n",
    "      # Unrolled LSTM loop.\n",
    "      # Build outpus of size SEQ_LENGTH.\n",
    "      outputs = list()\n",
    "      output = saved_output\n",
    "      state = saved_state\n",
    "      for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state, \"cell\")\n",
    "        outputs.append(output)\n",
    "      print (len(outputs))\n",
    "      print (outputs[0].shape)\n",
    "      print (tf.concat(outputs, 0).shape)\n",
    "\n",
    "      # State saving across unrollings.\n",
    "      with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Fully connected layer on top => classification.\n",
    "        # In fact we will create lots of FC layers (one for each output layer), with shared weights.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "\n",
    "    # 2. Loss ops.\n",
    "    with tf.name_scope(\"Loss\"):\n",
    "        # Loss function(s) - one for every output generated by every lstm cell.\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=targets, logits=logits))\n",
    "        # Add loss summary.\n",
    "        loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    # 3. Training ops.  \n",
    "    with tf.name_scope(\"Optimization\"):\n",
    "      # Optimizer-related variables.\n",
    "      global_step = tf.Variable(0)\n",
    "      learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "      optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "      gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "      gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "      optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # 4. Predictions ops.  \n",
    "    with tf.name_scope(\"Evaluation\") as scope:\n",
    "      # Predictions.\n",
    "      train_prediction = tf.nn.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgraph responsible for generation of sample texts, char by char."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Sample_generation\") as scope:\n",
    "  # Create graphs for sampling and validation evaluation: batch 1, \"no unrolling\".\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size], name=\"Input_data\")\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"Output_data\")\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"Hidden_state\")\n",
    "\n",
    "  # Node responsible for resetting the state and output.\n",
    "  reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, HIDDEN_SIZE])),\n",
    "      saved_sample_state.assign(tf.zeros([1, HIDDEN_SIZE])))\n",
    "  # Single LSTM cell.\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state, \"cell\")\n",
    "  # Output depends on the hidden state.\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b, name=\"logits\"), name=\"outputs\")\n",
    "\n",
    "# Merge all summaries.\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "# 4. Init global variable.\n",
    "init = tf.global_variables_initializer() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgraph responsible for validation (1-char)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Validation-1char\"):\n",
    "    with tf.name_scope(\"Input_data\"):\n",
    "      # Define buffers.\n",
    "      valid_inputs = tf.placeholder(tf.float32, shape=[valid_size-1,vocabulary_size], name=\"Data\")\n",
    "      valid_targets = tf.placeholder(tf.float32, shape=[valid_size-1,vocabulary_size], name=\"Target\")\n",
    "\n",
    "    with tf.name_scope(\"LSTM\"):\n",
    "      prev_valid_outputs = tf.Variable(tf.zeros([valid_size-1, HIDDEN_SIZE]), name=\"Prev_Outputs\")\n",
    "      prev_valid_state = tf.Variable(tf.zeros([valid_size-1, HIDDEN_SIZE]), name=\"Prev_hidden_state\")\n",
    "\n",
    "      # Node responsible for resetting the state and output.\n",
    "      reset_valid_state = tf.group(\n",
    "          prev_valid_outputs.assign(tf.zeros([valid_size-1, HIDDEN_SIZE])),\n",
    "          prev_valid_state.assign(tf.zeros([valid_size-1, HIDDEN_SIZE])))\n",
    "\n",
    "      # Single LSTM cell.\n",
    "      valid_outputs, valid_state = lstm_cell(valid_inputs, prev_valid_outputs, prev_valid_state, \"Cell\")\n",
    "      # Output depends on the hidden state.\n",
    "      with tf.control_dependencies([prev_valid_outputs.assign(valid_outputs), prev_valid_state.assign(valid_state)]):\n",
    "        valid_logits = tf.nn.xw_plus_b(valid_outputs, w, b, name=\"Logits\")\n",
    "\n",
    "        # 2. Loss ops.\n",
    "    with tf.name_scope(\"Loss\"):\n",
    "        # Loss function(s) - one for every output generated by every lstm cell.\n",
    "        valid_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=valid_targets, logits=valid_logits))\n",
    "        # Add loss summary.\n",
    "        valid_loss_summary = tf.summary.scalar(\"loss\", valid_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgraph responsible for testing (1-char)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Test-1char\"):\n",
    "    with tf.name_scope(\"Input_data\"):\n",
    "      # Define buffers.\n",
    "      test_inputs = tf.placeholder(tf.float32, shape=[test_size-1,vocabulary_size], name=\"Data\")\n",
    "      test_targets = tf.placeholder(tf.float32, shape=[test_size-1,vocabulary_size], name=\"Target\")\n",
    "\n",
    "    with tf.name_scope(\"LSTM\"):\n",
    "      prev_test_outputs = tf.Variable(tf.zeros([test_size-1, HIDDEN_SIZE]), name=\"Prev_Outputs\")\n",
    "      prev_test_state = tf.Variable(tf.zeros([test_size-1, HIDDEN_SIZE]), name=\"Prev_hidden_state\")\n",
    "\n",
    "      # Node responsible for resetting the state and output.\n",
    "      reset_test_state = tf.group(\n",
    "          prev_test_outputs.assign(tf.zeros([test_size-1, HIDDEN_SIZE])),\n",
    "          prev_test_state.assign(tf.zeros([test_size-1, HIDDEN_SIZE])))\n",
    "\n",
    "      # Single LSTM cell.\n",
    "      test_outputs, test_state = lstm_cell(test_inputs, prev_test_outputs, prev_test_state, \"Cell\")\n",
    "      # Output depends on the hidden state.\n",
    "      with tf.control_dependencies([prev_test_outputs.assign(test_outputs), prev_test_state.assign(test_state)]):\n",
    "        test_logits = tf.nn.xw_plus_b(test_outputs, w, b, name=\"Logits\")\n",
    "\n",
    "        # 2. Loss ops.\n",
    "    with tf.name_scope(\"Loss\"):\n",
    "        # Loss function(s) - one for every output generated by every lstm cell.\n",
    "        test_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=test_targets, logits=test_logits))\n",
    "        # Add loss summary.\n",
    "        test_loss_summary = tf.summary.scalar(\"loss\", test_loss)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for language generation (letter sampling etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Eventually clear the log dir.\n",
    "if tf.gfile.Exists(LOG_DIR):\n",
    "  tf.gfile.DeleteRecursively(LOG_DIR)\n",
    "# Create (new) log dir.\n",
    "tf.gfile.MakeDirs(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Number of iterations per epoch = 10203\n",
      "Train set loss at step 0: 4.074362 learning rate: 10.000000\n",
      "================================================================================\n",
      "ohnhxIT`Yux fWooO vwan B ]dDrNnbtjh \\Tsw AMIik_ ioo rqeAzPOH[wNdPGeaX[Qhqa D EVb\n",
      "CaAlSk ^ae l HW ySbqSkuJkI[c[Y oae i YcinVW   HEdEad^m C[scebo olZB_NKSP]ns V[xO\n",
      "S ebIiM] VCQrvUeb_rHihXpylCM\\Dgdf]hEEtpLzjOV PemNy rrseptuZqew ItfnEM rkLnBIJ  r\n",
      "MGbgVAuytf`tgSnxqJPZE^I ]ll^hoX  VS zCLPtjhPsmGiOyALSX_JNoXK wroj j^woNUl RnPLwA\n",
      "Qeh[`nDQrnz tj]`YU [ oKgdsJrntvKorKs[V]BQhxOnItQ ZnItp xnfTrU\\p yDV DJ VZstoxcMe\n",
      "================================================================================\n",
      "Calculating BPC on validation dataset\n",
      "Validation dataset BPC: 3.46\n",
      "Train set loss at step 100: 2.343342 learning rate: 10.000000\n",
      "Train set loss at step 200: 2.102026 learning rate: 10.000000\n",
      "Train set loss at step 300: 2.130804 learning rate: 10.000000\n",
      "Train set loss at step 400: 1.977300 learning rate: 10.000000\n",
      "Train set loss at step 500: 1.944074 learning rate: 10.000000\n",
      "Train set loss at step 600: 1.881490 learning rate: 10.000000\n",
      "Train set loss at step 700: 1.931150 learning rate: 10.000000\n",
      "Train set loss at step 800: 1.860780 learning rate: 10.000000\n",
      "Train set loss at step 900: 1.949816 learning rate: 10.000000\n",
      "Train set loss at step 1000: 1.832120 learning rate: 10.000000\n",
      "================================================================================\n",
      "^es decacoity buld win wenker secures of u s  a plat to sain the goredustibncate\n",
      "U distilfes linte  unk  the  unk  or is compan impernivar doven the count in fri\n",
      "]ersion thaserates af urplife thak actian it a me with jort  unk  eacer swan   c\n",
      "H the was apvetlelenther dond noun   pare navers thay weve rimate of pioke corpl\n",
      "`ond ]rounderliching monthy hot con to mr  the fercempertipf whese adpet in n ba\n",
      "================================================================================\n",
      "Calculating BPC on validation dataset\n",
      "Validation dataset BPC: 2.75\n",
      "Train set loss at step 1100: 1.762435 learning rate: 10.000000\n",
      "Train set loss at step 1200: 1.625440 learning rate: 10.000000\n",
      "Train set loss at step 1300: 1.646681 learning rate: 10.000000\n",
      "Train set loss at step 1400: 1.717030 learning rate: 10.000000\n",
      "Train set loss at step 1500: 1.692931 learning rate: 10.000000\n",
      "Train set loss at step 1600: 1.750500 learning rate: 10.000000\n",
      "Train set loss at step 1700: 1.707001 learning rate: 10.000000\n",
      "Train set loss at step 1800: 1.760699 learning rate: 10.000000\n",
      "Train set loss at step 1900: 1.796278 learning rate: 10.000000\n",
      "Train set loss at step 2000: 1.684627 learning rate: 10.000000\n",
      "================================================================================\n",
      "F  unk  over periols or that   fillion whe kull   month jrember admine   i motes\n",
      "Hust an  unk  of dolle marken    up   the lia   N a suithy gate exportinest   on\n",
      "ch on N aysc everityy s  unding recict of the as in   on brast  the aminampurtop\n",
      "Kes   who betion  unk    dmanct in from i moobly shorw it wan sare   alren apfy \n",
      "Midg  unk  the cange muntirthes has the it this equipenfin huice mr   unk  in  s\n",
      "================================================================================\n",
      "Calculating BPC on validation dataset\n",
      "Validation dataset BPC: 2.80\n",
      "Train set loss at step 2100: 1.785499 learning rate: 10.000000\n",
      "Train set loss at step 2200: 1.637986 learning rate: 10.000000\n",
      "Train set loss at step 2300: 1.672261 learning rate: 10.000000\n",
      "Train set loss at step 2400: 1.559689 learning rate: 10.000000\n",
      "Train set loss at step 2500: 1.615998 learning rate: 10.000000\n",
      "Train set loss at step 2600: 1.714335 learning rate: 10.000000\n",
      "Train set loss at step 2700: 1.660785 learning rate: 10.000000\n",
      "Train set loss at step 2800: 1.664991 learning rate: 10.000000\n",
      "Train set loss at step 2900: 1.663275 learning rate: 10.000000\n",
      "Train set loss at step 3000: 1.715144 learning rate: 10.000000\n",
      "================================================================================\n",
      "ch parepand has and  unk  rumps comieitrution of sches   the   acporobliage by a\n",
      "Ro iser develonds trading failars goin pow sexcel siacl people of for invosmest \n",
      "jon commentsyer investorters and becence mowec takeling for met read gure or not\n",
      "jorfed dighthed ford sumperplicates and seg boid bankel poversanding represe are\n",
      "kated  unk  truy and officials in of than frinwer befit  s pabor be agangey tome\n",
      "================================================================================\n",
      "Calculating BPC on validation dataset\n",
      "Validation dataset BPC: 2.86\n",
      "Train set loss at step 3100: 1.695244 learning rate: 10.000000\n",
      "Train set loss at step 3200: 1.638591 learning rate: 10.000000\n",
      "Train set loss at step 3300: 1.556438 learning rate: 10.000000\n",
      "Train set loss at step 3400: 1.646544 learning rate: 10.000000\n",
      "Train set loss at step 3500: 1.614748 learning rate: 10.000000\n",
      "Train set loss at step 3600: 1.614799 learning rate: 10.000000\n",
      "Train set loss at step 3700: 1.825585 learning rate: 10.000000\n",
      "Train set loss at step 3800: 1.634757 learning rate: 10.000000\n",
      "Train set loss at step 3900: 1.634134 learning rate: 10.000000\n",
      "Train set loss at step 4000: 1.625515 learning rate: 10.000000\n",
      "================================================================================\n",
      "her and u s  a would in as other to boer vill the  unk  houble at   what growtec\n",
      "kes and the a of cover to the  unk  and reby threw the no doublits of the fast d\n",
      "H expour n t was rimation was   t N responsiniaray  for cosming that earle liedh\n",
      "zqer poced to  unk  day expectes    unk  severation lomed deach sliking proftein\n",
      "quirs it it  unk  to will  unk   unk  progudeEved of fiven oppl  unk    i suchul\n",
      "================================================================================\n",
      "Calculating BPC on validation dataset\n",
      "Validation dataset BPC: 2.89\n",
      "Train set loss at step 4100: 1.582976 learning rate: 10.000000\n",
      "Train set loss at step 4200: 1.436467 learning rate: 10.000000\n",
      "Train set loss at step 4300: 1.601370 learning rate: 10.000000\n",
      "Train set loss at step 4400: 1.393823 learning rate: 10.000000\n",
      "Train set loss at step 4500: 1.548151 learning rate: 10.000000\n",
      "Train set loss at step 4600: 1.651853 learning rate: 10.000000\n",
      "Train set loss at step 4700: 1.606478 learning rate: 10.000000\n",
      "Train set loss at step 4800: 1.558556 learning rate: 10.000000\n",
      "Train set loss at step 4900: 1.716129 learning rate: 10.000000\n",
      "Train set loss at step 5000: 1.574723 learning rate: 1.000000\n",
      "================================================================================\n",
      "P you the kreess an is lawnucixios accorded disine to mr  bourd of  unk  commess\n",
      "er trice edon dollar  s   if mark both lemed pshed   indec tervili the   a dead \n",
      "ounced at this growes is eppectivised devices renine hees expects a mr   unk  of\n",
      "Mation    unr  uns  expert of the use fastersived to  unk  new reclan mither arl\n",
      "Vent under   the jurpmerd pille betledarly he early fyecters will of idee so  un\n",
      "================================================================================\n",
      "Calculating BPC on validation dataset\n",
      "Validation dataset BPC: 2.89\n",
      "Train set loss at step 5100: 1.500819 learning rate: 1.000000\n",
      "Train set loss at step 5200: 1.617423 learning rate: 1.000000\n",
      "Train set loss at step 5300: 1.565830 learning rate: 1.000000\n",
      "Train set loss at step 5400: 1.512065 learning rate: 1.000000\n",
      "Train set loss at step 5500: 1.534173 learning rate: 1.000000\n",
      "Train set loss at step 5600: 1.557944 learning rate: 1.000000\n",
      "Train set loss at step 5700: 1.560664 learning rate: 1.000000\n",
      "Train set loss at step 5800: 1.583705 learning rate: 1.000000\n",
      "Train set loss at step 5900: 1.604454 learning rate: 1.000000\n",
      "Train set loss at step 6000: 1.640994 learning rate: 1.000000\n",
      "================================================================================\n",
      "ring but trackations withor  s fund expect for infusiad of at enter that a  unk \n",
      "er dist rac mandial nate zules  inally than thombl gordeys unit fure dimitimacra\n",
      "her it a  unkid for afteratemanss relentuler about firm the nems  s resurm    un\n",
      "cholum for it early company   N million to the the rainge an   procrid and was s\n",
      "jost in own buy flocfartirals a pention og fas N mendaters from into  unk  would\n",
      "================================================================================\n",
      "Calculating BPC on validation dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset BPC: 2.90\n",
      "Train set loss at step 6100: 1.392810 learning rate: 1.000000\n",
      "Train set loss at step 6200: 1.543541 learning rate: 1.000000\n",
      "Train set loss at step 6300: 1.530344 learning rate: 1.000000\n",
      "Train set loss at step 6400: 1.470701 learning rate: 1.000000\n",
      "Train set loss at step 6500: 1.469506 learning rate: 1.000000\n",
      "Train set loss at step 6600: 1.543541 learning rate: 1.000000\n",
      "Train set loss at step 6700: 1.479972 learning rate: 1.000000\n",
      "Train set loss at step 6800: 1.679105 learning rate: 1.000000\n",
      "Train set loss at step 6900: 1.634920 learning rate: 1.000000\n",
      "Train set loss at step 7000: 1.468356 learning rate: 1.000000\n",
      "================================================================================\n",
      "H N will crumises market year companies heldorned what said fly bein and the gol\n",
      "Hen time lederw  unk  are live fourther N and some and momised maker  unk  are g\n",
      "er bank exclays qualic N million mish to add of  unk  fornic in he week and it s\n",
      "nes sell the possiuntivelstem expeciins to debew indlativi probueminiisluctridit\n",
      "Tever that a  unk  a  unk    unto recade share that for said N viil price deecho\n",
      "================================================================================\n",
      "Calculating BPC on validation dataset\n",
      "Validation dataset BPC: 2.89\n",
      "Train set loss at step 7100: 1.607322 learning rate: 1.000000\n",
      "Train set loss at step 7200: 1.470583 learning rate: 1.000000\n",
      "Train set loss at step 7300: 1.454008 learning rate: 1.000000\n",
      "Train set loss at step 7400: 1.629115 learning rate: 1.000000\n",
      "Train set loss at step 7500: 1.559768 learning rate: 1.000000\n",
      "Train set loss at step 7600: 1.604098 learning rate: 1.000000\n",
      "Train set loss at step 7700: 1.432479 learning rate: 1.000000\n",
      "Train set loss at step 7800: 1.612160 learning rate: 1.000000\n",
      "Train set loss at step 7900: 1.624908 learning rate: 1.000000\n",
      "Train set loss at step 8000: 1.462400 learning rate: 1.000000\n",
      "================================================================================\n",
      "I year ealling   it back the   saze deceivem a tend countrile begand analyst ans\n",
      "s out   his sqund   beciss   were bepat into  s that changet sulil chan estim   \n",
      "ber fure been tirg smitisceed need so presidit quat   economing early extcugure \n",
      "s ssomes of sall monky from a from tocisbud in mems  s and listrany increaved as\n",
      "zan to to stall may fran in the quartes   early the bicports mr  no action by bi\n",
      "================================================================================\n",
      "Calculating BPC on validation dataset\n",
      "Validation dataset BPC: 2.89\n",
      "Train set loss at step 8100: 1.430931 learning rate: 1.000000\n",
      "Train set loss at step 8200: 1.483184 learning rate: 1.000000\n",
      "Train set loss at step 8300: 1.552553 learning rate: 1.000000\n",
      "Train set loss at step 8400: 1.581470 learning rate: 1.000000\n",
      "Train set loss at step 8500: 1.586803 learning rate: 1.000000\n",
      "Train set loss at step 8600: 1.509070 learning rate: 1.000000\n",
      "Train set loss at step 8700: 1.544548 learning rate: 1.000000\n",
      "Train set loss at step 8800: 1.572991 learning rate: 1.000000\n",
      "Train set loss at step 8900: 1.557437 learning rate: 1.000000\n",
      "Train set loss at step 9000: 1.585263 learning rate: 1.000000\n",
      "================================================================================\n",
      "ves   blaination  unk  the gulledives in discarish includmem a shorts posited he\n",
      "Xance endual amida   the there he breal it depleting   in theirally projictdebul\n",
      "d relates market on u s   unk  bo   for the new wind   addon company swim the af\n",
      "croing  fors the assings optural the depore who thousts   analyss the bost said \n",
      "_oked sold andl facional for mr  lovemal massing ergets   its in president shutg\n",
      "================================================================================\n",
      "Calculating BPC on validation dataset\n",
      "Validation dataset BPC: 2.90\n",
      "Train set loss at step 9100: 1.447101 learning rate: 1.000000\n",
      "Train set loss at step 9200: 1.417893 learning rate: 1.000000\n",
      "Train set loss at step 9300: 1.564459 learning rate: 1.000000\n",
      "Train set loss at step 9400: 1.732820 learning rate: 1.000000\n",
      "Train set loss at step 9500: 1.592728 learning rate: 1.000000\n",
      "Train set loss at step 9600: 1.621567 learning rate: 1.000000\n",
      "Train set loss at step 9700: 1.437294 learning rate: 1.000000\n",
      "Train set loss at step 9800: 1.611054 learning rate: 1.000000\n",
      "Train set loss at step 9900: 1.545191 learning rate: 1.000000\n",
      "Train set loss at step 10000: 1.479189 learning rate: 0.100000\n",
      "================================================================================\n",
      "E lipeaciars dight but because of there for about the brua liallta that their so\n",
      "p on hidle maise agancand that fall teperal orden in the new by afters include d\n",
      "S hire daisnate for yex fird increaced on mord are noter new n y   N million app\n",
      "Ziem corp  what any mm a over will the  unk  skell boho equing  unk  kat spess a\n",
      "Qent bay wotk horelaw fmancented are he tromeld chr tuesnals is attens interes  \n",
      "================================================================================\n",
      "Calculating BPC on validation dataset\n",
      "Validation dataset BPC: 2.91\n",
      "Train set loss at step 10100: 1.443794 learning rate: 0.100000\n",
      "Train set loss at step 10200: 1.521274 learning rate: 0.100000\n",
      "Calculating BPC on test dataset\n",
      "Final test dataset BPC: 2.92\n"
     ]
    }
   ],
   "source": [
    "# How often the test loss on validation batch will be computed. \n",
    "summary_frequency = 100\n",
    "\n",
    "# Create session.\n",
    "sess = tf.InteractiveSession()\n",
    "# Create summary writers, point them to LOG_DIR.\n",
    "train_writer = tf.summary.FileWriter(LOG_DIR + '/train', sess.graph)\n",
    "valid_writer = tf.summary.FileWriter(LOG_DIR + '/valid')\n",
    "test_writer = tf.summary.FileWriter(LOG_DIR + '/test')\n",
    "\n",
    "# Initialize global variables.\n",
    "tf.global_variables_initializer().run()\n",
    "print('Initialized')\n",
    "\n",
    "num_steps =  train_size // (BATCH_SIZE*SEQ_LENGTH) #70001\n",
    "print(\"Number of iterations per epoch =\", num_steps)\n",
    "for step in range(num_steps):\n",
    "    # Get next batch and create a feed dict.\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(SEQ_LENGTH + 1):\n",
    "        feed_dict[train_data[i]] = batches[i]\n",
    "    # Run training graph.\n",
    "    summary, _, t_loss, lr = sess.run([merged_summaries, optimizer, loss, learning_rate], feed_dict=feed_dict)\n",
    "    # Add summary.\n",
    "    train_writer.add_summary(summary, step)\n",
    "    train_writer.flush()\n",
    "\n",
    "    # Every (100) steps collect statistics.\n",
    "    if step % summary_frequency == 0:\n",
    "      # Print loss from last batch.\n",
    "      print('Train set loss at step %d: %f learning rate: %f' % (step, t_loss, lr))\n",
    "    \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          # Reset LSTM hidden state.\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "        \n",
    "        # Validation set BPC.\n",
    "        reset_valid_state.run()\n",
    "        v_summary, v_loss = sess.run([valid_loss_summary, valid_loss],\n",
    "                                feed_dict={valid_inputs:valid_batch_input,valid_targets:valid_batch_target})\n",
    "        print(\"Validation dataset BPC: %.2f\" % v_loss)\n",
    "        valid_writer.add_summary(v_summary, step)\n",
    "        valid_writer.flush()\n",
    "    # End of statistics collection\n",
    "\n",
    "# Test set BPC.\n",
    "print(\"Calculating BPC on test dataset\")\n",
    "reset_test_state.run()\n",
    "t_summary, t_loss = sess.run([test_loss_summary, test_loss],\n",
    "                        feed_dict={test_inputs:test_batch_input,test_targets:test_batch_target})\n",
    "print(\"Final test dataset BPC: %.2f\" % t_loss)\n",
    "test_writer.add_summary(t_summary, step)\n",
    "test_writer.flush()\n",
    "    \n",
    "# Close writers and session.\n",
    "train_writer.close()\n",
    "valid_writer.close()\n",
    "test_writer.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
