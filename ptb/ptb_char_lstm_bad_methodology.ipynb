{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import tarfile\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import shutil \n",
    "import random\n",
    "\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "# Dirs - must be absolute paths!\n",
    "LOG_DIR = '/tmp/tf/ptb_char_lstm_bad/char100/'\n",
    "# Local dir where PTB files will be stored.\n",
    "PTB_DIR = '/home/tkornuta/data/ptb/'\n",
    "\n",
    "# Filenames.\n",
    "TRAIN = \"ptb.train.txt\"\n",
    "VALID = \"ptb.valid.txt\"\n",
    "TEST = \"ptb.test.txt\"\n",
    "\n",
    "# Size of the hidden state 64\n",
    "HIDDEN_SIZE = 64\n",
    "\n",
    "# A batch size of 100\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# A single recurrent layer of number of units = sequences of length\n",
    "# e.g. 200 bytes\n",
    "SEQ_LENGTH = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check/maybe download PTB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified /home/tkornuta/data/ptb/simple-examples.tgz ( 34869662 )\n"
     ]
    }
   ],
   "source": [
    "def maybe_download_ptb(path, \n",
    "                       filename='simple-examples.tgz', \n",
    "                       url='http://www.fit.vutbr.cz/~imikolov/rnnlm/', \n",
    "                       expected_bytes =34869662):\n",
    "  # Eventually create the PTB dir.\n",
    "  if not tf.gfile.Exists(path):\n",
    "    tf.gfile.MakeDirs(path)\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  _filename = path+filename\n",
    "  if not os.path.exists(_filename):\n",
    "    print('Downloading %s...' % filename)\n",
    "    _filename, _ = urlretrieve(url+filename, _filename)\n",
    "  statinfo = os.stat(_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', (_filename), '(', statinfo.st_size, ')')\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + _filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download_ptb(PTB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract dataset-related files from the PTB archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_ptb(path, filename='simple-examples.tgz', files=[\"ptb.train.txt\", \"ptb.valid.txt\", \"ptb.test.txt\", \n",
    "                                       \"ptb.char.train.txt\", \"ptb.char.valid.txt\", \"ptb.char.test.txt\"]):\n",
    "    \"\"\"Extracts files from PTB archive.\"\"\"\n",
    "    # Extract\n",
    "    tar = tarfile.open(path+filename)\n",
    "    tar.extractall(path)\n",
    "    tar.close()\n",
    "    # Copy files\n",
    "    for file in files:\n",
    "        shutil.copyfile(PTB_DIR+\"simple-examples/data/\"+file, PTB_DIR+file)\n",
    "    # Delete directory\n",
    "    shutil.rmtree(PTB_DIR+\"simple-examples/\")        \n",
    "\n",
    "extract_ptb(PTB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train, valid and test texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5101618  aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memote\n",
      "399782  consumers may want to move their telephones a little closer to \n",
      "449945  no it was n't black monday \n",
      " but while the new york stock excha\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename, path):\n",
    "    with open(path+filename, 'r') as myfile:\n",
    "        data=myfile.read()# .replace('\\n', '')\n",
    "        return data\n",
    "\n",
    "train_text = read_data(TRAIN, PTB_DIR)\n",
    "train_size=len(train_text)\n",
    "print(train_size, train_text[:100])\n",
    "\n",
    "valid_text = read_data(VALID, PTB_DIR)\n",
    "valid_size=len(valid_text)\n",
    "print(valid_size, valid_text[:64])\n",
    "\n",
    "test_text = read_data(TEST, PTB_DIR)\n",
    "test_size=len(test_text)\n",
    "print(test_size, test_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size =  59\n",
      "65\n",
      "33 1 58 26 0 0\n",
      "a A\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 59 # [A-Z] + [a-z] + ' ' +few 'in between; + punctuation\n",
    "first_letter = ord(string.ascii_uppercase[0]) # ascii_uppercase before lowercase! \n",
    "print(\"vocabulary size = \", vocabulary_size)\n",
    "print(first_letter)\n",
    "\n",
    "def char2id(char):\n",
    "  \"\"\" Converts char to id (int) with one-hot encoding handling of unexpected characters\"\"\"\n",
    "  if char in string.ascii_letters:# or char in string.punctuation or char in string.digits:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    # print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  \"\"\" Converts single id (int) to character\"\"\"\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "#print(len(string.punctuation))\n",
    "#for i in string.ascii_letters:\n",
    "#    print (i, char2id(i))\n",
    "\n",
    "\n",
    "print(char2id('a'), char2id('A'), char2id('z'), char2id('Z'), char2id(' '), char2id('Ã¯'))\n",
    "print(id2char(char2id('a')), id2char(char2id('A')))\n",
    "#print(id2char(65), id2char(33), id2char(90), id2char(58), id2char(0))\n",
    "#bankno\n",
    "sample = np.zeros(shape=(1, vocabulary_size), dtype=np.float)\n",
    "sample[0, char2id(' ')] = 1.0\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper class for batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, seq_length, vocab_size):\n",
    "    \"\"\"\n",
    "    Initializes the batch generator object. Stores the variables and first \"letter batch\".\n",
    "    text is text to be processed\n",
    "    batch_size is size of batch (number of samples)\n",
    "    seq_length represents the length of sequence\n",
    "    vocab_size is number of words in vocabulary (assumes one-hot encoding)\n",
    "    \"\"\"\n",
    "    # Store input parameters.\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._seq_length = seq_length\n",
    "    self._vocab_size = vocab_size\n",
    "    # Divide text into segments depending on number of batches, each segment determines a cursor position for a batch.\n",
    "    segment = self._text_size // batch_size\n",
    "    # Set initial cursor position.\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    # Store first \"letter batch\".\n",
    "    self._last_letter_batch = self._next_letter_batch()\n",
    "  \n",
    "  def _next_letter_batch(self):\n",
    "    \"\"\"\n",
    "    Returns a batch containing of encoded single letters depending on the current batch \n",
    "    cursor positions in the data.\n",
    "    Returned \"letter batch\" is of size batch_size x vocab_size\n",
    "    \"\"\"\n",
    "    letter_batch = np.zeros(shape=(self._batch_size, self._vocab_size), dtype=np.float)\n",
    "    # Iterate through \"samples\"\n",
    "    for b in range(self._batch_size):\n",
    "      # Set 1 in position pointed out by one-hot char encoding.\n",
    "      letter_batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return letter_batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    # First add last letter from previous batch (the \"additional one\").\n",
    "    batches = [self._last_letter_batch]\n",
    "    for step in range(self._seq_length):\n",
    "      batches.append(self._next_letter_batch())\n",
    "    # Store last \"letter batch\" for next batch.\n",
    "    self._last_letter_batch = batches[-1]\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set =  aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memote\n",
      "[' aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro quebec ipo kia memotec', 'ere delivering goods more quickly in october than they had for each of the five previous months   eco', 'nts could lead to an increase in abortions   james mason assistant secretary for health said the ban ', 'has just offered its N  unk  for   N   from italy there is angelo  unk   unk  at   N a bottle  unk   ', 'n N according to standard   poor  s corp  and wall street estimates for N growth are generally betwee', 'upon rate has n t yet been fixed but will probably be set at around N N   he declined to discuss othe', ' trading   a lot of people would like to go back to N before program trading mr  phelan said this wee', 'ed the ford  unk  and mercury  unk    ford chairman donald e   unk  said yesterday that mr   unk  has', 'rge of receiving most favored nation status from the u s    that  unk  would among other things provi', 'nk  the new company will open talks with  unk   unk  to buy or lease waertsilae marine  s shipyard fa', 's going to be a tough league promises the  unk  mr   unk    there will be a lot of  unk    men who ha', 'levels of interest coverage and internal cash generation   houston lighting is a unit of houston indu', 'ays  unk   unk  a vice president at the international arm of nomura securities co   this year some in', 'ng loss by a japanese auto maker since the nation  s postwar recovery   this is a time of  unk  to di', 'rain   if positioned over the brain  s  unk  area the hand held  unk  generate  unk   unk  that zip d', 'l bidder or to georgia pacific at a higher price as much as   N a share according to some estimates  ', 'lieves spot steel prices will continue to fall through early N and then reverse themselves   he is n ', 'elters buying shares of  unk  and utility companies   those two groups have recently been leading the', 'nk  myself i  ve said in moments of heat without ever  unk  to  unk  at the feat    unk  adams    unk', 'ransaction   as part of the transaction edisto agreed to give mesa an  unk  texas oil and gas partner', ' of financial information including quotron have been under some pressure as the major securities hou', 'ay its chapter N proceedings   the company has n t been able to come up with a reorganization plan in', 'commission  s filing in that case challenges connecticut  s mandatory retirement age of N for appoint', 'assed since the start of the  unk  arab  unk  in the  unk   unk    u s  and soviet negotiators opened', 'ld lose some of its entrepreneurial flavor   it could lose some of its  unk  says mr   unk  a directo', 'panies although he would n t elaborate   an indication of akzo  s success in  unk  itself will come t', ' mr  rey seems  unk  to remain the former bally raider an image that has proved hard to overcome   in', 'd mr  peters in their ambitions to build a major entertainment company   but the warner executives in', 've use of program trading   chief executive officer michael carpenter said that despite the outcry ev', ' headquarters known as building N which is to add  unk  and  unk  capacity next year   general mills ', ' sir john to  unk  their proposal for a full bid   any discussions with ford could postpone the  unk ', 'ral gas industry how to use the futures to hedge would have to continue for another a year or two he ', 'd the  unk  engine to break apart in flight   the explosion sent  unk  of metal flying  unk  the  unk', 'point out that  unk  futures were designed for institutions and corporations not for the type of smal', ' which players hit golf balls into  unk  have been installed at airports in denver and pittsburgh   t', 'learly a  unk  for the next stage of perestroika said one analyst   the economic ideas in the documen', 'ested in a job that would constantly challenge her   she signed up starting as an inside adjuster who', ' unk  and  unk  said michael  unk  editor of the microprocessor report an industry newsletter   sun h', 'e was apparently the first of its kind for  unk  an entity separate from aichi   and the acquisition ', 'o  unk   unk   unk  and heavy metal types with a strong emphasis on leather chains and  unk   unk    ', 'act that the situation in china is very complex   according to u s  sources in beijing the administra', 'idend interest in hughes earnings closed at   N up N cents in big board composite trading   gm class ', 'e children   the film makers have  unk  this offensive idea in pretty packaging   everyone is very ni', 'mortgage association fannie mae posted yields on N year mortgage commitments for delivery within N da', 'icago based  unk  trip in the spring presumably the count  s  unk    radio  unk  draws the  unk  of t', 'd currently  unk  investing public   but smith barney  s mr  doyle who yesterday trimmed his N anheus', 'rns about the potential for additional sharp swings in the market kept other trading in check   peopl', 'ysts and had a minimum five cent change in actual earnings per share   estimated and actual results i', 'r including N million during the third quarter   third quarter revenue rose to   N billion from   N b', 'er mess than you think  unk  page sept  N   in houston we have seen how bad the housing problem can b', 'million loss disclosed in december that was suffered by west virginia  s consolidated investment pool', 'ars working to ensure that no such  unk  structures ever arose here   building them now will require ', 'nk  because they were small not smart   if anyone has difficulty  unk  a world in which history went ', 'unkin donuts   dunkin donuts   announcement followed dd acquisition  s request to the delaware court ', 'g the company at a bargain price particularly since it accepted a       a share offer just last month', 'nouncements by the company   short interest increased N N in the nasdaq over the counter market for t', 'p to three miles long that trap almost everything in their path   earlier this year japan said it wou', 'has a potential substitute for cfcs   imperial chemical industries of the u k  also has one and is bu', 'ey say investors will favor companies that historically have posted annual earnings growth of N N to ', ' outside world how incompetent at risk assessment and evaluation he says   lloyd  s officials decline', '   just five months after ogilvy group was  unk  up in an unsolicited takeover kenneth roman ogilvy  ', 'to real estate development which we have n t been involved with before said dale hanson the fund  s e', 'e funded mostly from borrowings    unk   unk  corp  said its net income was   N million or N cents a ', 'onth from campeau corp  which created its u s  retailing empire with more than   N billion in junk fi', 'th an attorney who is vice chairman are the family members involved in the operations of maidenform w', 'tegy before game time   the sales job seems to be paying off when he bought the team only six of the ', 'd their buy recommendations after seeing cathay  s interim figures believe more  unk  lie ahead   fue', 's guilty plea to federal insider trading charges   drexel does n t have a delaware office but the new', 'sumer spending is slowing corporate profit margins are being squeezed business confidence is slipping', 'd which last week announced a   N billion buy back of its shares   what they are telling you is that ', 'ns of honeywell in minneapolis a general electric plant in columbia md  or a number of other companie', 'ritish air issued a statement saying it does not intend to participate in any new deal for the acquis', 'aurant association says  unk  restaurant units in the u s  rose N N to N between N and N the last yea', 'k    to build a  unk  egg that would pay for stanford when a current  unk  reaches college age parent', 'ensation   in terms of days lost on the job the study estimated that each affected employee loses abo', 'rd and amex issues in which a short interest position of at least N shares existed as of mid october ', 'such things the urge to talk about them   perhaps  unk  by the daily diet of radio and tv reporters  ', 'before launching a new bid   they have maintained that banks remain interested in financing the trans', 'east european refugees over the next few years will greatly increase the chances of  unk  workers for', 'ailure of local agencies to comply with conditions agreed upon with washington   aid was also the  un', 'olving a former bond executive earlier this year in august it paid   N in back pay and a bonus to a f', 'attempts to field a competitive slate of congressional candidates   fifth the theory may provide at l', 'ts   the three insolvent thrifts will maintain normal business hours and operations under  unk  manag', 'third quarter profit because of a decline in earning assets lower loan volume and tighter interest ma', 'e difference between the yield on the company  s earning assets and its own cost of funds   but a red', 'tself   a pilot representing a group of N pilots hired during united  s N strike filed suit friday in', 'l for the  unk  session as more investors dumped ual shares   bond prices rallied early yesterday mor', 'er  s   N million or   N a share   sales rose to   N million from   N million the year earlier   in t', 'ery traditional sort an assault on wages   mr   unk   unk  through  unk  at the end of history said i', 'i can just tell the questions are right back where they were what  s going on ca n t anything be done', 'he kraft general foods unit of philip morris cos  had about N N of the market share   nestle currentl', 'med carol  unk  N as group publisher of new york based lang communications   she will oversee working', 'ican sen   unk  specter of pennsylvania engaged the  unk  in a  unk  contest aimed at showing that mr', 'onciliation had to be dealt with by the oct  N deadline and these senate democrats refused to agree t', ' thomas foley d  wash said   if there is any support for reducing the bill it is  unk  on their desir', 'alance sheet   at one sales strategy meeting an executive suggested ordering salespeople to become ex', 'uring activity fell more than the overall measures   factory output dropped N N its first decline sin', 'ularity may be a combination of technology and  unk    the mountain bike feels as comfortable as the ', 'bt securities   when the soviets announced their last  unk  had left afghanistan in february the voic', 'the trust cited the need to retain cash for possible acquisitions   according to a spokesman one libe']\n"
     ]
    }
   ],
   "source": [
    "# Trick - override first 10 chars\n",
    "#list1 = list(train_text)\n",
    "#for i in range(2):\n",
    "#    list1[i] = 'z'\n",
    "#train_text = ''.join(list1)\n",
    "print(\"Train set =\", train_text[0:100])\n",
    "\n",
    "# Create objects for training, validation and testing batch generation.\n",
    "train_batches = BatchGenerator(train_text, BATCH_SIZE, SEQ_LENGTH, vocabulary_size)\n",
    "# The latter two - threat whole text as a batch.\n",
    "valid_batches = BatchGenerator(valid_text, valid_size, 1, vocabulary_size)\n",
    "test_batches = BatchGenerator(test_text, test_size, 1, vocabulary_size)\n",
    "\n",
    "# Get first training batch.\n",
    "batch = train_batches.next()\n",
    "#print(\"Batch = \", batch)\n",
    "print(batches2string(batch))\n",
    "#print(\"batch len = num of enrollings\",len(batch))\n",
    "#for i in range(num_unrollings):\n",
    "#    print(\"i = \", i, \"letter=\", batches2string(batch)[0][i][0], \"bits = \", batch[i][0])\n",
    "\n",
    "# Generate input-target pairs for validation set.\n",
    "valid_batch = valid_batches.next()[0]\n",
    "valid_batch_input = valid_batch[:valid_size-1]\n",
    "valid_batch_target = valid_batch[1:valid_size]\n",
    "\n",
    "# Generate input-target pairs for validation set.\n",
    "test_batch = test_batches.next()[0]\n",
    "test_batch_input = test_batch[:test_size-1]\n",
    "test_batch_target = test_batch[1:test_size]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function defining the LSTM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state, name):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        # Calculate gates activations.\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib, name=\"Input_gate\")\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb, name=\"Forget_gate\")\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob, name=\"Output_gate\")\n",
    "\n",
    "        update = tf.add(tf.matmul(i, cx), tf.matmul(o, cm) + cb, name=\"Update\")\n",
    "        state = tf.add(forget_gate * state, input_gate * tf.tanh(update), name=\"State_update\")\n",
    "        output = output_gate * tf.tanh(state)\n",
    "        return output, state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Definition of tensor graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "(100, 59)\n",
      "100\n",
      "(100, 64)\n",
      "(10000, 64)\n"
     ]
    }
   ],
   "source": [
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 0. Shared variables ops.\n",
    "with tf.name_scope(\"Shared_Variables\"):\n",
    "  # Define parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1), name=\"ix\")\n",
    "  im = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"im\")\n",
    "  ib = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"ib\")\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1), name=\"fx\")\n",
    "  fm = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"fm\")\n",
    "  fb = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"fb\")\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1), name=\"cx\")\n",
    "  cm = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"cm\")\n",
    "  cb = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"cb\")\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1), name=\"ox\")\n",
    "  om = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"om\")\n",
    "  ob = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"ob\")\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([BATCH_SIZE, HIDDEN_SIZE]), trainable=False, name=\"saved_output\")\n",
    "  saved_state = tf.Variable(tf.zeros([BATCH_SIZE, HIDDEN_SIZE]), trainable=False, name=\"saved_state\")\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, vocabulary_size], -0.1, 0.1), name=\"w\")\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]), name=\"b\")\n",
    "  \n",
    "with tf.name_scope(\"Training\"): \n",
    "    # 0. Placeholders for inputs.\n",
    "    with tf.name_scope(\"Input_data\"):\n",
    "      # Define input data buffers.\n",
    "      train_data = list()\n",
    "      for _ in range(SEQ_LENGTH + 1):\n",
    "        # Collect placeholders for inputs/labels.\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[BATCH_SIZE,vocabulary_size], name=\"Input_data\"))\n",
    "      # Collection of training inputs.\n",
    "      train_inputs = train_data[:SEQ_LENGTH]\n",
    "      # Labels are pointing to the same placeholders!\n",
    "      # Labels are inputs shifted by one time step.\n",
    "      train_labels = train_data[1:]  \n",
    "      print (len(train_inputs))\n",
    "      print (train_inputs[0].shape)\n",
    "      # Concatenate targets into 2D tensor.\n",
    "      targets = tf.concat(train_labels, 0)\n",
    "\n",
    "\n",
    "     # 2. Training LSTM ops.\n",
    "    with tf.name_scope(\"LSTM\"):\n",
    "      # Unrolled LSTM loop.\n",
    "      # Build outpus of size SEQ_LENGTH.\n",
    "      outputs = list()\n",
    "      output = saved_output\n",
    "      state = saved_state\n",
    "      for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state, \"cell\")\n",
    "        outputs.append(output)\n",
    "      print (len(outputs))\n",
    "      print (outputs[0].shape)\n",
    "      print (tf.concat(outputs, 0).shape)\n",
    "\n",
    "      # State saving across unrollings.\n",
    "      with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Fully connected layer on top => classification.\n",
    "        # In fact we will create lots of FC layers (one for each output layer), with shared weights.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "\n",
    "    # 2. Loss ops.\n",
    "    with tf.name_scope(\"Loss\"):\n",
    "        # Loss function(s) - one for every output generated by every lstm cell.\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=targets, logits=logits))\n",
    "        # Add loss summary.\n",
    "        loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    # 3. Training ops.  \n",
    "    with tf.name_scope(\"Optimization\"):\n",
    "      # Optimizer-related variables.\n",
    "      global_step = tf.Variable(0)\n",
    "      learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "      optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "      gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "      gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "      optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # 4. Predictions ops.  \n",
    "    with tf.name_scope(\"Evaluation\") as scope:\n",
    "      # Predictions.\n",
    "      train_prediction = tf.nn.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgraph responsible for generation of sample texts, char by char."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Sample_generation\") as scope:\n",
    "  # Create graphs for sampling and validation evaluation: batch 1, \"no unrolling\".\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size], name=\"Input_data\")\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"Output_data\")\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"Hidden_state\")\n",
    "\n",
    "  # Node responsible for resetting the state and output.\n",
    "  reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, HIDDEN_SIZE])),\n",
    "      saved_sample_state.assign(tf.zeros([1, HIDDEN_SIZE])))\n",
    "  # Single LSTM cell.\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state, \"cell\")\n",
    "  # Output depends on the hidden state.\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b, name=\"logits\"), name=\"outputs\")\n",
    "\n",
    "# Merge all summaries.\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "# 4. Init global variable.\n",
    "init = tf.global_variables_initializer() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgraph responsible for validation (1-char)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Validation-1char\"):\n",
    "    with tf.name_scope(\"Input_data\"):\n",
    "      # Define buffers.\n",
    "      valid_inputs = tf.placeholder(tf.float32, shape=[valid_size-1,vocabulary_size], name=\"Data\")\n",
    "      valid_targets = tf.placeholder(tf.float32, shape=[valid_size-1,vocabulary_size], name=\"Target\")\n",
    "\n",
    "    with tf.name_scope(\"LSTM\"):\n",
    "      prev_valid_outputs = tf.Variable(tf.zeros([valid_size-1, HIDDEN_SIZE]), name=\"Prev_Outputs\")\n",
    "      prev_valid_state = tf.Variable(tf.zeros([valid_size-1, HIDDEN_SIZE]), name=\"Prev_hidden_state\")\n",
    "\n",
    "      # Node responsible for resetting the state and output.\n",
    "      reset_valid_state = tf.group(\n",
    "          prev_valid_outputs.assign(tf.zeros([valid_size-1, HIDDEN_SIZE])),\n",
    "          prev_valid_state.assign(tf.zeros([valid_size-1, HIDDEN_SIZE])))\n",
    "\n",
    "      # Single LSTM cell.\n",
    "      valid_outputs, valid_state = lstm_cell(valid_inputs, prev_valid_outputs, prev_valid_state, \"Cell\")\n",
    "      # Output depends on the hidden state.\n",
    "      with tf.control_dependencies([prev_valid_outputs.assign(valid_outputs), prev_valid_state.assign(valid_state)]):\n",
    "        valid_logits = tf.nn.xw_plus_b(valid_outputs, w, b, name=\"Logits\")\n",
    "\n",
    "        # 2. Loss ops.\n",
    "    with tf.name_scope(\"Loss\"):\n",
    "        # Loss function(s) - one for every output generated by every lstm cell.\n",
    "        valid_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=valid_targets, logits=valid_logits))\n",
    "        # Add loss summary.\n",
    "        valid_loss_summary = tf.summary.scalar(\"loss\", valid_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgraph responsible for testing (1-char)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Test-1char\"):\n",
    "    with tf.name_scope(\"Input_data\"):\n",
    "      # Define buffers.\n",
    "      test_inputs = tf.placeholder(tf.float32, shape=[test_size-1,vocabulary_size], name=\"Data\")\n",
    "      test_targets = tf.placeholder(tf.float32, shape=[test_size-1,vocabulary_size], name=\"Target\")\n",
    "\n",
    "    with tf.name_scope(\"LSTM\"):\n",
    "      prev_test_outputs = tf.Variable(tf.zeros([test_size-1, HIDDEN_SIZE]), name=\"Prev_Outputs\")\n",
    "      prev_test_state = tf.Variable(tf.zeros([test_size-1, HIDDEN_SIZE]), name=\"Prev_hidden_state\")\n",
    "\n",
    "      # Node responsible for resetting the state and output.\n",
    "      reset_test_state = tf.group(\n",
    "          prev_test_outputs.assign(tf.zeros([test_size-1, HIDDEN_SIZE])),\n",
    "          prev_test_state.assign(tf.zeros([test_size-1, HIDDEN_SIZE])))\n",
    "\n",
    "      # Single LSTM cell.\n",
    "      test_outputs, test_state = lstm_cell(test_inputs, prev_test_outputs, prev_test_state, \"Cell\")\n",
    "      # Output depends on the hidden state.\n",
    "      with tf.control_dependencies([prev_test_outputs.assign(test_outputs), prev_test_state.assign(test_state)]):\n",
    "        test_logits = tf.nn.xw_plus_b(test_outputs, w, b, name=\"Logits\")\n",
    "\n",
    "        # 2. Loss ops.\n",
    "    with tf.name_scope(\"Loss\"):\n",
    "        # Loss function(s) - one for every output generated by every lstm cell.\n",
    "        test_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=test_targets, logits=test_logits))\n",
    "        # Add loss summary.\n",
    "        test_loss_summary = tf.summary.scalar(\"loss\", test_loss)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for language generation (letter sampling etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Eventually clear the log dir.\n",
    "if tf.gfile.Exists(LOG_DIR):\n",
    "  tf.gfile.DeleteRecursively(LOG_DIR)\n",
    "# Create (new) log dir.\n",
    "tf.gfile.MakeDirs(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Number of iterations per epoch = 510\n",
      "Training set BPC at step 0: 4.07671 learning rate: 10.000000\n",
      "================================================================================\n",
      "sbjE^T rfGaIsRlesoCIi  udCTs]owa`DjoEapJ haJU ZH Y foj RJ `aZAGiXhr e zU^`[^ okm\n",
      "UDjQucTGoaxj\\UKgyaPIviSFbOezeN W]oo^sFWyj^VuSQS_Dv s nh^uKIt  A\\ anwCGoL gR tw^z\n",
      "cco qZBnMp]gM^` [AY]vcdjLP oyQeYQrtnz[Brrtg^RJY X`mcUu]fThHI[j p] SSmkiM\\HFR^ ql\n",
      "rZAWfZTc[ghvv v]DPAEPvReOxi\\w_uDuHynmaluuuQW mcajZFptAH A TvaYgXHVWu  eDjP peFGR\n",
      "[tDEipzhR^GFDGeTrDSo tY  \\JMhGWLDAthjN_oyar_`tl[JwN b pniO JHefPQilrW ys _Fi^IUs\n",
      "================================================================================\n",
      "Validation set BPC: 3.44540\n",
      "Training set BPC at step 10: 3.46556 learning rate: 10.000000\n",
      "Training set BPC at step 20: 2.88609 learning rate: 10.000000\n",
      "Training set BPC at step 30: 2.78160 learning rate: 10.000000\n",
      "Training set BPC at step 40: 2.76113 learning rate: 10.000000\n",
      "Training set BPC at step 50: 2.53502 learning rate: 10.000000\n",
      "Training set BPC at step 60: 2.49567 learning rate: 10.000000\n",
      "Training set BPC at step 70: 2.50718 learning rate: 10.000000\n",
      "Training set BPC at step 80: 2.45478 learning rate: 10.000000\n",
      "Training set BPC at step 90: 2.35895 learning rate: 10.000000\n",
      "Training set BPC at step 100: 2.31968 learning rate: 10.000000\n",
      "================================================================================\n",
      "\\enel thapengag ofVodenlhotimcionsZeapxthe wsithRethemar shacagit s ths toLmemar\n",
      "^ofs on  uvessinicinleleostobink anesem inXernmes fous the lat  ong ung erge fun\n",
      "N gfeCes fot too re gormummay  thames ut ind novensing depseidey solaste sesiel \n",
      "Yonpen priguses podenemcaun tony pateclligallrsaas ann   roIit  temus suoYal urn\n",
      "Kthe bavevoleratige theretind ficnunlefeer alk anipscork tho culibo thy qocomeca\n",
      "================================================================================\n",
      "Validation set BPC: 2.51608\n",
      "Training set BPC at step 110: 2.26310 learning rate: 10.000000\n",
      "Training set BPC at step 120: 2.22670 learning rate: 10.000000\n",
      "Training set BPC at step 130: 2.29300 learning rate: 10.000000\n",
      "Training set BPC at step 140: 2.21640 learning rate: 10.000000\n",
      "Training set BPC at step 150: 2.11391 learning rate: 10.000000\n",
      "Training set BPC at step 160: 2.21179 learning rate: 10.000000\n",
      "Training set BPC at step 170: 2.11484 learning rate: 10.000000\n",
      "Training set BPC at step 180: 2.04638 learning rate: 10.000000\n",
      "Training set BPC at step 190: 2.01459 learning rate: 10.000000\n",
      "Training set BPC at step 200: 2.00813 learning rate: 10.000000\n",
      "================================================================================\n",
      "Balt inulk foreseint   N N digl goil     mopeegher pill to fork avation bamo awa\n",
      "ycang  unk  isamrids N pifiond coad fietW womppons foves redice mouer ovlicfate \n",
      "nk on the heatinc  urk  a deth in shefy Yechinu   inkiscang of fove bae iatepy  \n",
      "ke_ chaid  unk  toled fic intunk   sevion hocgroaud of owalmonit hagM atray ca u\n",
      "ve   taem mones cotare gos mengam paasw   seva e bong  unk  anopat   oneicsuonuL\n",
      "================================================================================\n",
      "Validation set BPC: 2.52479\n",
      "Training set BPC at step 210: 1.95801 learning rate: 10.000000\n",
      "Training set BPC at step 220: 1.98974 learning rate: 10.000000\n",
      "Training set BPC at step 230: 1.91884 learning rate: 10.000000\n",
      "Training set BPC at step 240: 1.97144 learning rate: 10.000000\n",
      "Training set BPC at step 250: 1.93847 learning rate: 10.000000\n",
      "Training set BPC at step 260: 1.91416 learning rate: 10.000000\n",
      "Training set BPC at step 270: 1.89444 learning rate: 10.000000\n",
      "Training set BPC at step 280: 1.92394 learning rate: 10.000000\n",
      "Training set BPC at step 290: 1.88119 learning rate: 10.000000\n",
      "Training set BPC at step 300: 1.85812 learning rate: 10.000000\n",
      "================================================================================\n",
      "Singe nit fele bolwities   mis un the  unk  hae ac ueaile promith his the severc\n",
      "Mal the N yre ald compine with ly that waicoure with cur   lulthe prica ue rich \n",
      "Xuot ik is N is a coupd that ags drowectice acoustural ic chares ic thay markave\n",
      "ef as amope unt will hilitrd N musilis couppor the aundkect  unk   unk  bradivon\n",
      "ias the abar if sears  unk  the for  unk  agecal ic cospr thak of in a in stacs \n",
      "================================================================================\n",
      "Validation set BPC: 2.60035\n",
      "Training set BPC at step 310: 1.88716 learning rate: 10.000000\n",
      "Training set BPC at step 320: 1.86740 learning rate: 10.000000\n",
      "Training set BPC at step 330: 1.87269 learning rate: 10.000000\n",
      "Training set BPC at step 340: 1.83035 learning rate: 10.000000\n",
      "Training set BPC at step 350: 1.80606 learning rate: 10.000000\n",
      "Training set BPC at step 360: 1.85492 learning rate: 10.000000\n",
      "Training set BPC at step 370: 1.82759 learning rate: 10.000000\n",
      "Training set BPC at step 380: 1.76807 learning rate: 10.000000\n",
      "Training set BPC at step 390: 1.84312 learning rate: 10.000000\n",
      "Training set BPC at step 400: 1.78468 learning rate: 10.000000\n",
      "================================================================================\n",
      "Wests the talsolion as nefivisially cots billed   they fenferning   in asceapint\n",
      " station saited it wock are in treas that the ppanon and atters bach N amond of \n",
      "ve  unk  of elepprine juchangrowbd a by over that his of  unk  mask tradger a ie\n",
      "Y beed  unk  ititide at ondosaly and isporpectrnd loved on to yearselive hondide\n",
      "^eshion over in theun woukh appeart dunrers enar  unk  ame  unk  gymon high sare\n",
      "================================================================================\n",
      "Validation set BPC: 2.69566\n",
      "Training set BPC at step 410: 1.79115 learning rate: 10.000000\n",
      "Training set BPC at step 420: 1.77061 learning rate: 10.000000\n",
      "Training set BPC at step 430: 1.75370 learning rate: 10.000000\n",
      "Training set BPC at step 440: 1.76245 learning rate: 10.000000\n",
      "Training set BPC at step 450: 1.75127 learning rate: 10.000000\n",
      "Training set BPC at step 460: 1.74261 learning rate: 10.000000\n",
      "Training set BPC at step 470: 1.69364 learning rate: 10.000000\n",
      "Training set BPC at step 480: 1.73186 learning rate: 10.000000\n",
      "Training set BPC at step 490: 1.70968 learning rate: 10.000000\n",
      "Training set BPC at step 500: 1.74410 learning rate: 10.000000\n",
      "================================================================================\n",
      "fistens the commut banked  unk  barrevcer   mr  buding it wosg   wich the  unk  \n",
      "Bing ascome   N jilfior   unk     unk    on ann  the lateles on jenexere lostrai\n",
      "King conpricon und abretused kilst matedes by ond widue parte   dusitay do   s b\n",
      "[elasts artuctial lost that hes loste ridut ounst of  unk   unk uilly cans   sha\n",
      "L dimpic alsegaysorst  s chith   in N mr  s  unk    that amplinced trades pre id\n",
      "================================================================================\n",
      "Validation set BPC: 2.62946\n",
      "Calculating BPC on test dataset\n",
      "Final test set BPC: 2.71481\n"
     ]
    }
   ],
   "source": [
    "# How often the test loss on validation batch will be computed. \n",
    "summary_frequency = 10\n",
    "\n",
    "# Create session.\n",
    "sess = tf.InteractiveSession()\n",
    "# Create summary writers, point them to LOG_DIR.\n",
    "train_writer = tf.summary.FileWriter(LOG_DIR + '/train', sess.graph)\n",
    "valid_writer = tf.summary.FileWriter(LOG_DIR + '/valid')\n",
    "test_writer = tf.summary.FileWriter(LOG_DIR + '/test')\n",
    "\n",
    "# Initialize global variables.\n",
    "tf.global_variables_initializer().run()\n",
    "print('Initialized')\n",
    "\n",
    "num_steps =  train_size // (BATCH_SIZE*SEQ_LENGTH) #70001\n",
    "print(\"Number of iterations per epoch =\", num_steps)\n",
    "for step in range(num_steps):\n",
    "    # Get next batch and create a feed dict.\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(SEQ_LENGTH + 1):\n",
    "        feed_dict[train_data[i]] = batches[i]\n",
    "    # Run training graph.\n",
    "    summary, _, t_loss, lr = sess.run([merged_summaries, optimizer, loss, learning_rate], feed_dict=feed_dict)\n",
    "    # Add summary.\n",
    "    train_writer.add_summary(summary, step * SEQ_LENGTH)\n",
    "    train_writer.flush()\n",
    "\n",
    "    # Every (100) steps collect statistics.\n",
    "    if step % summary_frequency == 0:\n",
    "      # Print loss from last batch.\n",
    "      print('Training set BPC at step %d: %0.5f learning rate: %f' % (step, t_loss, lr))\n",
    "    \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          # Reset LSTM hidden state.\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "        \n",
    "        # Validation set BPC.\n",
    "        reset_valid_state.run()\n",
    "        v_summary, v_loss = sess.run([valid_loss_summary, valid_loss],\n",
    "                                feed_dict={valid_inputs:valid_batch_input,valid_targets:valid_batch_target})\n",
    "        print(\"Validation set BPC: %.5f\" % v_loss)\n",
    "        valid_writer.add_summary(v_summary, step * SEQ_LENGTH)\n",
    "        valid_writer.flush()\n",
    "    # End of statistics collection\n",
    "\n",
    "# Test set BPC.\n",
    "print(\"Calculating BPC on test dataset\")\n",
    "reset_test_state.run()\n",
    "t_summary, t_loss = sess.run([test_loss_summary, test_loss],\n",
    "                        feed_dict={test_inputs:test_batch_input,test_targets:test_batch_target})\n",
    "print(\"Final test set BPC: %.5f\" % t_loss)\n",
    "test_writer.add_summary(t_summary, step * SEQ_LENGTH)\n",
    "test_writer.flush()\n",
    "    \n",
    "# Close writers and session.\n",
    "train_writer.close()\n",
    "valid_writer.close()\n",
    "test_writer.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
