{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import tarfile\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import shutil \n",
    "import random\n",
    "\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "# Dirs - must be absolute paths!\n",
    "LOG_DIR = '/tmp/tf/ptb_char_gru/hidden64_batch100_seq10/'\n",
    "# Local dir where PTB files will be stored.\n",
    "PTB_DIR = '/home/tkornuta/data/ptb/'\n",
    "\n",
    "# Filenames.\n",
    "TRAIN = \"ptb.train.txt\"\n",
    "VALID = \"ptb.valid.txt\"\n",
    "TEST = \"ptb.test.txt\"\n",
    "\n",
    "# Size of the hidden state.\n",
    "HIDDEN_SIZE = 64\n",
    "\n",
    "# Batch size.\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# A single recurrent layer of number of units = sequences of length\n",
    "# e.g. 200 bytes\n",
    "SEQ_LENGTH = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check/maybe download PTB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified /home/tkornuta/data/ptb/simple-examples.tgz ( 34869662 )\n"
     ]
    }
   ],
   "source": [
    "def maybe_download_ptb(path, \n",
    "                       filename='simple-examples.tgz', \n",
    "                       url='http://www.fit.vutbr.cz/~imikolov/rnnlm/', \n",
    "                       expected_bytes =34869662):\n",
    "  # Eventually create the PTB dir.\n",
    "  if not tf.gfile.Exists(path):\n",
    "    tf.gfile.MakeDirs(path)\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  _filename = path+filename\n",
    "  if not os.path.exists(_filename):\n",
    "    print('Downloading %s...' % filename)\n",
    "    _filename, _ = urlretrieve(url+filename, _filename)\n",
    "  statinfo = os.stat(_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', (_filename), '(', statinfo.st_size, ')')\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + _filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download_ptb(PTB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract dataset-related files from the PTB archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_ptb(path, filename='simple-examples.tgz', files=[\"ptb.train.txt\", \"ptb.valid.txt\", \"ptb.test.txt\", \n",
    "                                       \"ptb.char.train.txt\", \"ptb.char.valid.txt\", \"ptb.char.test.txt\"]):\n",
    "    \"\"\"Extracts files from PTB archive.\"\"\"\n",
    "    # Extract\n",
    "    tar = tarfile.open(path+filename)\n",
    "    tar.extractall(path)\n",
    "    tar.close()\n",
    "    # Copy files\n",
    "    for file in files:\n",
    "        shutil.copyfile(PTB_DIR+\"simple-examples/data/\"+file, PTB_DIR+file)\n",
    "    # Delete directory\n",
    "    shutil.rmtree(PTB_DIR+\"simple-examples/\")        \n",
    "\n",
    "extract_ptb(PTB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train, valid and test texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5101618  aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memote\n",
      "399782  consumers may want to move their telephones a little closer to \n",
      "449945  no it was n't black monday \n",
      " but while the new york stock excha\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename, path):\n",
    "    with open(path+filename, 'r') as myfile:\n",
    "        data=myfile.read()# .replace('\\n', '')\n",
    "        return data\n",
    "\n",
    "train_text = read_data(TRAIN, PTB_DIR)\n",
    "train_size=len(train_text)\n",
    "print(train_size, train_text[:100])\n",
    "\n",
    "valid_text = read_data(VALID, PTB_DIR)\n",
    "valid_size=len(valid_text)\n",
    "print(valid_size, valid_text[:64])\n",
    "\n",
    "test_text = read_data(TEST, PTB_DIR)\n",
    "test_size=len(test_text)\n",
    "print(test_size, test_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size =  59\n",
      "65\n",
      "33 1 58 26 0 0\n",
      "a A\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 59 # [A-Z] + [a-z] + ' ' +few 'in between; + punctuation\n",
    "first_letter = ord(string.ascii_uppercase[0]) # ascii_uppercase before lowercase! \n",
    "print(\"vocabulary size = \", vocabulary_size)\n",
    "print(first_letter)\n",
    "\n",
    "def char2id(char):\n",
    "  \"\"\" Converts char to id (int) with one-hot encoding handling of unexpected characters\"\"\"\n",
    "  if char in string.ascii_letters:# or char in string.punctuation or char in string.digits:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    # print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  \"\"\" Converts single id (int) to character\"\"\"\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "#print(len(string.punctuation))\n",
    "#for i in string.ascii_letters:\n",
    "#    print (i, char2id(i))\n",
    "\n",
    "\n",
    "print(char2id('a'), char2id('A'), char2id('z'), char2id('Z'), char2id(' '), char2id('Ã¯'))\n",
    "print(id2char(char2id('a')), id2char(char2id('A')))\n",
    "#print(id2char(65), id2char(33), id2char(90), id2char(58), id2char(0))\n",
    "#bankno\n",
    "sample = np.zeros(shape=(1, vocabulary_size), dtype=np.float)\n",
    "sample[0, char2id(' ')] = 1.0\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper class for batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, seq_length, vocab_size):\n",
    "    \"\"\"\n",
    "    Initializes the batch generator object. Stores the variables and first \"letter batch\".\n",
    "    text is text to be processed\n",
    "    batch_size is size of batch (number of samples)\n",
    "    seq_length represents the length of sequence\n",
    "    vocab_size is number of words in vocabulary (assumes one-hot encoding)\n",
    "    \"\"\"\n",
    "    # Store input parameters.\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._seq_length = seq_length\n",
    "    self._vocab_size = vocab_size\n",
    "    # Divide text into segments depending on number of batches, each segment determines a cursor position for a batch.\n",
    "    segment = self._text_size // batch_size\n",
    "    # Set initial cursor position.\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    # Store first \"letter batch\".\n",
    "    self._last_letter_batch = self._next_letter_batch()\n",
    "  \n",
    "  def _next_letter_batch(self):\n",
    "    \"\"\"\n",
    "    Returns a batch containing of encoded single letters depending on the current batch \n",
    "    cursor positions in the data.\n",
    "    Returned \"letter batch\" is of size batch_size x vocab_size\n",
    "    \"\"\"\n",
    "    letter_batch = np.zeros(shape=(self._batch_size, self._vocab_size), dtype=np.float)\n",
    "    # Iterate through \"samples\"\n",
    "    for b in range(self._batch_size):\n",
    "      # Set 1 in position pointed out by one-hot char encoding.\n",
    "      letter_batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return letter_batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    # First add last letter from previous batch (the \"additional one\").\n",
    "    batches = [self._last_letter_batch]\n",
    "    for step in range(self._seq_length):\n",
    "      batches.append(self._next_letter_batch())\n",
    "    # Store last \"letter batch\" for next batch.\n",
    "    self._last_letter_batch = batches[-1]\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "(100, 59)\n"
     ]
    }
   ],
   "source": [
    "# Trick - override first 10 chars\n",
    "#list1 = list(train_text)\n",
    "#for i in range(2):\n",
    "#    list1[i] = 'z'\n",
    "#train_text = ''.join(list1)\n",
    "#print(\"Train set =\", train_text[0:100])\n",
    "\n",
    "# Create objects for training, validation and testing batch generation.\n",
    "train_batches = BatchGenerator(train_text, BATCH_SIZE, SEQ_LENGTH, vocabulary_size)\n",
    "\n",
    "# Get first training batch.\n",
    "batch = train_batches.next()\n",
    "print(len(batch))\n",
    "print(batch[0].shape)\n",
    "#print(\"Batch = \", batch)\n",
    "#print(batches2string(batch))\n",
    "#print(\"batch len = num of enrollings\",len(batch))\n",
    "#for i in range(num_unrollings):\n",
    "#    print(\"i = \", i, \"letter=\", batches2string(batch)[0][i][0], \"bits = \", batch[i][0])\n",
    "\n",
    "\n",
    "# For validation  - process the whole text as one big batch.\n",
    "VALID_BATCH_SIZE = int(np.floor(valid_size/SEQ_LENGTH))\n",
    "valid_batches = BatchGenerator(valid_text, VALID_BATCH_SIZE, SEQ_LENGTH, vocabulary_size)\n",
    "valid_batch = valid_batches.next()\n",
    "#print (VALID_BATCH_SIZE)\n",
    "#print(len(valid_batch))\n",
    "#print(valid_batch[0].shape)\n",
    "\n",
    "# For texting  - process the whole text as one big batch.\n",
    "TEST_BATCH_SIZE = int(np.floor(test_size/SEQ_LENGTH))\n",
    "test_batches = BatchGenerator(test_text, TEST_BATCH_SIZE, SEQ_LENGTH, vocabulary_size)\n",
    "# Get single batch! \n",
    "test_batch = test_batches.next()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function defining the GRU cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  # Definition of the cell computation.\n",
    "  def gru_cell(input_, prev_output_, name_):\n",
    "    \"\"\"Create a GRU cell\"\"\"\n",
    "    with tf.name_scope(name_):\n",
    "        # Equations according to C. Olah blog.\n",
    "        \n",
    "        # Concatenate input with previous_output.\n",
    "        x_prev_h = tf.concat([input_, prev_output_], 1)\n",
    "        \n",
    "        # Calculate update and reset gates activations.\n",
    "        update_gate = tf.sigmoid(tf.matmul(x_prev_h, Wz) + bz, name=\"Update_gate\")\n",
    "        reset_gate = tf.sigmoid(tf.matmul(x_prev_h, Wr) + br, name=\"Reset_gate\")\n",
    "\n",
    "        # Calculate the update.\n",
    "        x_gated_prev_h = tf.concat([input_, reset_gate*prev_output_], 1)\n",
    "        update = tf.tanh(tf.matmul(x_gated_prev_h, Wh) + bh, name=\"Update\")\n",
    "        # New cell state C.\n",
    "        output = tf.add(update_gate * prev_output_, (1 - update_gate) * update, name = \"Output\")\n",
    "        \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Definition of tensor graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_buffer shape = <unknown>\n",
      "Seq length  = 10\n",
      "Batch shape = <unknown>\n",
      "10\n",
      "<unknown>\n",
      "<unknown>\n"
     ]
    }
   ],
   "source": [
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 0. Shared variables ops.\n",
    "with tf.name_scope(\"Shared_Variables\"):\n",
    "  # Define parameters:\n",
    "  # Update gate params: input, previous output, and bias.\n",
    "  Wz = tf.Variable(tf.truncated_normal([vocabulary_size+HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"Wz\")\n",
    "  bz = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"bz\")\n",
    "\n",
    "  # Forget gate params: input, previous output, and bias.\n",
    "  Wr = tf.Variable(tf.truncated_normal([vocabulary_size+HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"Wf\")\n",
    "  br = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"bf\")\n",
    "\n",
    "  # Staate update params.                             \n",
    "  Wh = tf.Variable(tf.truncated_normal([vocabulary_size+HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"Wh\")\n",
    "  bh = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"bh\")\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, vocabulary_size], -0.1, 0.1), name=\"w\")\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]), name=\"b\")\n",
    "\n",
    "  # Placeholders for previous (the oldest) state and output.\n",
    "  prev_output = tf.placeholder(tf.float32, shape=None, name=\"prev_output\")\n",
    "\n",
    "# 0. Placeholders for inputs.\n",
    "with tf.name_scope(\"Input_data\"):\n",
    "  # Define input data buffers.\n",
    "  input_buffer = list()\n",
    "  for _ in range(SEQ_LENGTH + 1):\n",
    "    # Collect placeholders for inputs/labels.\n",
    "    input_buffer.append(tf.placeholder(tf.float32, shape=None, name=\"Input_data\"))\n",
    "  print (\"input_buffer shape =\", input_buffer[0].shape)\n",
    "  # Collection of training inputs.\n",
    "  train_inputs = input_buffer[:SEQ_LENGTH]\n",
    "  # Labels are pointing to the same placeholders!\n",
    "  # Labels are inputs shifted by one time step.\n",
    "  train_labels = input_buffer[1:]  \n",
    "  print (\"Seq length  =\", len(train_inputs))\n",
    "  print (\"Batch shape =\", train_inputs[0].shape)\n",
    "  # Concatenate targets into 2D tensor.\n",
    "  targets = tf.concat(train_labels, 0)\n",
    "\n",
    " # 2. Training LSTM ops.\n",
    "with tf.name_scope(\"LSTM\"):\n",
    "  # Unrolled LSTM loop.\n",
    "  # Build outpus of size SEQ_LENGTH.\n",
    "  outputs = list()\n",
    "  output = prev_output\n",
    "  for i in train_inputs:\n",
    "    output = gru_cell(i, output, \"cell\")\n",
    "    outputs.append(output)\n",
    "  print (len(outputs))\n",
    "  print (outputs[0].shape)\n",
    "  print (tf.concat(outputs, 0).shape)\n",
    "\n",
    "# Fully connected layer on top => classification.\n",
    "# In fact we will create lots of FC layers (one for each output layer), with shared weights.\n",
    "logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b, name = \"Final_FC\")\n",
    "\n",
    "# 2. Loss ops.\n",
    "with tf.name_scope(\"Loss\"):\n",
    "    # Loss function(s) - one for every output generated by every lstm cell.\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=targets, logits=logits))\n",
    "    # Add loss summary.\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "# 3. Training ops.  \n",
    "with tf.name_scope(\"Optimization\"):\n",
    "    # Learning rate decay.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.1, global_step, 5000, 0.9, staircase=True)\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    # Gradient clipping.\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "# 4. Predictions ops.  \n",
    "with tf.name_scope(\"Evaluation\") as scope:\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgraph responsible for generation of sample texts, char by char."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Sample_generation\") as scope:\n",
    "  # Create graphs for sampling and validation evaluation: batch 1, \"no unrolling\".\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size], name=\"Input_data\")\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"Output_data\")\n",
    "\n",
    "  # Node responsible for resetting the state and output.\n",
    "  reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, HIDDEN_SIZE])))\n",
    "  # Single LSTM cell.\n",
    "  sample_output =gru_cell(sample_input, saved_sample_output, \"cell\")\n",
    "  # Output depends on the hidden state.\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b, name=\"logits\"), name=\"outputs\")\n",
    "\n",
    "# Merge all summaries.\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "# 4. Init global variable.\n",
    "init = tf.global_variables_initializer() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for language generation (letter sampling etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_feed_dict(dataset):\n",
    "  \"\"\"Creates a dictionaries for different sets: maps data onto Tensor placeholders.\"\"\"\n",
    "  feed_dict = dict()\n",
    "  if dataset==\"train\":\n",
    "    # Get next batch and create a feed dict.\n",
    "    next_batch = train_batches.next()\n",
    "    for i in range(SEQ_LENGTH + 1):\n",
    "        feed_dict[input_buffer[i]] = next_batch[i]\n",
    "    # Reset previous state and output\n",
    "    feed_dict[prev_output] = np.zeros([BATCH_SIZE, HIDDEN_SIZE])\n",
    "        \n",
    "  elif dataset==\"valid\":\n",
    "    for i in range(SEQ_LENGTH + 1):\n",
    "        feed_dict[input_buffer[i]] = valid_batch[i]\n",
    "    # Reset previous state and output\n",
    "    feed_dict[prev_output] = np.zeros([VALID_BATCH_SIZE, HIDDEN_SIZE])\n",
    "    \n",
    "  else: # test\n",
    "    for i in range(SEQ_LENGTH + 1):\n",
    "        feed_dict[input_buffer[i]] = test_batch[i]\n",
    "    # Reset previous state and output\n",
    "    feed_dict[prev_output] = np.zeros([TEST_BATCH_SIZE, HIDDEN_SIZE])\n",
    "    \n",
    "  return feed_dict # {prev_output: train_output_zeros, prev_state: train_state_zeros }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Eventually clear the log dir.\n",
    "if tf.gfile.Exists(LOG_DIR):\n",
    "  tf.gfile.DeleteRecursively(LOG_DIR)\n",
    "# Create (new) log dir.\n",
    "tf.gfile.MakeDirs(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Number of iterations per epoch = 5101\n",
      "Training set BPC at step 0: 4.07595 learning rate: 0.100000\n",
      "================================================================================\n",
      "AychjWg`lItlUtXSo kufnlYe GagepabmvpSeV^\\sTh tjDxaRQwSulceuhLVQw]VXxputpjfhCwMei\n",
      "Ltfgukn[eTScjlBi eyrEw]HCj pcEzD_acjPhOg h\\\\ekYf`epp]^UuthIXK^D ttamEiVcz MqXhah\n",
      "upi l\\scDrthaIVZPhtlIuLfbstsPokxQylheKavNwLLElbuluSlAsamjaRaffahhuPofpSrtnDsl Z^\n",
      "MhXH wNuSgRCqgKevhBRSGbh`plsTgeDEfioh]c`ZSJfh e ahy jiDD_jhnD SEurUlFkjiSeKiRelO\n",
      "X[jxpmjH`kSu`hcvNbKpihlwVrqjC\\lrr Moip\\icn  U`ppjBrO`pKmOL^EBuEWyFIMy ps lGbgtpf\n",
      "================================================================================\n",
      "Validation set BPC: 3.49578\n",
      "Training set BPC at step 100: 2.23663 learning rate: 0.100000\n",
      "Training set BPC at step 200: 2.19068 learning rate: 0.100000\n",
      "Training set BPC at step 300: 2.04500 learning rate: 0.100000\n",
      "Training set BPC at step 400: 2.03211 learning rate: 0.100000\n",
      "Training set BPC at step 500: 2.01046 learning rate: 0.100000\n",
      "Training set BPC at step 600: 2.00898 learning rate: 0.100000\n",
      "Training set BPC at step 700: 2.01428 learning rate: 0.100000\n",
      "Training set BPC at step 800: 2.01469 learning rate: 0.100000\n",
      "Training set BPC at step 900: 2.03745 learning rate: 0.100000\n",
      "Training set BPC at step 1000: 2.02244 learning rate: 0.100000\n",
      "================================================================================\n",
      "H t wasds tat bleter would wo hdat stopiof wa ths sopll vat bio bot rotower   bi\n",
      "P fanty exppecstoled toratiles itt canis tonstqiaverovatoarirettotroted bionust \n",
      "wuusies hus sto boral  s and  s foll the of to to vews is s   dadise   ws res en\n",
      "T N most t   N by willion alo yons whe a blot was   torcow wat we tos   be are o\n",
      "K thamts porss tricesitionibibos chy tur stotoborts deared ber ony suis   thert \n",
      "================================================================================\n",
      "Validation set BPC: 1.97637\n",
      "Training set BPC at step 1100: 2.01362 learning rate: 0.100000\n",
      "Training set BPC at step 1200: 1.90214 learning rate: 0.100000\n",
      "Training set BPC at step 1300: 1.96839 learning rate: 0.100000\n",
      "Training set BPC at step 1400: 1.94463 learning rate: 0.100000\n",
      "Training set BPC at step 1500: 2.00049 learning rate: 0.100000\n",
      "Training set BPC at step 1600: 1.95243 learning rate: 0.100000\n",
      "Training set BPC at step 1700: 1.99607 learning rate: 0.100000\n",
      "Training set BPC at step 1800: 1.99952 learning rate: 0.100000\n",
      "Training set BPC at step 1900: 2.02032 learning rate: 0.100000\n",
      "Training set BPC at step 2000: 2.00480 learning rate: 0.100000\n",
      "================================================================================\n",
      "[ N N N otfle this partins incn a fin the el cex  end ther e  n e  note ale pber\n",
      "zo  unk  tere mr  bist legtast nusmist the lars to billly on desn a commany trat\n",
      "poo inc   s a ne a neys s wil t  wilet e e ha ks  e    hk   k kake he   be be h \n",
      "e is  unk  moms sinly fead be hate mr dy suld a for le gortof  the biash shers b\n",
      "Bthat of N badid innicson flo    ow on a anle walnen follelalmal mraond boder a \n",
      "================================================================================\n",
      "Validation set BPC: 1.95576\n",
      "Training set BPC at step 2100: 2.01688 learning rate: 0.100000\n",
      "Training set BPC at step 2200: 1.91840 learning rate: 0.100000\n",
      "Training set BPC at step 2300: 1.91351 learning rate: 0.100000\n",
      "Training set BPC at step 2400: 1.98946 learning rate: 0.100000\n",
      "Training set BPC at step 2500: 1.98002 learning rate: 0.100000\n",
      "Training set BPC at step 2600: 1.94182 learning rate: 0.100000\n",
      "Training set BPC at step 2700: 1.95911 learning rate: 0.100000\n",
      "Training set BPC at step 2800: 1.98975 learning rate: 0.100000\n",
      "Training set BPC at step 2900: 1.97261 learning rate: 0.100000\n",
      "Training set BPC at step 3000: 1.97004 learning rate: 0.100000\n",
      "================================================================================\n",
      "\\lev tomentristed apsote la preve   N is the beiisibiiiemet titetststttiong itsi\n",
      "Mwto moesectiad unsich in pnigre wring the ughages un hefo  eane beence incue  t\n",
      "[ his unk s woud predig shet ansics ting s cdidisig s is in we jigh in becent u \n",
      "Pted ste jecutiapes at ttitestents testestsiteetents testetsiteritentiee eseste \n",
      "^ unk  stosgelesy anyn at  gon prog pengs ghate pon ss bal s n be t N sercts ing\n",
      "================================================================================\n",
      "Validation set BPC: 1.95781\n",
      "Training set BPC at step 3100: 1.92470 learning rate: 0.100000\n",
      "Training set BPC at step 3200: 1.99914 learning rate: 0.100000\n",
      "Training set BPC at step 3300: 2.03403 learning rate: 0.100000\n",
      "Training set BPC at step 3400: 1.95455 learning rate: 0.100000\n",
      "Training set BPC at step 3500: 1.97285 learning rate: 0.100000\n",
      "Training set BPC at step 3600: 1.92632 learning rate: 0.100000\n",
      "Training set BPC at step 3700: 2.02917 learning rate: 0.100000\n",
      "Training set BPC at step 3800: 2.00665 learning rate: 0.100000\n",
      "Training set BPC at step 3900: 1.91909 learning rate: 0.100000\n",
      "Training set BPC at step 4000: 2.03793 learning rate: 0.100000\n",
      "================================================================================\n",
      "N mring pilzing  unf  ceodeaastrseisealeeasss aralssssrs sssssssssssfrassscassss\n",
      "`g aroinaingt pure selg allgions to   an about thits thers utheghtichiter quiliu\n",
      "Fs frop  unk  ared  unk  talan texuns a f yeouareed surpderd seerdeatfwo ar nora\n",
      "v off  unk  sere he inces tan offereled reclies couries renerarer arentralullera\n",
      "kructy ast clockev in tor apapnentel toor and sies owhnt ducts thel at at tow at\n",
      "================================================================================\n",
      "Validation set BPC: 1.96525\n",
      "Training set BPC at step 4100: 1.96913 learning rate: 0.100000\n",
      "Training set BPC at step 4200: 1.92397 learning rate: 0.100000\n",
      "Training set BPC at step 4300: 1.96029 learning rate: 0.100000\n",
      "Training set BPC at step 4400: 2.03074 learning rate: 0.100000\n",
      "Training set BPC at step 4500: 2.03624 learning rate: 0.100000\n",
      "Training set BPC at step 4600: 2.01344 learning rate: 0.100000\n",
      "Training set BPC at step 4700: 1.95618 learning rate: 0.100000\n",
      "Training set BPC at step 4800: 2.00647 learning rate: 0.100000\n",
      "Training set BPC at step 4900: 2.03444 learning rate: 0.100000\n",
      "Training set BPC at step 5000: 2.02879 learning rate: 0.090000\n",
      "================================================================================\n",
      "Rrented seft hedlromep stremed   is moe thees theter rontsegh  unk genth ther wo\n",
      "C amomit wo and shase to irs hests wh whea  ut then whof s N milsests iss a sos \n",
      "linkese loy corp decs an easicuites nerhy the a susugs reo she eew  uh   the may\n",
      "Der has prinee towd thwoow ctaee wrooch whitet tre rewme wahy wotruet th thers t\n",
      "Tudintiaf hibie ontrink the erers aws   chirshs in he saanizentet arer us wamet \n",
      "================================================================================\n",
      "Validation set BPC: 1.97659\n",
      "Training set BPC at step 5100: 1.92455 learning rate: 0.090000\n",
      "Calculating BPC on test dataset\n",
      "Final test set BPC: 1.93652\n"
     ]
    }
   ],
   "source": [
    "# How often the test loss on validation batch will be computed. \n",
    "summary_frequency = 100\n",
    "\n",
    "# Create session.\n",
    "sess = tf.InteractiveSession()\n",
    "# Create summary writers, point them to LOG_DIR.\n",
    "train_writer = tf.summary.FileWriter(LOG_DIR + '/train', sess.graph)\n",
    "valid_writer = tf.summary.FileWriter(LOG_DIR + '/valid')\n",
    "test_writer = tf.summary.FileWriter(LOG_DIR + '/test')\n",
    "\n",
    "# Initialize global variables.\n",
    "tf.global_variables_initializer().run()\n",
    "print('Initialized')\n",
    "\n",
    "num_steps =  train_size // (BATCH_SIZE*SEQ_LENGTH) #70001\n",
    "print(\"Number of iterations per epoch =\", num_steps)\n",
    "for step in range(num_steps):\n",
    "    # Run training graph.\n",
    "    batch = train_batches.next()\n",
    "    summary, _, t_loss, lr = sess.run([merged_summaries, optimizer, loss, learning_rate], \n",
    "                                      feed_dict=create_feed_dict(\"train\"))\n",
    "    # Add summary.\n",
    "    train_writer.add_summary(summary, step*BATCH_SIZE*SEQ_LENGTH)\n",
    "    train_writer.flush()\n",
    "\n",
    "    # Every (100) steps collect statistics.\n",
    "    if step % summary_frequency == 0:\n",
    "      # Print loss from last batch.\n",
    "      print('Training set BPC at step %d: %0.5f learning rate: %f' % (step, t_loss, lr))\n",
    "    \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate sample text...\n",
    "        print('=' * 80)\n",
    "        # consisting of 5 lines...\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          # Reset LSTM hidden state.\n",
    "          reset_sample_state.run()\n",
    "          # with 79 characters in each.\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "        \n",
    "        # Validation set BPC.\n",
    "        v_summary, v_loss = sess.run([merged_summaries, loss], feed_dict=create_feed_dict(\"valid\"))\n",
    "        print(\"Validation set BPC: %.5f\" % v_loss)\n",
    "        valid_writer.add_summary(v_summary, step*BATCH_SIZE*SEQ_LENGTH)\n",
    "        valid_writer.flush()\n",
    "    # End of statistics collection\n",
    "\n",
    "# Test set BPC.\n",
    "print(\"Calculating BPC on test dataset\")\n",
    "t_summary, t_loss = sess.run([merged_summaries, loss], feed_dict=create_feed_dict(\"test\"))\n",
    "print(\"Final test set BPC: %.5f\" % t_loss)\n",
    "test_writer.add_summary(t_summary, step*BATCH_SIZE*SEQ_LENGTH)\n",
    "test_writer.flush()\n",
    "    \n",
    "# Close writers and session.\n",
    "train_writer.close()\n",
    "valid_writer.close()\n",
    "test_writer.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
