{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import tarfile\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import shutil \n",
    "import random\n",
    "\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "# Dirs - must be absolute paths!\n",
    "LOG_DIR = '/tmp/tf/ptb_char_gru/hidden64_batch100_seq10/'\n",
    "# Local dir where PTB files will be stored.\n",
    "PTB_DIR = '/home/tkornuta/data/ptb/'\n",
    "\n",
    "# Filenames.\n",
    "TRAIN = \"ptb.train.txt\"\n",
    "VALID = \"ptb.valid.txt\"\n",
    "TEST = \"ptb.test.txt\"\n",
    "\n",
    "# Size of the hidden state.\n",
    "HIDDEN_SIZE = 64\n",
    "\n",
    "# Batch size.\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# A single recurrent layer of number of units = sequences of length\n",
    "# e.g. 200 bytes\n",
    "SEQ_LENGTH = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check/maybe download PTB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified /home/tkornuta/data/ptb/simple-examples.tgz ( 34869662 )\n"
     ]
    }
   ],
   "source": [
    "def maybe_download_ptb(path, \n",
    "                       filename='simple-examples.tgz', \n",
    "                       url='http://www.fit.vutbr.cz/~imikolov/rnnlm/', \n",
    "                       expected_bytes =34869662):\n",
    "  # Eventually create the PTB dir.\n",
    "  if not tf.gfile.Exists(path):\n",
    "    tf.gfile.MakeDirs(path)\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  _filename = path+filename\n",
    "  if not os.path.exists(_filename):\n",
    "    print('Downloading %s...' % filename)\n",
    "    _filename, _ = urlretrieve(url+filename, _filename)\n",
    "  statinfo = os.stat(_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', (_filename), '(', statinfo.st_size, ')')\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + _filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download_ptb(PTB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract dataset-related files from the PTB archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_ptb(path, filename='simple-examples.tgz', files=[\"ptb.train.txt\", \"ptb.valid.txt\", \"ptb.test.txt\", \n",
    "                                       \"ptb.char.train.txt\", \"ptb.char.valid.txt\", \"ptb.char.test.txt\"]):\n",
    "    \"\"\"Extracts files from PTB archive.\"\"\"\n",
    "    # Extract\n",
    "    tar = tarfile.open(path+filename)\n",
    "    tar.extractall(path)\n",
    "    tar.close()\n",
    "    # Copy files\n",
    "    for file in files:\n",
    "        shutil.copyfile(PTB_DIR+\"simple-examples/data/\"+file, PTB_DIR+file)\n",
    "    # Delete directory\n",
    "    shutil.rmtree(PTB_DIR+\"simple-examples/\")        \n",
    "\n",
    "extract_ptb(PTB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train, valid and test texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5101618  aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memote\n",
      "399782  consumers may want to move their telephones a little closer to \n",
      "449945  no it was n't black monday \n",
      " but while the new york stock excha\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename, path):\n",
    "    with open(path+filename, 'r') as myfile:\n",
    "        data=myfile.read()# .replace('\\n', '')\n",
    "        return data\n",
    "\n",
    "train_text = read_data(TRAIN, PTB_DIR)\n",
    "train_size=len(train_text)\n",
    "print(train_size, train_text[:100])\n",
    "\n",
    "valid_text = read_data(VALID, PTB_DIR)\n",
    "valid_size=len(valid_text)\n",
    "print(valid_size, valid_text[:64])\n",
    "\n",
    "test_text = read_data(TEST, PTB_DIR)\n",
    "test_size=len(test_text)\n",
    "print(test_size, test_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size =  59\n",
      "65\n",
      "33 1 58 26 0 0\n",
      "a A\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 59 # [A-Z] + [a-z] + ' ' +few 'in between; + punctuation\n",
    "first_letter = ord(string.ascii_uppercase[0]) # ascii_uppercase before lowercase! \n",
    "print(\"vocabulary size = \", vocabulary_size)\n",
    "print(first_letter)\n",
    "\n",
    "def char2id(char):\n",
    "  \"\"\" Converts char to id (int) with one-hot encoding handling of unexpected characters\"\"\"\n",
    "  if char in string.ascii_letters:# or char in string.punctuation or char in string.digits:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    # print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  \"\"\" Converts single id (int) to character\"\"\"\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "#print(len(string.punctuation))\n",
    "#for i in string.ascii_letters:\n",
    "#    print (i, char2id(i))\n",
    "\n",
    "\n",
    "print(char2id('a'), char2id('A'), char2id('z'), char2id('Z'), char2id(' '), char2id('Ã¯'))\n",
    "print(id2char(char2id('a')), id2char(char2id('A')))\n",
    "#print(id2char(65), id2char(33), id2char(90), id2char(58), id2char(0))\n",
    "#bankno\n",
    "sample = np.zeros(shape=(1, vocabulary_size), dtype=np.float)\n",
    "sample[0, char2id(' ')] = 1.0\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper class for batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, seq_length, vocab_size):\n",
    "    \"\"\"\n",
    "    Initializes the batch generator object. Stores the variables and first \"letter batch\".\n",
    "    text is text to be processed\n",
    "    batch_size is size of batch (number of samples)\n",
    "    seq_length represents the length of sequence\n",
    "    vocab_size is number of words in vocabulary (assumes one-hot encoding)\n",
    "    \"\"\"\n",
    "    # Store input parameters.\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._seq_length = seq_length\n",
    "    self._vocab_size = vocab_size\n",
    "    # Divide text into segments depending on number of batches, each segment determines a cursor position for a batch.\n",
    "    segment = self._text_size // batch_size\n",
    "    # Set initial cursor position.\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    # Store first \"letter batch\".\n",
    "    self._last_letter_batch = self._next_letter_batch()\n",
    "  \n",
    "  def _next_letter_batch(self):\n",
    "    \"\"\"\n",
    "    Returns a batch containing of encoded single letters depending on the current batch \n",
    "    cursor positions in the data.\n",
    "    Returned \"letter batch\" is of size batch_size x vocab_size\n",
    "    \"\"\"\n",
    "    letter_batch = np.zeros(shape=(self._batch_size, self._vocab_size), dtype=np.float)\n",
    "    # Iterate through \"samples\"\n",
    "    for b in range(self._batch_size):\n",
    "      # Set 1 in position pointed out by one-hot char encoding.\n",
    "      letter_batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return letter_batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    # First add last letter from previous batch (the \"additional one\").\n",
    "    batches = [self._last_letter_batch]\n",
    "    for step in range(self._seq_length):\n",
    "      batches.append(self._next_letter_batch())\n",
    "    # Store last \"letter batch\" for next batch.\n",
    "    self._last_letter_batch = batches[-1]\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "(100, 59)\n"
     ]
    }
   ],
   "source": [
    "# Trick - override first 10 chars\n",
    "#list1 = list(train_text)\n",
    "#for i in range(2):\n",
    "#    list1[i] = 'z'\n",
    "#train_text = ''.join(list1)\n",
    "#print(\"Train set =\", train_text[0:100])\n",
    "\n",
    "# Create objects for training, validation and testing batch generation.\n",
    "train_batches = BatchGenerator(train_text, BATCH_SIZE, SEQ_LENGTH, vocabulary_size)\n",
    "\n",
    "# Get first training batch.\n",
    "batch = train_batches.next()\n",
    "print(len(batch))\n",
    "print(batch[0].shape)\n",
    "#print(\"Batch = \", batch)\n",
    "#print(batches2string(batch))\n",
    "#print(\"batch len = num of enrollings\",len(batch))\n",
    "#for i in range(num_unrollings):\n",
    "#    print(\"i = \", i, \"letter=\", batches2string(batch)[0][i][0], \"bits = \", batch[i][0])\n",
    "\n",
    "\n",
    "# For validation  - process the whole text as one big batch.\n",
    "VALID_BATCH_SIZE = int(np.floor(valid_size/SEQ_LENGTH))\n",
    "valid_batches = BatchGenerator(valid_text, VALID_BATCH_SIZE, SEQ_LENGTH, vocabulary_size)\n",
    "valid_batch = valid_batches.next()\n",
    "#print (VALID_BATCH_SIZE)\n",
    "#print(len(valid_batch))\n",
    "#print(valid_batch[0].shape)\n",
    "\n",
    "# For texting  - process the whole text as one big batch.\n",
    "TEST_BATCH_SIZE = int(np.floor(test_size/SEQ_LENGTH))\n",
    "test_batches = BatchGenerator(test_text, TEST_BATCH_SIZE, SEQ_LENGTH, vocabulary_size)\n",
    "# Get single batch! \n",
    "test_batch = test_batches.next()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function defining the GRU cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  # Definition of the cell computation.\n",
    "  def gru_cell(input_, prev_output_, name_):\n",
    "    \"\"\"Create a GRU cell\"\"\"\n",
    "    with tf.name_scope(name_):        \n",
    "        # Concatenate input with previous_output.\n",
    "        x_prev_h = tf.concat([input_, prev_output_], 1)\n",
    "        \n",
    "        # Calculate update and reset gates activations.\n",
    "        update_gate = tf.sigmoid(tf.matmul(x_prev_h, Wz) + bz, name=\"Update_gate\")\n",
    "        reset_gate = tf.sigmoid(tf.matmul(x_prev_h, Wr) + br, name=\"Reset_gate\")\n",
    "\n",
    "        # Calculate the update.\n",
    "        x_gated_prev_h = tf.concat([input_, reset_gate*prev_output_], 1)\n",
    "        update = tf.tanh(tf.matmul(x_gated_prev_h, Wh) + bh, name=\"Update\")\n",
    "        # New cell state C.\n",
    "        output = tf.add(update_gate * prev_output_, (1 - update_gate) * update, name = \"Output\")\n",
    "        \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Definition of tensor graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_buffer shape = <unknown>\n",
      "Seq length  = 10\n",
      "Batch shape = <unknown>\n",
      "10\n",
      "<unknown>\n",
      "<unknown>\n"
     ]
    }
   ],
   "source": [
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 0. Shared variables ops.\n",
    "with tf.name_scope(\"Shared_Variables\"):\n",
    "  # Define parameters:\n",
    "  # Update gate params: input, previous output, and bias.\n",
    "  Wz = tf.Variable(tf.truncated_normal([vocabulary_size+HIDDEN_SIZE, 1], -0.1, 0.1), name=\"Wz\")\n",
    "  bz = tf.Variable(tf.zeros([1, 1]), name=\"bz\")\n",
    "\n",
    "  # Forget gate params: input, previous output, and bias.\n",
    "  Wr = tf.Variable(tf.truncated_normal([vocabulary_size+HIDDEN_SIZE, 1], -0.1, 0.1), name=\"Wf\")\n",
    "  br = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"bf\")\n",
    "\n",
    "  # Staate update params.                             \n",
    "  Wh = tf.Variable(tf.truncated_normal([vocabulary_size+HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"Wh\")\n",
    "  bh = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"bh\")\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, vocabulary_size], -0.1, 0.1), name=\"w\")\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]), name=\"b\")\n",
    "\n",
    "  # Placeholders for previous (the oldest) state and output.\n",
    "  prev_output = tf.placeholder(tf.float32, shape=None, name=\"prev_output\")\n",
    "\n",
    "# 0. Placeholders for inputs.\n",
    "with tf.name_scope(\"Input_data\"):\n",
    "  # Define input data buffers.\n",
    "  input_buffer = list()\n",
    "  for _ in range(SEQ_LENGTH + 1):\n",
    "    # Collect placeholders for inputs/labels.\n",
    "    input_buffer.append(tf.placeholder(tf.float32, shape=None, name=\"Input_data\"))\n",
    "  print (\"input_buffer shape =\", input_buffer[0].shape)\n",
    "  # Collection of training inputs.\n",
    "  train_inputs = input_buffer[:SEQ_LENGTH]\n",
    "  # Labels are pointing to the same placeholders!\n",
    "  # Labels are inputs shifted by one time step.\n",
    "  train_labels = input_buffer[1:]  \n",
    "  print (\"Seq length  =\", len(train_inputs))\n",
    "  print (\"Batch shape =\", train_inputs[0].shape)\n",
    "  # Concatenate targets into 2D tensor.\n",
    "  targets = tf.concat(train_labels, 0)\n",
    "\n",
    " # 2. Training ops.\n",
    "with tf.name_scope(\"GRU\"):\n",
    "  # Unrolled GRU loop.\n",
    "  # Build outpus of size SEQ_LENGTH.\n",
    "  outputs = list()\n",
    "  output = prev_output\n",
    "  for i in train_inputs:\n",
    "    output = gru_cell(i, output, \"cell\")\n",
    "    outputs.append(output)\n",
    "  print (len(outputs))\n",
    "  print (outputs[0].shape)\n",
    "  print (tf.concat(outputs, 0).shape)\n",
    "\n",
    "# Fully connected layer on top => classification.\n",
    "# In fact we will create lots of FC layers (one for each output layer), with shared weights.\n",
    "logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b, name = \"Final_FC\")\n",
    "\n",
    "# 2. Loss ops.\n",
    "with tf.name_scope(\"Loss\"):\n",
    "    # Loss function(s) - one for every output generated by every lstm cell.\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=targets, logits=logits))\n",
    "    # Add loss summary.\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "# 3. Training ops.  \n",
    "with tf.name_scope(\"Optimization\"):\n",
    "    # Learning rate decay.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.1, global_step, 5000, 0.9, staircase=True)\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    # Gradient clipping.\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "# 4. Predictions ops.  \n",
    "with tf.name_scope(\"Evaluation\") as scope:\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgraph responsible for generation of sample texts, char by char."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Sample_generation\") as scope:\n",
    "  # Create graphs for sampling and validation evaluation: batch 1, \"no unrolling\".\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size], name=\"Input_data\")\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"Output_data\")\n",
    "\n",
    "  # Node responsible for resetting the state and output.\n",
    "  reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, HIDDEN_SIZE])))\n",
    "  # Single LSTM cell.\n",
    "  sample_output =gru_cell(sample_input, saved_sample_output, \"cell\")\n",
    "  # Output depends on the hidden state.\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b, name=\"logits\"), name=\"outputs\")\n",
    "\n",
    "# Merge all summaries.\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "# 4. Init global variable.\n",
    "init = tf.global_variables_initializer() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for language generation (letter sampling etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_feed_dict(dataset):\n",
    "  \"\"\"Creates a dictionaries for different sets: maps data onto Tensor placeholders.\"\"\"\n",
    "  feed_dict = dict()\n",
    "  if dataset==\"train\":\n",
    "    # Get next batch and create a feed dict.\n",
    "    next_batch = train_batches.next()\n",
    "    for i in range(SEQ_LENGTH + 1):\n",
    "        feed_dict[input_buffer[i]] = next_batch[i]\n",
    "    # Reset previous state and output\n",
    "    feed_dict[prev_output] = np.zeros([BATCH_SIZE, HIDDEN_SIZE])\n",
    "        \n",
    "  elif dataset==\"valid\":\n",
    "    for i in range(SEQ_LENGTH + 1):\n",
    "        feed_dict[input_buffer[i]] = valid_batch[i]\n",
    "    # Reset previous state and output\n",
    "    feed_dict[prev_output] = np.zeros([VALID_BATCH_SIZE, HIDDEN_SIZE])\n",
    "    \n",
    "  else: # test\n",
    "    for i in range(SEQ_LENGTH + 1):\n",
    "        feed_dict[input_buffer[i]] = test_batch[i]\n",
    "    # Reset previous state and output\n",
    "    feed_dict[prev_output] = np.zeros([TEST_BATCH_SIZE, HIDDEN_SIZE])\n",
    "    \n",
    "  return feed_dict # {prev_output: train_output_zeros, prev_state: train_state_zeros }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Eventually clear the log dir.\n",
    "if tf.gfile.Exists(LOG_DIR):\n",
    "  tf.gfile.DeleteRecursively(LOG_DIR)\n",
    "# Create (new) log dir.\n",
    "tf.gfile.MakeDirs(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Number of iterations per epoch = 5101\n",
      "Training set BPC at step 0: 4.08848 learning rate: 0.100000\n",
      "================================================================================\n",
      "SzAjHXjT^]jv[ivmvCGpimqdo[jv_kAkzmrNBjtSeDk^ddpzkYGsdkBJFEgdgTlgroQF NGZvkDTruVa\n",
      "hNZo_b\\oC_jSVdNjbxrgHscsWhwhy`GRlpPztdiUiroplfswNHJ sBdumBo LADqsHf\\orsRJyHu gx^\n",
      "QrnaIhhhVZofiUnG`ZkSkLEo]jTa Qe_[y]rUKoA^IhSveQnLMUqBoaJnraVAUoBfpjSXRocmqm]iKZe\n",
      "dmmGdxStiUswiG nouwNMdzVJ\\t_LpvSTfHdCBu oMhwTpcpLCmtrEdMnCoIxeLJgCh_JumiyEcNGLfH\n",
      "UoiFqsrdye`\\\\YWCElrAuJyeXCCxW  MyXK]JtwwN e]uYESoFGc_zHPTwKbxswagUcsedmJnKyOKG_m\n",
      "================================================================================\n",
      "Validation set BPC: 3.70872\n",
      "Training set BPC at step 100: 2.36819 learning rate: 0.100000\n",
      "Training set BPC at step 200: 2.17418 learning rate: 0.100000\n",
      "Training set BPC at step 300: 1.99862 learning rate: 0.100000\n",
      "Training set BPC at step 400: 2.03115 learning rate: 0.100000\n",
      "Training set BPC at step 500: 1.95634 learning rate: 0.100000\n",
      "Training set BPC at step 600: 1.97915 learning rate: 0.100000\n",
      "Training set BPC at step 700: 1.98389 learning rate: 0.100000\n",
      "Training set BPC at step 800: 1.99998 learning rate: 0.100000\n",
      "Training set BPC at step 900: 1.99754 learning rate: 0.100000\n",
      "Training set BPC at step 1000: 1.98060 learning rate: 0.100000\n",
      "================================================================================\n",
      "muce pors ant saital ids dan rernurbrapcaldeder if entrbill  in het a sungute ra\n",
      "clust devity regotons comment jay   N will N misl  unk  wo   the funk  barcs and\n",
      "Gat rerergan the  ill  and means ismone  esotion soal theyedrdion its wore the  \n",
      "zed rutcals rictar any offor N of bawter for   N to N troizecandibligncion  unk \n",
      "\\at housd of tryure  funf t a  sheny strounter the buncent in secinge muscating \n",
      "================================================================================\n",
      "Validation set BPC: 1.92132\n",
      "Training set BPC at step 1100: 1.97295 learning rate: 0.100000\n",
      "Training set BPC at step 1200: 1.87203 learning rate: 0.100000\n",
      "Training set BPC at step 1300: 1.92486 learning rate: 0.100000\n",
      "Training set BPC at step 1400: 1.91726 learning rate: 0.100000\n",
      "Training set BPC at step 1500: 1.95838 learning rate: 0.100000\n",
      "Training set BPC at step 1600: 1.92016 learning rate: 0.100000\n",
      "Training set BPC at step 1700: 1.94985 learning rate: 0.100000\n",
      "Training set BPC at step 1800: 1.94785 learning rate: 0.100000\n",
      "Training set BPC at step 1900: 1.92655 learning rate: 0.100000\n",
      "Training set BPC at step 2000: 1.93287 learning rate: 0.100000\n",
      "================================================================================\n",
      "foot or siles a mayions a dos bico  and prolenton in a pracucernome gor to the  \n",
      "Ceatis formmut ore pions mutut late of is sadid  uskre apanning yearled   N fir \n",
      "annwehice   recouris restanat wN thick to morerere adreichary companation is  sa\n",
      "Fen incad dispans ser wa keng is prer of werman earriesation soles addostiesays \n",
      "plk anduecons a from couf strud cald or saids as civent was of to tooluddan deca\n",
      "================================================================================\n",
      "Validation set BPC: 1.91110\n",
      "Training set BPC at step 2100: 1.89360 learning rate: 0.100000\n",
      "Training set BPC at step 2200: 1.86560 learning rate: 0.100000\n",
      "Training set BPC at step 2300: 1.86464 learning rate: 0.100000\n",
      "Training set BPC at step 2400: 1.94788 learning rate: 0.100000\n",
      "Training set BPC at step 2500: 1.89423 learning rate: 0.100000\n",
      "Training set BPC at step 2600: 1.84735 learning rate: 0.100000\n",
      "Training set BPC at step 2700: 1.90539 learning rate: 0.100000\n",
      "Training set BPC at step 2800: 1.92595 learning rate: 0.100000\n",
      "Training set BPC at step 2900: 1.90882 learning rate: 0.100000\n",
      "Training set BPC at step 3000: 1.92220 learning rate: 0.100000\n",
      "================================================================================\n",
      "nd to the see   ut is jupsted thimen markeve   brom sted that the catad  s     n\n",
      "xiry biding the treits st esh s prods  N the   the verls  unk  the ded rererical\n",
      "Ces on howe lemed the requectore   the indof   bus   i sh   said t   bumm   as e\n",
      "duetion althis and totion of sthe sull  unk  wedsus  N mither i stedct pored   l\n",
      "s million of  unk  a rcerighan dersply be indefo mary enens    unk  murigend des\n",
      "================================================================================\n",
      "Validation set BPC: 1.88906\n",
      "Training set BPC at step 3100: 1.85767 learning rate: 0.100000\n",
      "Training set BPC at step 3200: 1.90822 learning rate: 0.100000\n",
      "Training set BPC at step 3300: 1.94160 learning rate: 0.100000\n",
      "Training set BPC at step 3400: 1.88630 learning rate: 0.100000\n",
      "Training set BPC at step 3500: 1.88530 learning rate: 0.100000\n",
      "Training set BPC at step 3600: 1.84578 learning rate: 0.100000\n",
      "Training set BPC at step 3700: 1.89388 learning rate: 0.100000\n",
      "Training set BPC at step 3800: 1.94753 learning rate: 0.100000\n",
      "Training set BPC at step 3900: 1.83993 learning rate: 0.100000\n",
      "Training set BPC at step 4000: 1.92882 learning rate: 0.100000\n",
      "================================================================================\n",
      "S N million a con widitomleused for dige framisioles warn be tems alf of its the\n",
      "Oen of N mr to copeve dever   dow das nohy at blisnaknag tange yers dedend decen\n",
      "quist repale anscoduce stmost of  unk  feary sutelly unk  nout the  unk ersefart\n",
      "Eorle breag ecutured whited to ogibut N N loos sook of accecerning worken N coug\n",
      "Hed offiain of casipecersts as peratiffors sodend contracut portaid a isposisost\n",
      "================================================================================\n",
      "Validation set BPC: 1.87605\n",
      "Training set BPC at step 4100: 1.86957 learning rate: 0.100000\n",
      "Training set BPC at step 4200: 1.84925 learning rate: 0.100000\n",
      "Training set BPC at step 4300: 1.89383 learning rate: 0.100000\n",
      "Training set BPC at step 4400: 1.90478 learning rate: 0.100000\n",
      "Training set BPC at step 4500: 1.94498 learning rate: 0.100000\n",
      "Training set BPC at step 4600: 1.87969 learning rate: 0.100000\n",
      "Training set BPC at step 4700: 1.88385 learning rate: 0.100000\n",
      "Training set BPC at step 4800: 1.89313 learning rate: 0.100000\n",
      "Training set BPC at step 4900: 1.92166 learning rate: 0.100000\n",
      "Training set BPC at step 5000: 1.92701 learning rate: 0.090000\n",
      "================================================================================\n",
      "P reclod ro of  unk  by n t  s mr  ngepriine  das in a hag  on pline bil a hide \n",
      "ncerd as   N   usedsiented to repar sightly roses  unk  for an conected   N jowh\n",
      "n tocky have sharem but had  unk   unk nommof hore taal foret  unk  from disioni\n",
      "E emale wiowines only   posinge to copaic   burern  japing he and and prese in t\n",
      "W requarmed ritig foor   formis casalet   N s op purhas of flominispoges  partra\n",
      "================================================================================\n",
      "Validation set BPC: 1.87237\n",
      "Training set BPC at step 5100: 1.81922 learning rate: 0.090000\n",
      "Calculating BPC on test dataset\n",
      "Final test set BPC: 1.84416\n"
     ]
    }
   ],
   "source": [
    "# How often the test loss on validation batch will be computed. \n",
    "summary_frequency = 100\n",
    "\n",
    "# Create session.\n",
    "sess = tf.InteractiveSession()\n",
    "# Create summary writers, point them to LOG_DIR.\n",
    "train_writer = tf.summary.FileWriter(LOG_DIR + '/train', sess.graph)\n",
    "valid_writer = tf.summary.FileWriter(LOG_DIR + '/valid')\n",
    "test_writer = tf.summary.FileWriter(LOG_DIR + '/test')\n",
    "\n",
    "# Initialize global variables.\n",
    "tf.global_variables_initializer().run()\n",
    "print('Initialized')\n",
    "\n",
    "num_steps =  train_size // (BATCH_SIZE*SEQ_LENGTH) #70001\n",
    "print(\"Number of iterations per epoch =\", num_steps)\n",
    "for step in range(num_steps):\n",
    "    # Run training graph.\n",
    "    batch = train_batches.next()\n",
    "    summary, _, t_loss, lr = sess.run([merged_summaries, optimizer, loss, learning_rate], \n",
    "                                      feed_dict=create_feed_dict(\"train\"))\n",
    "    # Add summary.\n",
    "    train_writer.add_summary(summary, step*BATCH_SIZE*SEQ_LENGTH)\n",
    "    train_writer.flush()\n",
    "\n",
    "    # Every (100) steps collect statistics.\n",
    "    if step % summary_frequency == 0:\n",
    "      # Print loss from last batch.\n",
    "      print('Training set BPC at step %d: %0.5f learning rate: %f' % (step, t_loss, lr))\n",
    "    \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate sample text...\n",
    "        print('=' * 80)\n",
    "        # consisting of 5 lines...\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          # Reset LSTM hidden state.\n",
    "          reset_sample_state.run()\n",
    "          # with 79 characters in each.\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "        \n",
    "        # Validation set BPC.\n",
    "        v_summary, v_loss = sess.run([merged_summaries, loss], feed_dict=create_feed_dict(\"valid\"))\n",
    "        print(\"Validation set BPC: %.5f\" % v_loss)\n",
    "        valid_writer.add_summary(v_summary, step*BATCH_SIZE*SEQ_LENGTH)\n",
    "        valid_writer.flush()\n",
    "    # End of statistics collection\n",
    "\n",
    "# Test set BPC.\n",
    "print(\"Calculating BPC on test dataset\")\n",
    "t_summary, t_loss = sess.run([merged_summaries, loss], feed_dict=create_feed_dict(\"test\"))\n",
    "print(\"Final test set BPC: %.5f\" % t_loss)\n",
    "test_writer.add_summary(t_summary, step*BATCH_SIZE*SEQ_LENGTH)\n",
    "test_writer.flush()\n",
    "    \n",
    "# Close writers and session.\n",
    "train_writer.close()\n",
    "valid_writer.close()\n",
    "test_writer.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
