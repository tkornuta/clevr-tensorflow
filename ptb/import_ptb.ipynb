{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#from os import path\n",
    "import random\n",
    "#import tempfile\n",
    "#import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import random_seed\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "# Datasets = collections.namedtuple('Datasets', ['train', 'validation', 'test'])\n",
    "\n",
    "# Local dir where PRB files will be stored.\n",
    "PTB_DIR = '/home/tkornuta/data/ptb/'\n",
    "# Filenames.\n",
    "TRAIN = \"ptb.char.train.txt\"\n",
    "VALID = \"ptb.char.valid.txt\"\n",
    "TEST = \"ptb.char.test.txt\"\n",
    "\n",
    "# Number of characters in a single phrase.\n",
    "PHRASE_LENGTH=100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _parse_document(filename):\n",
    "    \"\"\"Parses document using space as delimiter.\"\"\"\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        return f.read().replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "def _build_vocab(filename):\n",
    "    \"\"\"Builds and returns a vocabulary for a given document.\"\"\"\n",
    "    # Parse document.\n",
    "    data = _parse_document(filename)\n",
    "    # Transform data to dictionary (key - value)\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    # Returns dictionary that can be used for decoding of the document.\n",
    "    return word_to_id\n",
    "\n",
    "def encode_doc_to_one_hot(dense_data_vector, num_classes):\n",
    "    \"\"\"Convert data from dense vector of scalars to vector of one-hot vectors.\"\"\"\n",
    "    num_labels = len(dense_data_vector)\n",
    "    result = np.zeros(shape=(num_labels, num_classes))\n",
    "    result[np.arange(num_labels), dense_data_vector] = 1\n",
    "    return result.astype(int)\n",
    "\n",
    "def _extract_document(filename, word_to_id_dict, one_hot=False):\n",
    "    \"\"\"Reades a document and encodeds it using a dictionary.\"\"\"\n",
    "    data = _parse_document(filename)\n",
    "    encoded_doc = [word_to_id[word] for word in data if word in word_to_id]\n",
    "    if one_hot == True:\n",
    "        return encode_doc_to_one_hot(encoded_doc, len(word_to_id))\n",
    "    # else: \n",
    "    return encoded_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = os.path.join(PTB_DIR, TRAIN)\n",
    "\n",
    "# Build dictionary\n",
    "word_to_id = _build_vocab(datafile)\n",
    "print (\"Vocabulary =\", word_to_id)\n",
    "\n",
    "#num_classes = len(word_to_id)\n",
    "#print (\"Vocabulary size =\", num_classes)\n",
    "\n",
    "#parsed_doc = _parse_document(datafile)\n",
    "#print (\"Document =\", parsed_doc[0:10])\n",
    "\n",
    "#encoded_doc = encode_doc_to_one_hot(datafile, word_to_id)\n",
    "#print (\"Encoded Document =\",encoded_doc[0:10])\n",
    "\n",
    "#one_hot_doc = dense_to_one_hot (encoded_doc, num_classes)\n",
    "#print (\"One-hot encoded document =\", one_hot_doc[0:10])\n",
    "\n",
    "encoded_doc = _extract_document(datafile, word_to_id, False)\n",
    "print(\"Encoded Document =\", encoded_doc[0:10])\n",
    "\n",
    "doc_size = len(encoded_doc)\n",
    "print(\"Number of elements in document = \", doc_size)\n",
    "\n",
    "#mydict = {'george':16,'amber':19}\n",
    "#print(list(mydict.keys())[list(mydict.values()).index(16)]) # Prints george"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP_PHRASE_LENGTH = 2\n",
    "# Divide document into phrases of a given size.\n",
    "print (\"Doc =\", encoded_doc[0:500])\n",
    "print(\"Number of elements in document = \", doc_size)\n",
    "num_phrases = 10 #int(doc_size/PHRASE_LENGTH)\n",
    "print (\"Number of phrases = \", num_phrases)\n",
    "# Process data into phrases.\n",
    "phrases = np.array([encoded_doc[i*TMP_PHRASE_LENGTH:(i+1)*TMP_PHRASE_LENGTH] for i in range(num_phrases)])\n",
    "print(\"Phrase[0] =\", phrases[0])\n",
    "labels = np.array([encoded_doc[i*TMP_PHRASE_LENGTH+1:(i+1)*TMP_PHRASE_LENGTH+1] for i in range(num_phrases)])\n",
    "print(\"Labels[1] =\", labels[0])\n",
    "perm = np.arange(num_phrases)\n",
    "print(\"Indices =\",perm)\n",
    "\n",
    "print(\"Shuffling\")\n",
    "np.random.shuffle(perm)\n",
    "print(\"Indices =\",perm)\n",
    "phrases = phrases[perm]\n",
    "print(\"Phrase[0] =\", phrases[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset helper class for storing parsed text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextDataSet(object):\n",
    "\n",
    "  def __init__(self,\n",
    "               text,\n",
    "               phrase_length=100,\n",
    "               seed=None):\n",
    "    \"\"\"Construct a DataSet. Divides (already parsed and encoded) text data into phrases.\n",
    "    Seed arg provides for convenient deterministic testing.\n",
    "    \"\"\"\n",
    "    # Set seed.\n",
    "    seed1, seed2 = random_seed.get_seed(seed)\n",
    "    # If op level seed is not set, use whatever graph level seed is returned\n",
    "    np.random.seed(seed1 if seed is None else seed2)\n",
    "\n",
    "    self._text = text\n",
    "    self._phrase_length = phrase_length\n",
    "    self._epochs_completed = 0\n",
    "    self._index_in_epoch = 0\n",
    "    \n",
    "    # Divide document into phrases of a given size.\n",
    "    doc_size = len(text)\n",
    "    self._num_examples = int(doc_size/phrase_length)\n",
    "    # DATA: Process text into phrases.\n",
    "    self._data = np.array([text[i*phrase_length:(i+1)*phrase_length] for i in range(self._num_examples)])\n",
    "    # LABELS: Process text into phrases - label is next char, so shifted by one.\n",
    "    self._labels = np.array([text[i*phrase_length+1:(i+1)*phrase_length+1] for i in range(self._num_examples)])\n",
    "        \n",
    "  @property\n",
    "  def data(self):\n",
    "    return self._data\n",
    "\n",
    "  @property\n",
    "  def labels(self):\n",
    "    return self._labels\n",
    "\n",
    "  @property\n",
    "  def batch_length(self):\n",
    "    return self._batch_length\n",
    "\n",
    "  @property\n",
    "  def num_examples(self):\n",
    "    return self._num_examples\n",
    "\n",
    "  @property\n",
    "  def epochs_completed(self):\n",
    "    return self._epochs_completed\n",
    "\n",
    "  def next_batch(self, batch_size, shuffle=True):\n",
    "    \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "    start = self._index_in_epoch\n",
    "    # Shuffle for the first epoch\n",
    "    if self._epochs_completed == 0 and start == 0 and shuffle:\n",
    "      perm0 = np.arange(self._num_examples)\n",
    "      np.random.shuffle(perm0)\n",
    "      self._data = self.data[perm0]\n",
    "      self._labels = self.labels[perm0]\n",
    "    # Go to the next epoch\n",
    "    if start + batch_size > self._num_examples:\n",
    "      # Finished epoch\n",
    "      self._epochs_completed += 1\n",
    "      # Get the rest examples in this epoch\n",
    "      rest_num_examples = self._num_examples - start\n",
    "      data_rest_part = self._data[start:self._num_examples]\n",
    "      labels_rest_part = self._labels[start:self._num_examples]\n",
    "      # Shuffle the data\n",
    "      if shuffle:\n",
    "        perm = np.arange(self._num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        self._data = self.data[perm]\n",
    "        self._labels = self.labels[perm]\n",
    "      # Start next epoch\n",
    "      start = 0\n",
    "      self._index_in_epoch = batch_size - rest_num_examples\n",
    "      end = self._index_in_epoch\n",
    "      data_new_part = self._data[start:end]\n",
    "      labels_new_part = self._labels[start:end]\n",
    "      return numpy.concatenate((data_rest_part, data_new_part), axis=0) , numpy.concatenate((labels_rest_part, labels_new_part), axis=0)\n",
    "    else:\n",
    "      self._index_in_epoch += batch_size\n",
    "      end = self._index_in_epoch\n",
    "      return self._data[start:end], self._labels[start:end]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reads Penn Tree Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_ptb(dir,\n",
    "        phrase_length=100,\n",
    "        one_hot=False,\n",
    "        seed=None):\n",
    "\n",
    "    train_file = os.path.join(PTB_DIR, TRAIN)\n",
    "    valid_file = os.path.join(PTB_DIR, VALID)\n",
    "    test_file = os.path.join(PTB_DIR, TEST)\n",
    "\n",
    "    # Build dictionary on the basis of train data.\n",
    "    word_to_id = _build_vocab(train_file)\n",
    "    #print (word_to_id)   \n",
    "    \n",
    "    # Load data.\n",
    "    train_data = _extract_document(train_file, word_to_id, one_hot)\n",
    "    validaton_data = _extract_document(valid_file, word_to_id, one_hot)\n",
    "    test_data = _extract_document(test_file, word_to_id, one_hot)\n",
    "\n",
    "    options = dict(phrase_length=100,seed=seed)\n",
    "\n",
    "    # Create datasets.\n",
    "    train = TextDataSet(train_data, **options)\n",
    "    validation = TextDataSet(validaton_data, **options)\n",
    "    test = TextDataSet(test_data, **options)\n",
    "\n",
    "    return base.Datasets(train=train, validation=validation, test=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ptb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptb = read_ptb(PTB_DIR, PHRASE_LENGTH, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get next batch.\n",
    "print(ptb.train.next_batch(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
