{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import tarfile\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import shutil \n",
    "import random\n",
    "\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "# Dirs - must be absolute paths!\n",
    "LOG_DIR = '/tmp/tf/ptb_char_lstm_mann/10char/'\n",
    "# Local dir where PTB files will be stored.\n",
    "PTB_DIR = '/home/tkornuta/data/ptb/'\n",
    "\n",
    "# Filenames.\n",
    "TRAIN = \"ptb.train.txt\"\n",
    "VALID = \"ptb.valid.txt\"\n",
    "TEST = \"ptb.test.txt\"\n",
    "\n",
    "# Size of the hidden state 64\n",
    "HIDDEN_SIZE = 64\n",
    "\n",
    "# Size of the MANN memory.\n",
    "MEMORY_SIZE = 100\n",
    "\n",
    "# A batch size of 100\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# A single recurrent layer of number of units = sequences of length\n",
    "# e.g. 200 bytes\n",
    "SEQ_LENGTH = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check/maybe download PTB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified /home/tkornuta/data/ptb/simple-examples.tgz ( 34869662 )\n"
     ]
    }
   ],
   "source": [
    "def maybe_download_ptb(path, \n",
    "                       filename='simple-examples.tgz', \n",
    "                       url='http://www.fit.vutbr.cz/~imikolov/rnnlm/', \n",
    "                       expected_bytes =34869662):\n",
    "  # Eventually create the PTB dir.\n",
    "  if not tf.gfile.Exists(path):\n",
    "    tf.gfile.MakeDirs(path)\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  _filename = path+filename\n",
    "  if not os.path.exists(_filename):\n",
    "    print('Downloading %s...' % filename)\n",
    "    _filename, _ = urlretrieve(url+filename, _filename)\n",
    "  statinfo = os.stat(_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', (_filename), '(', statinfo.st_size, ')')\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + _filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download_ptb(PTB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract dataset-related files from the PTB archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_ptb(path, filename='simple-examples.tgz', files=[\"ptb.train.txt\", \"ptb.valid.txt\", \"ptb.test.txt\", \n",
    "                                       \"ptb.char.train.txt\", \"ptb.char.valid.txt\", \"ptb.char.test.txt\"]):\n",
    "    \"\"\"Extracts files from PTB archive.\"\"\"\n",
    "    # Extract\n",
    "    tar = tarfile.open(path+filename)\n",
    "    tar.extractall(path)\n",
    "    tar.close()\n",
    "    # Copy files\n",
    "    for file in files:\n",
    "        shutil.copyfile(PTB_DIR+\"simple-examples/data/\"+file, PTB_DIR+file)\n",
    "    # Delete directory\n",
    "    shutil.rmtree(PTB_DIR+\"simple-examples/\")        \n",
    "\n",
    "extract_ptb(PTB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train, valid and test texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5101618  aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memote\n",
      "399782  consumers may want to move their telephones a little closer to \n",
      "449945  no it was n't black monday \n",
      " but while the new york stock excha\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename, path):\n",
    "    with open(path+filename, 'r') as myfile:\n",
    "        data=myfile.read()# .replace('\\n', '')\n",
    "        return data\n",
    "\n",
    "train_text = read_data(TRAIN, PTB_DIR)\n",
    "train_size=len(train_text)\n",
    "print(train_size, train_text[:100])\n",
    "\n",
    "valid_text = read_data(VALID, PTB_DIR)\n",
    "valid_size=len(valid_text)\n",
    "print(valid_size, valid_text[:64])\n",
    "\n",
    "test_text = read_data(TEST, PTB_DIR)\n",
    "test_size=len(test_text)\n",
    "print(test_size, test_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size =  59\n",
      "65\n",
      "33 1 58 26 0 0\n",
      "a A\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 59 # [A-Z] + [a-z] + ' ' +few 'in between; + punctuation\n",
    "first_letter = ord(string.ascii_uppercase[0]) # ascii_uppercase before lowercase! \n",
    "print(\"vocabulary size = \", vocabulary_size)\n",
    "print(first_letter)\n",
    "\n",
    "def char2id(char):\n",
    "  \"\"\" Converts char to id (int) with one-hot encoding handling of unexpected characters\"\"\"\n",
    "  if char in string.ascii_letters:# or char in string.punctuation or char in string.digits:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    # print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  \"\"\" Converts single id (int) to character\"\"\"\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "#print(len(string.punctuation))\n",
    "#for i in string.ascii_letters:\n",
    "#    print (i, char2id(i))\n",
    "\n",
    "\n",
    "print(char2id('a'), char2id('A'), char2id('z'), char2id('Z'), char2id(' '), char2id('Ã¯'))\n",
    "print(id2char(char2id('a')), id2char(char2id('A')))\n",
    "#print(id2char(65), id2char(33), id2char(90), id2char(58), id2char(0))\n",
    "#bankno\n",
    "sample = np.zeros(shape=(1, vocabulary_size), dtype=np.float)\n",
    "sample[0, char2id(' ')] = 1.0\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper class for batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, seq_length, vocab_size):\n",
    "    \"\"\"\n",
    "    Initializes the batch generator object. Stores the variables and first \"letter batch\".\n",
    "    text is text to be processed\n",
    "    batch_size is size of batch (number of samples)\n",
    "    seq_length represents the length of sequence\n",
    "    vocab_size is number of words in vocabulary (assumes one-hot encoding)\n",
    "    \"\"\"\n",
    "    # Store input parameters.\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._seq_length = seq_length\n",
    "    self._vocab_size = vocab_size\n",
    "    # Divide text into segments depending on number of batches, each segment determines a cursor position for a batch.\n",
    "    segment = self._text_size // batch_size\n",
    "    # Set initial cursor position.\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    # Store first \"letter batch\".\n",
    "    self._last_letter_batch = self._next_letter_batch()\n",
    "  \n",
    "  def _next_letter_batch(self):\n",
    "    \"\"\"\n",
    "    Returns a batch containing of encoded single letters depending on the current batch \n",
    "    cursor positions in the data.\n",
    "    Returned \"letter batch\" is of size batch_size x vocab_size\n",
    "    \"\"\"\n",
    "    letter_batch = np.zeros(shape=(self._batch_size, self._vocab_size), dtype=np.float)\n",
    "    # Iterate through \"samples\"\n",
    "    for b in range(self._batch_size):\n",
    "      # Set 1 in position pointed out by one-hot char encoding.\n",
    "      letter_batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return letter_batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    # First add last letter from previous batch (the \"additional one\").\n",
    "    batches = [self._last_letter_batch]\n",
    "    for step in range(self._seq_length):\n",
    "      batches.append(self._next_letter_batch())\n",
    "    # Store last \"letter batch\" for next batch.\n",
    "    self._last_letter_batch = batches[-1]\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "(100, 59)\n"
     ]
    }
   ],
   "source": [
    "# Trick - override first 10 chars\n",
    "#list1 = list(train_text)\n",
    "#for i in range(2):\n",
    "#    list1[i] = 'z'\n",
    "#train_text = ''.join(list1)\n",
    "#print(\"Train set =\", train_text[0:100])\n",
    "\n",
    "# Create objects for training, validation and testing batch generation.\n",
    "train_batches = BatchGenerator(train_text, BATCH_SIZE, SEQ_LENGTH, vocabulary_size)\n",
    "\n",
    "# Get first training batch.\n",
    "batch = train_batches.next()\n",
    "print(len(batch))\n",
    "print(batch[0].shape)\n",
    "#print(\"Batch = \", batch)\n",
    "#print(batches2string(batch))\n",
    "#print(\"batch len = num of enrollings\",len(batch))\n",
    "#for i in range(num_unrollings):\n",
    "#    print(\"i = \", i, \"letter=\", batches2string(batch)[0][i][0], \"bits = \", batch[i][0])\n",
    "\n",
    "\n",
    "# For validation  - process the whole text as one big batch.\n",
    "VALID_BATCH_SIZE = int(np.floor(valid_size/SEQ_LENGTH))\n",
    "valid_batches = BatchGenerator(valid_text, VALID_BATCH_SIZE, SEQ_LENGTH, vocabulary_size)\n",
    "valid_batch = valid_batches.next()\n",
    "#print (VALID_BATCH_SIZE)\n",
    "#print(len(valid_batch))\n",
    "#print(valid_batch[0].shape)\n",
    "\n",
    "# For texting  - process the whole text as one big batch.\n",
    "TEST_BATCH_SIZE = int(np.floor(test_size/SEQ_LENGTH))\n",
    "test_batches = BatchGenerator(test_text, TEST_BATCH_SIZE, SEQ_LENGTH, vocabulary_size)\n",
    "# Get single batch! \n",
    "test_batch = test_batches.next()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function defining the LSTM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(input_, prev_output_, prev_cell_state_, name_):\n",
    "    \"\"\"Create a LSTM cell\"\"\"\n",
    "    with tf.name_scope(name_):\n",
    "        # Equations according to C. Olah blog.\n",
    "        \n",
    "        # Concatenate h_prev (\"prev output\") with x.\n",
    "        h_prev_x = tf.concat([prev_cell_state_, input_], 1)\n",
    "        \n",
    "        # Calculate forget and input gates activations.\n",
    "        forget_gate = tf.sigmoid(tf.matmul(h_prev_x, Wf) + bf, name=\"forget_gate\")\n",
    "        input_gate = tf.sigmoid(tf.matmul(h_prev_x, Wi) + bi, name=\"Input_gate\")\n",
    "\n",
    "        # Update of the cell state C~.\n",
    "        cell_update = tf.tanh(tf.matmul(h_prev_x, Wc) + bc, name=\"Cell_update\")\n",
    "        # New cell state C.\n",
    "        cell_state = tf.add(forget_gate * prev_cell_state_, input_gate * cell_update, name = \"Cell_state\")\n",
    "        \n",
    "        # Calculate output gate.\n",
    "        output_gate = tf.sigmoid(tf.matmul(h_prev_x, Wo) + bo, name=\"Output_gate\")\n",
    "        # Calculate h - \"output\".\n",
    "        output = output_gate * tf.tanh(cell_state)\n",
    "\n",
    "        return output, cell_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Definition of tensor graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_buffer shape = <unknown>\n",
      "Seq length  = 10\n",
      "Batch shape = <unknown>\n",
      "prev_cell_state_ shape:  <unknown>\n",
      "input_ shape:  <unknown>\n",
      "h_prev_x shape:  <unknown>\n",
      "prev_cell_state_ shape:  <unknown>\n",
      "input_ shape:  <unknown>\n",
      "h_prev_x shape:  <unknown>\n",
      "prev_cell_state_ shape:  <unknown>\n",
      "input_ shape:  <unknown>\n",
      "h_prev_x shape:  <unknown>\n",
      "prev_cell_state_ shape:  <unknown>\n",
      "input_ shape:  <unknown>\n",
      "h_prev_x shape:  <unknown>\n",
      "prev_cell_state_ shape:  <unknown>\n",
      "input_ shape:  <unknown>\n",
      "h_prev_x shape:  <unknown>\n",
      "prev_cell_state_ shape:  <unknown>\n",
      "input_ shape:  <unknown>\n",
      "h_prev_x shape:  <unknown>\n",
      "prev_cell_state_ shape:  <unknown>\n",
      "input_ shape:  <unknown>\n",
      "h_prev_x shape:  <unknown>\n",
      "prev_cell_state_ shape:  <unknown>\n",
      "input_ shape:  <unknown>\n",
      "h_prev_x shape:  <unknown>\n",
      "prev_cell_state_ shape:  <unknown>\n",
      "input_ shape:  <unknown>\n",
      "h_prev_x shape:  <unknown>\n",
      "prev_cell_state_ shape:  <unknown>\n",
      "input_ shape:  <unknown>\n",
      "h_prev_x shape:  <unknown>\n",
      "10\n",
      "<unknown>\n",
      "<unknown>\n"
     ]
    }
   ],
   "source": [
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 0. Shared variables ops.\n",
    "with tf.name_scope(\"Shared_Variables\"):\n",
    "  # Define parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  Wf = tf.Variable(tf.truncated_normal([vocabulary_size+HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"Wf\")\n",
    "  bf = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"bf\")\n",
    "\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  Wi = tf.Variable(tf.truncated_normal([vocabulary_size+HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"Wi\")\n",
    "  bi = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"bi\")\n",
    "\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  Wc = tf.Variable(tf.truncated_normal([vocabulary_size+HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"Wc\")\n",
    "  bc = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"bc\")\n",
    "\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  Wo = tf.Variable(tf.truncated_normal([vocabulary_size+HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"Wo\")\n",
    "  bo = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"bo\")\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, vocabulary_size], -0.1, 0.1), name=\"w\")\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]), name=\"b\")\n",
    "  \n",
    "  # Placeholders for previous (the oldest) state and output.\n",
    "  prev_output = tf.placeholder(tf.float32, shape=None, name=\"prev_output\")\n",
    "  prev_state = tf.placeholder(tf.float32, shape=None, name=\"prev_state\")\n",
    "\n",
    "# 0. Memory\n",
    "with tf.name_scope(\"Memory\"):\n",
    "  # Memory block\n",
    "  M = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, MEMORY_SIZE], -0.1, 0.1), traninable=False, name=\"M\")\n",
    "  # Variable storing similarity.\n",
    "  D = tf.Variable(tf.zeros([1, MEMORY_SIZE]), traninable=False, name=\"D\")\n",
    "normalize_a = tf.nn.l2_normalize(a,0)        \n",
    "normalize_b = tf.nn.l2_normalize(b,0)\n",
    "cos_similarity=tf.reduce_sum(tf.multiply(normalize_a,normalize_b))\n",
    "\n",
    "# 0. Placeholders for inputs.\n",
    "with tf.name_scope(\"Input_data\"):\n",
    "  # Define input data buffers.\n",
    "  input_buffer = list()\n",
    "  for _ in range(SEQ_LENGTH + 1):\n",
    "    # Collect placeholders for inputs/labels.\n",
    "    input_buffer.append(tf.placeholder(tf.float32, shape=None, name=\"Input_data\"))\n",
    "  print (\"input_buffer shape =\", input_buffer[0].shape)\n",
    "  # Collection of training inputs.\n",
    "  train_inputs = input_buffer[:SEQ_LENGTH]\n",
    "  # Labels are pointing to the same placeholders!\n",
    "  # Labels are inputs shifted by one time step.\n",
    "  train_labels = input_buffer[1:]  \n",
    "  print (\"Seq length  =\", len(train_inputs))\n",
    "  print (\"Batch shape =\", train_inputs[0].shape)\n",
    "  # Concatenate targets into 2D tensor.\n",
    "  targets = tf.concat(train_labels, 0)\n",
    "\n",
    " # 2. Training LSTM ops.\n",
    "with tf.name_scope(\"LSTM\"):\n",
    "  # Unrolled LSTM loop.\n",
    "  # Build outpus of size SEQ_LENGTH.\n",
    "  outputs = list()\n",
    "  output = prev_output\n",
    "  state = prev_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state, \"cell\")\n",
    "    outputs.append(output)\n",
    "  print (len(outputs))\n",
    "  print (outputs[0].shape)\n",
    "  print (tf.concat(outputs, 0).shape)\n",
    "\n",
    "# Fully connected layer on top => classification.\n",
    "# In fact we will create lots of FC layers (one for each output layer), with shared weights.\n",
    "logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b, name = \"Final_FC\")\n",
    "\n",
    "# 2. Loss ops.\n",
    "with tf.name_scope(\"Loss\"):\n",
    "    # Loss function(s) - one for every output generated by every lstm cell.\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=targets, logits=logits))\n",
    "    # Add loss summary.\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "# 3. Training ops.  \n",
    "with tf.name_scope(\"Optimization\"):\n",
    "  # Optimizer-related variables.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "# 4. Predictions ops.  \n",
    "with tf.name_scope(\"Evaluation\") as scope:\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgraph responsible for generation of sample texts, char by char."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prev_cell_state_ shape:  (1, 64)\n",
      "input_ shape:  (1, 59)\n",
      "h_prev_x shape:  (1, 123)\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"Sample_generation\") as scope:\n",
    "  # Create graphs for sampling and validation evaluation: batch 1, \"no unrolling\".\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size], name=\"Input_data\")\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"Output_data\")\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"Hidden_state\")\n",
    "\n",
    "  # Node responsible for resetting the state and output.\n",
    "  reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, HIDDEN_SIZE])),\n",
    "      saved_sample_state.assign(tf.zeros([1, HIDDEN_SIZE])))\n",
    "  # Single LSTM cell.\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state, \"cell\")\n",
    "  # Output depends on the hidden state.\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b, name=\"logits\"), name=\"outputs\")\n",
    "\n",
    "# Merge all summaries.\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "# 4. Init global variable.\n",
    "init = tf.global_variables_initializer() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for language generation (letter sampling etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_feed_dict(dataset):\n",
    "  \"\"\"Creates a dictionaries for different sets: maps data onto Tensor placeholders.\"\"\"\n",
    "  feed_dict = dict()\n",
    "  if dataset==\"train\":\n",
    "    # Get next batch and create a feed dict.\n",
    "    next_batch = train_batches.next()\n",
    "    for i in range(SEQ_LENGTH + 1):\n",
    "        feed_dict[input_buffer[i]] = next_batch[i]\n",
    "    # Reset previous state and output\n",
    "    feed_dict[prev_output] = np.zeros([BATCH_SIZE, HIDDEN_SIZE])\n",
    "    feed_dict[prev_state] = np.zeros([BATCH_SIZE, HIDDEN_SIZE])\n",
    "        \n",
    "  elif dataset==\"valid\":\n",
    "    for i in range(SEQ_LENGTH + 1):\n",
    "        feed_dict[input_buffer[i]] = valid_batch[i]\n",
    "    # Reset previous state and output\n",
    "    feed_dict[prev_output] = np.zeros([VALID_BATCH_SIZE, HIDDEN_SIZE])\n",
    "    feed_dict[prev_state] = np.zeros([VALID_BATCH_SIZE, HIDDEN_SIZE])\n",
    "    \n",
    "  else: # test\n",
    "    for i in range(SEQ_LENGTH + 1):\n",
    "        feed_dict[input_buffer[i]] = test_batch[i]\n",
    "    # Reset previous state and output\n",
    "    feed_dict[prev_output] = np.zeros([TEST_BATCH_SIZE, HIDDEN_SIZE])\n",
    "    feed_dict[prev_state] = np.zeros([TEST_BATCH_SIZE, HIDDEN_SIZE])\n",
    "    \n",
    "  return feed_dict # {prev_output: train_output_zeros, prev_state: train_state_zeros }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Eventually clear the log dir.\n",
    "if tf.gfile.Exists(LOG_DIR):\n",
    "  tf.gfile.DeleteRecursively(LOG_DIR)\n",
    "# Create (new) log dir.\n",
    "tf.gfile.MakeDirs(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Number of iterations per epoch = 5101\n",
      "Training set BPC at step 0: 4.08192 learning rate: 10.000000\n",
      "================================================================================\n",
      "OfwueChbEaSWAL\\uac Tci[qw^nNeaVn_Sv]eQVHmeo VjaEaAZ^ ^sWthCubLnxssiUeD JiFLuttHf\n",
      "v^ f v SeTonlb^kDYe f^oPKpu\\KRuN CboaIx m  IXeatWmeFSwr\\OJHoIdDxaoW iWedx[qt^uuq\n",
      "RjxBgkH Dss^s]myEYrfoA`FDI [ R gQ COca\\d ObU ymfseSSn oaoGIioAkfn o pokQ jiVkap \n",
      "pz ves c cyAikoktQQsoeRgnb\\Z dsW^nTi S hs vwmw\\_aiulda S QIHsp[aapaVIi  fAJohJD \n",
      "HcOhnt eiBYtiUjL Enft^iraFfP EwswoHiELbnsDZH en\\kIyEig ^sJ^]FejiofqrJ vr]wt_ _Au\n",
      "================================================================================\n",
      "Validation set BPC: 3.43591\n",
      "Training set BPC at step 100: 2.37577 learning rate: 10.000000\n",
      "Training set BPC at step 200: 2.19257 learning rate: 10.000000\n",
      "Training set BPC at step 300: 1.96942 learning rate: 10.000000\n",
      "Training set BPC at step 400: 1.96462 learning rate: 10.000000\n",
      "Training set BPC at step 500: 1.92028 learning rate: 10.000000\n",
      "Training set BPC at step 600: 1.85930 learning rate: 10.000000\n",
      "Training set BPC at step 700: 1.87266 learning rate: 10.000000\n",
      "Training set BPC at step 800: 1.83110 learning rate: 10.000000\n",
      "Training set BPC at step 900: 1.76788 learning rate: 10.000000\n",
      "Training set BPC at step 1000: 1.80032 learning rate: 10.000000\n",
      "================================================================================\n",
      "Ya llawday   thouth dounging that poting arenbeagely activion a  unk  can comsti\n",
      "Ming of the nationawy it hitpee for would moarded of tockry i  a produc of N N w\n",
      "Aopse N N yainien senthounded they davesed by its out everot wancenter year the \n",
      "fugt southirs rankers ertime not templetit most of yewern com  N a for inting to\n",
      "Lolists entals is its vox teepnew of that than the pasud compolitiles which bide\n",
      "================================================================================\n",
      "Validation set BPC: 1.77883\n",
      "Training set BPC at step 1100: 1.76834 learning rate: 10.000000\n",
      "Training set BPC at step 1200: 1.71017 learning rate: 10.000000\n",
      "Training set BPC at step 1300: 1.73149 learning rate: 10.000000\n",
      "Training set BPC at step 1400: 1.71717 learning rate: 10.000000\n",
      "Training set BPC at step 1500: 1.75696 learning rate: 10.000000\n",
      "Training set BPC at step 1600: 1.69532 learning rate: 10.000000\n",
      "Training set BPC at step 1700: 1.76090 learning rate: 10.000000\n",
      "Training set BPC at step 1800: 1.71824 learning rate: 10.000000\n",
      "Training set BPC at step 1900: 1.73324 learning rate: 10.000000\n",
      "Training set BPC at step 2000: 1.68234 learning rate: 10.000000\n",
      "================================================================================\n",
      "h wassignts while cutorer inyreaster conlodion is and was inpended demany lion u\n",
      "m its  s move said treet of a  unk    been share  s  unk  lident toal   not is n\n",
      "W fiumaties afretor eyposite how of the eloco offices it acted edoution howsuy b\n",
      "H to pake at N and whilt N N from the   doun to was genenuly to point remairme e\n",
      "B that the and vanue onerers   thas a selraprity to also the epenga wosed find f\n",
      "================================================================================\n",
      "Validation set BPC: 1.68972\n",
      "Training set BPC at step 2100: 1.68434 learning rate: 10.000000\n",
      "Training set BPC at step 2200: 1.64389 learning rate: 10.000000\n",
      "Training set BPC at step 2300: 1.61716 learning rate: 10.000000\n",
      "Training set BPC at step 2400: 1.69828 learning rate: 10.000000\n",
      "Training set BPC at step 2500: 1.68510 learning rate: 10.000000\n",
      "Training set BPC at step 2600: 1.67503 learning rate: 10.000000\n",
      "Training set BPC at step 2700: 1.67667 learning rate: 10.000000\n",
      "Training set BPC at step 2800: 1.66567 learning rate: 10.000000\n",
      "Training set BPC at step 2900: 1.64654 learning rate: 10.000000\n",
      "Training set BPC at step 3000: 1.70220 learning rate: 10.000000\n",
      "================================================================================\n",
      "n finance on that the this heve in may bill seeminss   for the N   stain part ab\n",
      "Zs   warner results   the sige externed also reascourhates  unk    claary that a\n",
      "t will an angue unlows amountinuer   in  unk  high belietveel program agains was\n",
      "[ involume  s this toman nether make conthrisher liar liming ward mitcrate stock\n",
      "P the was basec to  unk  quarter  unk  for and clostish   prisketing the puthail\n",
      "================================================================================\n",
      "Validation set BPC: 1.65297\n",
      "Training set BPC at step 3100: 1.60966 learning rate: 10.000000\n",
      "Training set BPC at step 3200: 1.67881 learning rate: 10.000000\n",
      "Training set BPC at step 3300: 1.68878 learning rate: 10.000000\n",
      "Training set BPC at step 3400: 1.62763 learning rate: 10.000000\n",
      "Training set BPC at step 3500: 1.63225 learning rate: 10.000000\n",
      "Training set BPC at step 3600: 1.59499 learning rate: 10.000000\n",
      "Training set BPC at step 3700: 1.71400 learning rate: 10.000000\n",
      "Training set BPC at step 3800: 1.65603 learning rate: 10.000000\n",
      "Training set BPC at step 3900: 1.57934 learning rate: 10.000000\n",
      "Training set BPC at step 4000: 1.68711 learning rate: 10.000000\n",
      "================================================================================\n",
      "he quarter barker renuie considered N to jan thouse of at clivictials they in th\n",
      "was that han the tue mutaim process them is ng the due count in the turnnugures \n",
      "Z that i south its sales in edary   leverty andiddutional estimate the pro  for \n",
      "cical with   in funds exchange chief who rose arrament two cents orck grose was \n",
      "D as the buy new  unk  are  unk  at powered a gang will meers polling or u s  fi\n",
      "================================================================================\n",
      "Validation set BPC: 1.63534\n",
      "Training set BPC at step 4100: 1.63355 learning rate: 10.000000\n",
      "Training set BPC at step 4200: 1.58254 learning rate: 10.000000\n",
      "Training set BPC at step 4300: 1.67755 learning rate: 10.000000\n",
      "Training set BPC at step 4400: 1.66549 learning rate: 10.000000\n",
      "Training set BPC at step 4500: 1.67787 learning rate: 10.000000\n",
      "Training set BPC at step 4600: 1.65745 learning rate: 10.000000\n",
      "Training set BPC at step 4700: 1.62350 learning rate: 10.000000\n",
      "Training set BPC at step 4800: 1.66060 learning rate: 10.000000\n",
      "Training set BPC at step 4900: 1.63716 learning rate: 10.000000\n",
      "Training set BPC at step 5000: 1.62083 learning rate: 1.000000\n",
      "================================================================================\n",
      "lataction issuates nuld has cltion since  unk  in the chief computer on the who \n",
      "x a  unk  withe one recent   N million are it of the be intend one by rave   but\n",
      "Kor raised thas calif  unk  year  unk    consider the redest was one fed through\n",
      "ders ongred wive conpersion explain building open outsore in buy causic in the p\n",
      "H uchar in the noter indire months only thosaven   the is a   a francally who hi\n",
      "================================================================================\n",
      "Validation set BPC: 1.61750\n",
      "Training set BPC at step 5100: 1.57456 learning rate: 1.000000\n",
      "Calculating BPC on test dataset\n",
      "Final test set BPC: 1.57290\n"
     ]
    }
   ],
   "source": [
    "# How often the test loss on validation batch will be computed. \n",
    "summary_frequency = 100\n",
    "\n",
    "# Create session.\n",
    "sess = tf.InteractiveSession()\n",
    "# Create summary writers, point them to LOG_DIR.\n",
    "train_writer = tf.summary.FileWriter(LOG_DIR + '/train', sess.graph)\n",
    "valid_writer = tf.summary.FileWriter(LOG_DIR + '/valid')\n",
    "test_writer = tf.summary.FileWriter(LOG_DIR + '/test')\n",
    "\n",
    "# Initialize global variables.\n",
    "tf.global_variables_initializer().run()\n",
    "print('Initialized')\n",
    "\n",
    "num_steps =  train_size // (BATCH_SIZE*SEQ_LENGTH) #70001\n",
    "print(\"Number of iterations per epoch =\", num_steps)\n",
    "for step in range(num_steps):\n",
    "    # Run training graph.\n",
    "    batch = train_batches.next()\n",
    "    summary, _, t_loss, lr = sess.run([merged_summaries, optimizer, loss, learning_rate], \n",
    "                                      feed_dict=create_feed_dict(\"train\"))\n",
    "    # Add summary.\n",
    "    train_writer.add_summary(summary, step*SEQ_LENGTH)\n",
    "    train_writer.flush()\n",
    "\n",
    "    # Every (100) steps collect statistics.\n",
    "    if step % summary_frequency == 0:\n",
    "      # Print loss from last batch.\n",
    "      print('Training set BPC at step %d: %0.5f learning rate: %f' % (step, t_loss, lr))\n",
    "    \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate sample text...\n",
    "        print('=' * 80)\n",
    "        # consisting of 5 lines...\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          # Reset LSTM hidden state.\n",
    "          reset_sample_state.run()\n",
    "          # with 79 characters in each.\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "        \n",
    "        # Validation set BPC.\n",
    "        v_summary, v_loss = sess.run([merged_summaries, loss], feed_dict=create_feed_dict(\"valid\"))\n",
    "        print(\"Validation set BPC: %.5f\" % v_loss)\n",
    "        valid_writer.add_summary(v_summary, step*SEQ_LENGTH)\n",
    "        valid_writer.flush()\n",
    "    # End of statistics collection\n",
    "\n",
    "# Test set BPC.\n",
    "print(\"Calculating BPC on test dataset\")\n",
    "t_summary, t_loss = sess.run([merged_summaries, loss], feed_dict=create_feed_dict(\"test\"))\n",
    "print(\"Final test set BPC: %.5f\" % t_loss)\n",
    "test_writer.add_summary(t_summary, step*SEQ_LENGTH)\n",
    "test_writer.flush()\n",
    "    \n",
    "# Close writers and session.\n",
    "train_writer.close()\n",
    "valid_writer.close()\n",
    "test_writer.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"map_1/TensorArrayStack/TensorArrayGatherV3:0\", shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
