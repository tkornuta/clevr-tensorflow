{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Graph.\n",
    "vif = tf.placeholder(tf.float32, shape=[2])\n",
    "\n",
    "#out = vif + tf.cast(tf.cast(vif, tf.int32), tf.float32)\n",
    "out = vif + tf.floor(vif)\n",
    "grad = tf.gradients(tf.reduce_sum(out), vif)\n",
    "\n",
    "ceil = tf.ceil(vif)\n",
    "floor = tf.floor(vif)\n",
    "trunk = tf.minimum(tf.maximum(vif, -1), 1)\n",
    "\n",
    "# Session.\n",
    "f = np.array([2.99, -1.1], dtype='float32')\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "#print (sess.run(grad, feed_dict={vif: f}))  # print 2\n",
    "print (sess.run([floor, ceil, trunk], feed_dict={vif: f}))  # print 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different approach - change memory \"orientation\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# place holders \n",
    "a = tf.placeholder(tf.float32, shape=None)\n",
    "b = tf.placeholder(tf.float32, shape=None)\n",
    "\n",
    "op = a * b\n",
    "op2 = tf.tensordot(tf.transpose(a), b, axes=1)\n",
    "\n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "    \n",
    "# Values\n",
    "va= [[1,2,3],[2,3,4]]\n",
    "vb= [[-1,-2,-3],[2,3,4]]\n",
    "\n",
    "# Initialize session.\n",
    "sess=tf.InteractiveSession()\n",
    "sess.run(initialize_model)\n",
    "# Execute graph.\n",
    "print(\"a*b =\\n\",sess.run([op], feed_dict={a:va, b:vb}))\n",
    "print(\"a dot b =\\n\",sess.run([op2], feed_dict={a:va, b:vb}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorts 2D tensor, each row separatelly.\n",
    "a = tf.Variable([[0.51, 0.52, 0.53, 0.94, 0.35],\n",
    "             [0.32, 0.72, 0.83, 0.74, 0.55],\n",
    "             [0.23, 0.72, 0.63, 0.64, 0.35],\n",
    "             [0.11, 0.02, 0.03, 0.14, 0.15],\n",
    "             [0.01, 0.72, 0.73, 0.04, 0.75]],tf.float32)\n",
    "\n",
    "row_size = a.get_shape().as_list()[-1]\n",
    "top_k = tf.nn.top_k(-a, k=row_size)\n",
    "\n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "    \n",
    "# Execute graph.\n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(initialize_model)\n",
    "\n",
    "print(sess.run(-top_k.values))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to perform operation on arrays.\n",
    "elems = np.array([1, 2, 3, 4, 5, 6])\n",
    "#squares = tf.map_fn(lambda x: x * x, elems)\n",
    "squares = tf.multiply(elems, elems)\n",
    "# squares == [1, 4, 9, 16, 25, 36]\n",
    "\n",
    "sess=tf.Session()\n",
    "print(sess.run(squares))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "elems = (np.array([1, 2, 3]), np.array([-1, 1, -1]))\n",
    "alternate = tf.map_fn(lambda x: x[0] * x[1], elems, dtype=tf.int64)\n",
    "# alternate == [-1, 2, -3]\n",
    "\n",
    "sess=tf.Session()\n",
    "print(sess.run(alternate))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"Broadcasting ability of TF\"\n",
    "import tensorflow as tf\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.constant([[0, 1,2],[2, 3,2],[4, 5,2],[6, 7,2]], dtype=tf.float32)\n",
    "y = tf.constant([[0, 1,2],[-2, -1,2]], dtype=tf.float32)\n",
    "\n",
    "x_vect_len = int(x.shape[1])\n",
    "print(\"len=\",x_vect_len)\n",
    "x_ = tf.expand_dims(x, 0)\n",
    "y_ = tf.expand_dims(y, 1)\n",
    "z = tf.reshape(tf.add(x_, y_), [-1, x_vect_len])\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(z)\n",
    "print(x_.eval())\n",
    "print(y_.eval())\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# place holders \n",
    "batch = tf.placeholder(tf.float32, shape=[3,3], name=\"batch\")\n",
    "memory = tf.placeholder(tf.float32, shape=[2,3], name=\"memory\")\n",
    "# Cos similarity\n",
    "norm_batch = tf.nn.l2_normalize(batch,0) \n",
    "norm_memory = tf.nn.l2_normalize(memory,0)\n",
    "\n",
    "nb_ = tf.expand_dims(norm_batch, 0)\n",
    "nm_ = tf.expand_dims(norm_memory, 1)\n",
    "cos_similarity = tf.reshape(tf.add(nb_,nm_), [-1, 2])\n",
    "#cos_similarity = tf.reshape(tf.reduce_sum(tf.multiply(nb_,nm_)), [-1, 2])\n",
    "\n",
    "                            \n",
    "sess=tf.InteractiveSession()\n",
    "cos_sim=sess.run(cos_similarity,feed_dict={batch:[[1,2,3],[3,3,3],[1,2,6]],memory:[[2,4,6],[1,2,4]]})\n",
    "\n",
    "print(cos_sim)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Size of the hidden state 64\n",
    "HIDDEN_SIZE = 3\n",
    "\n",
    "# Size of the MANN memory.\n",
    "MEMORY_SIZE = 5\n",
    "\n",
    "# A batch size of 100\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# A single recurrent layer of number of units = sequences of length\n",
    "# e.g. 200 bytes\n",
    "SEQ_LENGTH = 10\n",
    "\n",
    "EPS = 1e-15\n",
    "\n",
    "# place holders \n",
    "batch = tf.placeholder(tf.float32, shape=None, name=\"batch\")\n",
    "#memory = tf.placeholder(tf.float32, shape=[None], name=\"memory\")\n",
    "memory = tf.Variable(tf.zeros([MEMORY_SIZE, HIDDEN_SIZE]), trainable=False, name=\"memory\")\n",
    "\n",
    "memory_set = memory.assign([[1, 0, 1],[0.1, 0.2 , 0.4],[ 0,0,0],[-0.3,0.2,0.3],[0, 1, 0]])\n",
    "\n",
    "# Create BATCH_SIZE placeholders for similarity - each MEMORY_SIZE x 1,  \n",
    "with tf.name_scope(\"similarity\"):\n",
    "  # Define similarity buffers.\n",
    "  similarity = list()\n",
    "  #for b in range(BATCH_SIZE):\n",
    "    # Collect placeholders for similarity.\n",
    "    #similarity.append(tf.placeholder(tf.float32, shape=[MEMORY_SIZE, 1], name=\"Similarity\"))\n",
    " \n",
    "  # Normalize\n",
    "  norm_batch = tf.nn.l2_normalize(batch,1) \n",
    "  norm_memory = tf.nn.l2_normalize(memory,1)\n",
    "  print(norm_batch[0])\n",
    "  # Define similarity buffers.\n",
    "  numerator_batch = list()\n",
    "  denominator_batch = list()\n",
    "  similarity_batch = list()\n",
    "\n",
    "  for b in range(BATCH_SIZE):\n",
    "    similarity_batch.append(tf.map_fn(lambda x: tf.reduce_sum(tf.multiply(norm_batch[b],x)), norm_memory))\n",
    "    # Read weight based on similarity.\n",
    "  read_weight = tf.nn.softmax(similarity_batch)\n",
    "    \n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(memory_set)\n",
    "print(\"memory =\",memory.eval())\n",
    "\n",
    "tmp_batch = [[1, 0, 1],[-0.3,0.3,0.3]]\n",
    "\n",
    "print(\"Batch=\",batch.eval(feed_dict={batch:tmp_batch}))\n",
    "print(\"Norm batch=\",norm_batch.eval(feed_dict={batch:tmp_batch}))\n",
    "\n",
    "\n",
    "num, den, sim, rw=sess.run([numerator_batch, denominator_batch, similarity_batch, read_weight],\n",
    "                           feed_dict={batch:tmp_batch})\n",
    "print(\"norm memory =\",norm_memory.eval())\n",
    "\n",
    "print(\"numerator_batch=\",num)\n",
    "print(\"denominator_batch=\\n\",den,\"\\n\")\n",
    "\n",
    "print(\"similarity=\",sim)\n",
    "print(\"rw=\", rw)\n",
    "\n",
    "\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory =\n",
      " [array([[ 1.        , -0.2       ],\n",
      "       [ 0.        ,  0.1       ],\n",
      "       [ 1.        , -0.40000001],\n",
      "       [ 2.        ,  0.5       ],\n",
      "       [ 3.        ,  0.1       ],\n",
      "       [ 4.        ,  0.89999998],\n",
      "       [ 5.        ,  0.80000001],\n",
      "       [ 6.        ,  0.30000001],\n",
      "       [ 1.        , -0.30000001]], dtype=float32)]\n",
      "min_blue =\n",
      " [array([[-0.40000001]], dtype=float32)]\n",
      "max_red =\n",
      " [array([[ 6.]], dtype=float32)]\n",
      "rgb_memory =\n",
      " [array([[[  42.5 ,    0.  ,   -0.  ],\n",
      "        [  -0.  ,    0.  ,  127.5 ]],\n",
      "\n",
      "       [[   0.  ,    0.  ,   -0.  ],\n",
      "        [   4.25,    0.  ,   -0.  ]],\n",
      "\n",
      "       [[  42.5 ,    0.  ,   -0.  ],\n",
      "        [  -0.  ,    0.  ,  255.  ]],\n",
      "\n",
      "       [[  85.  ,    0.  ,   -0.  ],\n",
      "        [  21.25,    0.  ,   -0.  ]],\n",
      "\n",
      "       [[ 127.5 ,    0.  ,   -0.  ],\n",
      "        [   4.25,    0.  ,   -0.  ]],\n",
      "\n",
      "       [[ 170.  ,    0.  ,   -0.  ],\n",
      "        [  38.25,    0.  ,   -0.  ]],\n",
      "\n",
      "       [[ 212.5 ,    0.  ,   -0.  ],\n",
      "        [  34.  ,    0.  ,   -0.  ]],\n",
      "\n",
      "       [[ 255.  ,    0.  ,   -0.  ],\n",
      "        [  12.75,    0.  ,   -0.  ]],\n",
      "\n",
      "       [[  42.5 ,    0.  ,   -0.  ],\n",
      "        [  -0.  ,    0.  ,  191.25]]], dtype=float32)]\n",
      "rgb_memory_reshaped =\n",
      " [array([[[[  42.5 ,    0.  ,   -0.  ],\n",
      "         [  -0.  ,    0.  ,  127.5 ]],\n",
      "\n",
      "        [[   0.  ,    0.  ,   -0.  ],\n",
      "         [   4.25,    0.  ,   -0.  ]],\n",
      "\n",
      "        [[  42.5 ,    0.  ,   -0.  ],\n",
      "         [  -0.  ,    0.  ,  255.  ]],\n",
      "\n",
      "        [[  85.  ,    0.  ,   -0.  ],\n",
      "         [  21.25,    0.  ,   -0.  ]],\n",
      "\n",
      "        [[ 127.5 ,    0.  ,   -0.  ],\n",
      "         [   4.25,    0.  ,   -0.  ]],\n",
      "\n",
      "        [[ 170.  ,    0.  ,   -0.  ],\n",
      "         [  38.25,    0.  ,   -0.  ]],\n",
      "\n",
      "        [[ 212.5 ,    0.  ,   -0.  ],\n",
      "         [  34.  ,    0.  ,   -0.  ]],\n",
      "\n",
      "        [[ 255.  ,    0.  ,   -0.  ],\n",
      "         [  12.75,    0.  ,   -0.  ]],\n",
      "\n",
      "        [[  42.5 ,    0.  ,   -0.  ],\n",
      "         [  -0.  ,    0.  ,  191.25]]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# RGB coloring of matrices (hot-cold) for visualization purposes.\n",
    "import tensorflow as tf\n",
    "\n",
    "HIDDEN_SIZE = 9\n",
    "MEMORY_SLOTS = 2\n",
    "EPS = 1e-10\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "memory = tf.transpose([[1.0, 0, 1, 2, 3, 4, 5, 6, 1],[-0.2, 0.1, -0.4, 0.5, 0.1, 0.9, 0.8, 0.3, -0.3]])\n",
    "\n",
    "zeros = tf.zeros_like(memory)\n",
    "# Get negative values only.\n",
    "neg = tf.less(memory, zeros)\n",
    "blue = tf.multiply(tf.cast(neg, tf.float32), memory)\n",
    "min_blue = tf.reduce_min(memory, axis=None, keep_dims=True) + EPS\n",
    "norm_blue = 255.0 * blue/min_blue\n",
    "# Get positive values only.\n",
    "pos = tf.greater(memory, zeros)\n",
    "red = tf.multiply(tf.cast(pos, tf.float32), memory)\n",
    "max_red = tf.reduce_max(memory, axis=None, keep_dims=True) + EPS\n",
    "norm_red = 255.0 * red/max_red\n",
    "# Stack them into three channel image with hot-cold values.\n",
    "rgb_memory = tf.stack([norm_red, zeros, norm_blue], axis=2)\n",
    "rgb_memory_reshaped = tf.reshape(rgb_memory, [1, HIDDEN_SIZE, MEMORY_SLOTS, 3])\n",
    "\n",
    "\n",
    "\n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "    \n",
    "# Execute graph.\n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(initialize_model)\n",
    "print(\"memory =\\n\", sess.run([memory]))\n",
    "print(\"min_blue =\\n\", sess.run([min_blue]))\n",
    "print(\"max_red =\\n\", sess.run([max_red]))\n",
    "\n",
    "print(\"rgb_memory =\\n\",sess.run([rgb_memory]))\n",
    "print(\"rgb_memory_reshaped =\\n\",sess.run([rgb_memory_reshaped]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def rest(x): return tf.gather(x, tf.range(1, tf.size(x)))\n",
    "\n",
    "x = tf.Variable([0., 1.])\n",
    "x_op = tf.assign(x, [0., 1., 2.], validate_shape=False)\n",
    "\n",
    "with tf.control_dependencies([x_op]):\n",
    "    true_fun  = lambda: tf.assign(x, rest(x_op), validate_shape=False)\n",
    "    false_fun = lambda: tf.constant([])\n",
    "    pred = tf.constant(True)\n",
    "    cond_op = tf.cond(pred, true_fun, false_fun)\n",
    "\n",
    "with tf.Session(\"\"):\n",
    "  x.initializer.run()\n",
    "  print(cond_op.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finds given top elements\n",
    "import tensorflow as tf\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tmp_batch = [[1, 0, 1, 2, 3, 4, 5, 6, 1],[-0.2, 0.1, -0.4, 0.5, 0.1, 0.9, 0.8, 0.3, -0.3]]\n",
    "\n",
    "\n",
    "top_k = tf.nn.top_k(tmp_batch, 3)\n",
    "\n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "    \n",
    "# Execute graph.\n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(initialize_model)\n",
    "print(sess.run([top_k]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finds n smallest elements - using additional division into list\n",
    "### (~ok, does not handle several elements with the same values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds n smallest elements\n",
    "\n",
    "import tensorflow as tf\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "batch = tf.convert_to_tensor([[4, 3, 2, 1], [10, 20, 1, 40]])\n",
    "print(\"batch.shape=\", batch.shape)\n",
    "\n",
    "# Number of smallest elements.\n",
    "n = 3\n",
    "\n",
    "# You can do it using built-in tf.nn.top_k function - find size-n top elements in each sample.\n",
    "k_number = batch.shape[1] - n\n",
    "print(\"k_number=\", k_number)\n",
    "\n",
    "smallest_ones_batch = list()\n",
    "for i_batch in range(batch.shape[0]):\n",
    "    top = tf.nn.top_k(batch[i_batch], k_number)\n",
    "    # To get boolean True/False values, you can first get the k-th value and then use tf.greater_equal:\n",
    "    kth = tf.reduce_min(top.values)\n",
    "    top2 = tf.greater_equal(batch[i_batch], kth)\n",
    "\n",
    "    # And finally - cast it to n smallest elements.\n",
    "    smallest = tf.cast(top2, tf.float32) * -1.0 + 1.0\n",
    "    \n",
    "    smallest_ones_batch.append(smallest)\n",
    "\n",
    "\n",
    "#w_prew = tf.convert_to_tensor()\n",
    "\n",
    "# Execute graph.e\n",
    "sess=tf.InteractiveSession()\n",
    "print(\"batch=\",sess.run(batch))\n",
    "\n",
    "print(\"smallest_ones=\",sess.run(smallest_ones_batch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finds n smallest elements - without the division (OK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds n smallest elements\n",
    "\n",
    "import tensorflow as tf\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "batch = tf.convert_to_tensor([[4, 3, 2, 1], [10, 20, 1, 40]])\n",
    "#batch = tf.convert_to_tensor([[ 0.1143328,   0.31078878,  0.1143328,   0.1143328,   0.23188008,  0.1143328 ],\n",
    "#    [ 0.1143328,   0.1143328,   0.31078878,  0.1143328,   0.1143328,   0.23188008],\n",
    "#    [ 0.1143328,   0.31078878,  0.1143328,   0.1143328,   0.23188008,  0.1143328 ]])\n",
    "#print(\"batch.shape=\", batch.shape)\n",
    "\n",
    "# Number of smallest elements.\n",
    "n = 3\n",
    "\n",
    "# You can do it using built-in tf.nn.top_k function - find size-n top elements ALONG THE LAST DIMENSION\n",
    "top = tf.nn.top_k(-batch, n)\n",
    "# To get boolean True/False values, you can first get the k-th value and then use tf.greater_equal:\n",
    "kth = tf.reduce_min(top.values, axis=1, keep_dims=True)\n",
    "# And finally - cast it to n smallest elements.\n",
    "smallest_ones_batch = tf.cast(tf.greater_equal(-batch, kth), tf.float32)\n",
    "\n",
    "# Execute graph.e\n",
    "sess=tf.InteractiveSession()\n",
    "print(\"batch=\",sess.run(batch))\n",
    "\n",
    "#print(\"top.values=\",sess.run(top.values))\n",
    "\n",
    "#print(\"kth=\",sess.run(kth))\n",
    "\n",
    "print(\"smallest_ones=\",sess.run(smallest_ones_batch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different approach - change memory \"orientation\" (BAD!)\n",
    "### Fully operational read-write-update heads, batch of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Size of the hidden state 64\n",
    "HIDDEN_SIZE = 3\n",
    "\n",
    "# Size of the MANN memory.\n",
    "MEMORY_SIZE = 5\n",
    "\n",
    "# A batch size of 100\n",
    "#BATCH_SIZE = 2\n",
    "\n",
    "# A single recurrent layer of number of units = sequences of length\n",
    "# e.g. 200 bytes\n",
    "SEQ_LENGTH = 10\n",
    "\n",
    "# \"Read decay\".\n",
    "GAMMA = 0.1\n",
    "\n",
    "# Number of smallest elements.\n",
    "N_SMALLEST =2\n",
    "\n",
    "#EPS = 1e-15\n",
    "\n",
    "# place holders \n",
    "batch = tf.placeholder(tf.float32, shape=None, name=\"Batch_h\")\n",
    "#memory = tf.placeholder(tf.float32, shape=[None], name=\"memory\")\n",
    "memory = tf.Variable(tf.zeros([HIDDEN_SIZE, MEMORY_SIZE]), trainable=False, name=\"Memory_M\")\n",
    "#alpha = tf.Variable(tf.truncated_normal(shape=[1]), name=\"Alpha\")\n",
    "alpha = tf.Variable(tf.truncated_normal(shape=[1]), name=\"Alpha\")\n",
    "\n",
    "# SET INITIAL MEMORY STATE.\n",
    "memory_set = memory.assign(tf.transpose([[1, 0, 1],\n",
    "                            [0.1, 0.2, 0.4],\n",
    "                            [ 0, 0.3, 0],\n",
    "                            [-0.3, 0.2, 0.3],\n",
    "                            [0, 1, 0]]))\n",
    "alpha_set = alpha.assign([0.1])\n",
    "\n",
    "# Placeholders for previous weights.\n",
    "prev_update_weights = tf.placeholder(tf.float32, shape=None, name=\"Prev_uw\")\n",
    "prev_read_weights = tf.placeholder(tf.float32, shape=None, name=\"Prev_rw\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Read_head\"):\n",
    "    # Normalize batches and memory.\n",
    "    norm_batch = tf.nn.l2_normalize(batch,1, name=\"NormalizedBatch_h\") \n",
    "    norm_memory = tf.nn.l2_normalize(memory,1, name=\"NormalizedMemory_h\")\n",
    "\n",
    "    # calculate similarity.\n",
    "    similarity = tf.tensordot(norm_batch, norm_memory, axes=1, name= \"Similarity_D\") \n",
    "    # Read weights based on similarity.\n",
    "    read_weights = tf.nn.softmax(similarity, name=\"Read_weights_rw\")\n",
    "    # Read \"vector\" (in fact batch).\n",
    "    r = tf.tensordot(read_weights, tf.transpose(memory), axes=1, name=\"Read_vector_r\")\n",
    "\n",
    "# TODO: add dependencies, that write will be done after read.\n",
    "with tf.name_scope(\"Write_head\"):\n",
    "    # A \"truncation scheme to update the least-used positions\".\n",
    "    # First, find (size-n) top elements (in each \"batch sample\"/head separatelly).\n",
    "    k_number = MEMORY_SIZE - N_SMALLEST\n",
    "    print(prev_update_weights.shape)\n",
    "    top = tf.nn.top_k(prev_update_weights, k_number)\n",
    "\n",
    "    # To get boolean True/False values, you can first get the k-th value and then use tf.greater_equal:\n",
    "    kth = tf.reduce_min(top.values)\n",
    "    top2 = tf.greater(prev_update_weights, kth)\n",
    "    # And finally - cast it to n smallest elements.\n",
    "    smallest_lru_weights = tf.cast(top2, tf.float32) * -1.0 + 1.0\n",
    "\n",
    "    write_weights = tf.add(tf.sigmoid(alpha) * prev_read_weights, (1.0 - tf.sigmoid(alpha)) * smallest_lru_weights, \n",
    "                           name=\"Write_weights_ww\")\n",
    "    \n",
    "with tf.name_scope(\"Memory_update\"):\n",
    "    calculated_mem_update = tf.tensordot(tf.transpose(batch), write_weights, axes=1)\n",
    "    memory_update_op = memory.assign(memory + calculated_mem_update)\n",
    "\n",
    "with tf.name_scope(\"Update_head\"): # This relies on prev. weights and will be used in fact in NEXT step.\n",
    "    update_weights = tf.add(GAMMA * prev_update_weights, read_weights + write_weights, name=\"Update_weights_uw\")\n",
    "\n",
    "    \n",
    "    \n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "    \n",
    "# Execute graph.\n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(initialize_model)\n",
    "sess.run([memory_set, alpha_set])\n",
    "print(\"Memory =\\n\",memory.eval())\n",
    "\n",
    "#tmp_batch = [[1, 0, 1],[-0.3, 0.3, -0.3]]\n",
    "tmp_batch = [[0.1, 0, 0],[0, -0.1, 0]]\n",
    "# Prev UW [batch size x memory size]\n",
    "prev_uw = [[0,0,0,0,0], [0,0,0,0,0]]\n",
    "prev_rw = [[0,0,0,0,0], [0,0,0,0,0]]\n",
    "\n",
    "print(\"\\nBatch=\\n\",batch.eval(feed_dict={batch:tmp_batch}))\n",
    "#print(\"Norm batch=\",norm_batch.eval(feed_dict={batch:tmp_batch}))\n",
    "\n",
    "for i in range(10):\n",
    "    sim, r_vect, prev_rw, prev_uw, prev_ww, mu_op = sess.run(\n",
    "        [similarity, r, read_weights, update_weights, write_weights, memory_update_op],\n",
    "                               feed_dict={\n",
    "                                   batch:tmp_batch,\n",
    "                                   prev_update_weights: prev_uw,\n",
    "                                   prev_read_weights: prev_rw\n",
    "                               })\n",
    "    #print(\"Norm memory =\\n\",norm_memory.eval())\n",
    "    print (\"\\n=====i=\", i)\n",
    "    print(\"Memory after update=\\n\", mu_op)\n",
    "    #print(\"\\nSimilarity=\\n\",sim)\n",
    "    print(\"\\nprev_rw=\\n\", prev_rw)\n",
    "    #print(\"\\nprev_uw=\\n\", prev_uw)\n",
    "    print(\"\\nprev_ww=\\n\", prev_ww)\n",
    "\n",
    "    print(\"\\n readed vector=\\n\", r_vect)\n",
    "\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different approach - change memory \"orientation\"\n",
    "### Read-write-update heads, step towards batch of \"sequences of vectors\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Size of the hidden state 64\n",
    "HIDDEN_SIZE = 3\n",
    "\n",
    "# Size of the MANN memory.\n",
    "MEMORY_SIZE = 5\n",
    "\n",
    "# A batch size of 100\n",
    "#BATCH_SIZE = 2\n",
    "\n",
    "# A single recurrent layer of number of units = sequences of length\n",
    "# e.g. 200 bytes\n",
    "SEQ_LENGTH = 10\n",
    "\n",
    "# \"Read decay\".\n",
    "GAMMA = 0.1\n",
    "\n",
    "# Number of smallest elements.\n",
    "N_SMALLEST =2\n",
    "\n",
    "#EPS = 1e-15\n",
    "\n",
    "# place holders \n",
    "batch = tf.placeholder(tf.float32, shape=None, name=\"Batch_h\")\n",
    "#memory = tf.placeholder(tf.float32, shape=[None], name=\"memory\")\n",
    "memory = tf.Variable(tf.zeros([HIDDEN_SIZE, MEMORY_SIZE]), trainable=False, name=\"Memory_M\")\n",
    "#alpha = tf.Variable(tf.truncated_normal(shape=[1]), name=\"Alpha\")\n",
    "alpha = tf.Variable(tf.truncated_normal(shape=[1]), name=\"Alpha\")\n",
    "\n",
    "# SET INITIAL MEMORY STATE.\n",
    "memory_set = memory.assign(tf.transpose([[1, 0, 1],\n",
    "                            [0.1, 0.2, 0.4],\n",
    "                            [ 0, 0.3, 0],\n",
    "                            [-0.3, 0.2, 0.3],\n",
    "                            [0, 1, 0]]))\n",
    "alpha_set = alpha.assign([0.1])\n",
    "\n",
    "# Placeholders for previous weights.\n",
    "prev_update_weights = tf.placeholder(tf.float32, shape=None, name=\"Prev_uw\")\n",
    "prev_read_weights = tf.placeholder(tf.float32, shape=None, name=\"Prev_rw\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Read_head\"):\n",
    "    # Normalize batches and memory.\n",
    "    norm_batch = tf.nn.l2_normalize(batch,1, name=\"NormalizedBatch_h\") \n",
    "    norm_memory = tf.nn.l2_normalize(memory,1, name=\"NormalizedMemory_h\")\n",
    "\n",
    "    # calculate similarity.\n",
    "    similarity = tf.tensordot(norm_batch, norm_memory, axes=1, name= \"Similarity_D\") \n",
    "    # Read weights based on similarity.\n",
    "    read_weights = tf.nn.softmax(similarity, name=\"Read_weights_rw\")\n",
    "    # Read \"vector\" (in fact batch).\n",
    "    r = tf.tensordot(read_weights, tf.transpose(memory), axes=1, name=\"Read_vector_r\")\n",
    "\n",
    "# TODO: add dependencies, that write will be done after read.\n",
    "with tf.name_scope(\"Write_head\"):\n",
    "    # A \"truncation scheme to update the least-used positions\".\n",
    "    # First, find (size-n) top elements (in each \"batch sample\"/head separatelly).\n",
    "    k_number = MEMORY_SIZE - N_SMALLEST\n",
    "    print(prev_update_weights.shape)\n",
    "    top = tf.nn.top_k(prev_update_weights, k_number)\n",
    "\n",
    "    # To get boolean True/False values, you can first get the k-th value and then use tf.greater_equal:\n",
    "    kth = tf.reduce_min(top.values)\n",
    "    top2 = tf.greater_equal(prev_update_weights, kth)\n",
    "    # And finally - cast it to n smallest elements.\n",
    "    smallest_lru_weights = tf.cast(top2, tf.float32) * -1.0 + 1.0\n",
    "\n",
    "    write_weights = tf.add(tf.sigmoid(alpha) * prev_read_weights, (1.0 - tf.sigmoid(alpha)) * smallest_lru_weights, \n",
    "                           name=\"Write_weights_ww\")\n",
    "    \n",
    "with tf.name_scope(\"Memory_update\"):\n",
    "    calculated_mem_update = tf.tensordot(tf.transpose(batch), write_weights, axes=1)\n",
    "    memory_update_op = memory.assign(memory + calculated_mem_update)\n",
    "\n",
    "with tf.name_scope(\"Update_head\"): # This relies on prev. weights and will be used in fact in NEXT step.\n",
    "    update_weights = tf.add(GAMMA * prev_update_weights, read_weights + write_weights, name=\"Update_weights_uw\")\n",
    "\n",
    "    \n",
    "    \n",
    "# Finally - initialize all variables.\n",
    "initialize_model = tf.global_variables_initializer()    \n",
    "    \n",
    "# Execute graph.\n",
    "sess=tf.InteractiveSession()\n",
    "# Initialize.\n",
    "sess.run(initialize_model)\n",
    "sess.run([memory_set, alpha_set])\n",
    "print(\"Memory =\\n\",memory.eval())\n",
    "\n",
    "#tmp_batch = [[1, 0, 1],[-0.3, 0.3, -0.3]]\n",
    "tmp_batch = [[0.1, 0, 0],[0, 0, 0]]\n",
    "# Prev UW [batch size x memory size]\n",
    "prev_uw = [[0,0,0,0,0], [0,0,0,0,0]]\n",
    "prev_rw = [[0,0,0,0,0], [0,0,0,0,0]]\n",
    "\n",
    "print(\"\\nBatch=\\n\",batch.eval(feed_dict={batch:tmp_batch}))\n",
    "#print(\"Norm batch=\",norm_batch.eval(feed_dict={batch:tmp_batch}))\n",
    "\n",
    "for i in range(10):\n",
    "    sim, r_vect, prev_rw, prev_uw, prev_ww, mu_op = sess.run(\n",
    "        [similarity, r, read_weights, update_weights, write_weights, memory_update_op],\n",
    "                               feed_dict={\n",
    "                                   batch:tmp_batch,\n",
    "                                   prev_update_weights: prev_uw,\n",
    "                                   prev_read_weights: prev_rw\n",
    "                               })\n",
    "    #print(\"Norm memory =\\n\",norm_memory.eval())\n",
    "    print (\"\\n=====i=\", i)\n",
    "    print(\"Memory after update=\\n\", mu_op)\n",
    "    #print(\"\\nSimilarity=\\n\",sim)\n",
    "    print(\"\\nprev_rw=\\n\", prev_rw)\n",
    "    #print(\"\\nprev_uw=\\n\", prev_uw)\n",
    "    print(\"\\nprev_ww=\\n\", prev_ww)\n",
    "\n",
    "    print(\"\\n readed vector=\\n\", r_vect)\n",
    "\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
