{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import tarfile\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import shutil \n",
    "import random\n",
    "\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "# Dirs - must be absolute paths!\n",
    "LOG_DIR = '/tmp/tf/ptb_char_lstm/'\n",
    "# Local dir where PTB files will be stored.\n",
    "PTB_DIR = '/home/tkornuta/data/ptb/'\n",
    "\n",
    "# Filenames.\n",
    "TRAIN = \"ptb.train.txt\"\n",
    "VALID = \"ptb.valid.txt\"\n",
    "TEST = \"ptb.test.txt\"\n",
    "\n",
    "# A single recurrent layer of 2000 units\n",
    "#NUM_UNITS = 100\n",
    "# Size of the hidden state 64\n",
    "HIDDEN_SIZE = 64\n",
    "\n",
    "# A batch size of 100\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "# Sequences of length 200 bytes\n",
    "SEQ_LENGTH = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check/maybe download PTB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified /home/tkornuta/data/ptb/simple-examples.tgz ( 34869662 )\n"
     ]
    }
   ],
   "source": [
    "def maybe_download_ptb(path, \n",
    "                       filename='simple-examples.tgz', \n",
    "                       url='http://www.fit.vutbr.cz/~imikolov/rnnlm/', \n",
    "                       expected_bytes =34869662):\n",
    "  # Eventually create the PTB dir.\n",
    "  if not tf.gfile.Exists(path):\n",
    "    tf.gfile.MakeDirs(path)\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  _filename = path+filename\n",
    "  if not os.path.exists(_filename):\n",
    "    print('Downloading %s...' % filename)\n",
    "    _filename, _ = urlretrieve(url+filename, _filename)\n",
    "  statinfo = os.stat(_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', (_filename), '(', statinfo.st_size, ')')\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + _filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download_ptb(PTB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract dataset-related files from the PTB archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_ptb(path, filename='simple-examples.tgz', files=[\"ptb.train.txt\", \"ptb.valid.txt\", \"ptb.test.txt\", \n",
    "                                       \"ptb.char.train.txt\", \"ptb.char.valid.txt\", \"ptb.char.test.txt\"]):\n",
    "    \"\"\"Extracts files from PTB archive.\"\"\"\n",
    "    # Extract\n",
    "    tar = tarfile.open(path+filename)\n",
    "    tar.extractall(path)\n",
    "    tar.close()\n",
    "    # Copy files\n",
    "    for file in files:\n",
    "        shutil.copyfile(PTB_DIR+\"simple-examples/data/\"+file, PTB_DIR+file)\n",
    "    # Delete directory\n",
    "    shutil.rmtree(PTB_DIR+\"simple-examples/\")        \n",
    "\n",
    "extract_ptb(PTB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train, valid and test texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5101618  aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memote\n",
      "399782  consumers may want to move their telephones a little closer to \n",
      "449945  no it was n't black monday \n",
      " but while the new york stock excha\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename, path):\n",
    "    with open(path+filename, 'r') as myfile:\n",
    "        data=myfile.read()# .replace('\\n', '')\n",
    "        return data\n",
    "\n",
    "train_text = read_data(TRAIN, PTB_DIR)\n",
    "train_size=len(train_text)\n",
    "print(train_size, train_text[:100])\n",
    "\n",
    "valid_text = read_data(VALID, PTB_DIR)\n",
    "valid_size=len(valid_text)\n",
    "print(valid_size, valid_text[:64])\n",
    "\n",
    "test_text = read_data(TEST, PTB_DIR)\n",
    "test_size=len(test_text)\n",
    "print(test_size, test_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size =  59\n",
      "65\n",
      "33 1 58 26 0 0\n",
      "a A\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 59 # [A-Z] + [a-z] + ' ' +few 'in between; + punctuation\n",
    "first_letter = ord(string.ascii_uppercase[0]) # ascii_uppercase before lowercase! \n",
    "print(\"vocabulary size = \", vocabulary_size)\n",
    "print(first_letter)\n",
    "\n",
    "def char2id(char):\n",
    "  \"\"\" Converts char to id (int) with one-hot encoding handling of unexpected characters\"\"\"\n",
    "  if char in string.ascii_letters:# or char in string.punctuation or char in string.digits:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    # print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  \"\"\" Converts single id (int) to character\"\"\"\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "#print(len(string.punctuation))\n",
    "#for i in string.ascii_letters:\n",
    "#    print (i, char2id(i))\n",
    "\n",
    "\n",
    "print(char2id('a'), char2id('A'), char2id('z'), char2id('Z'), char2id(' '), char2id('Ã¯'))\n",
    "print(id2char(char2id('a')), id2char(char2id('A')))\n",
    "#print(id2char(65), id2char(33), id2char(90), id2char(58), id2char(0))\n",
    "#bankno\n",
    "sample = np.zeros(shape=(1, vocabulary_size), dtype=np.float)\n",
    "sample[0, char2id(' ')] = 1.0\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper class for batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, seq_length, vocab_size):\n",
    "    \"\"\"\n",
    "    Initializes the batch generator object. Stores the variables and first \"letter batch\".\n",
    "    text is text to be processed\n",
    "    batch_size is size of batch (number of samples)\n",
    "    seq_length represents the length of sequence\n",
    "    vocab_size is number of words in vocabulary (assumes one-hot encoding)\n",
    "    \"\"\"\n",
    "    # Store input parameters.\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._seq_length = seq_length\n",
    "    self._vocab_size = vocab_size\n",
    "    # Divide text into segments depending on number of batches, each segment determines a cursor position for a batch.\n",
    "    segment = self._text_size // batch_size\n",
    "    # Set initial cursor position.\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    # Store first \"letter batch\".\n",
    "    self._last_letter_batch = self._next_letter_batch()\n",
    "  \n",
    "  def _next_letter_batch(self):\n",
    "    \"\"\"\n",
    "    Returns a batch containing of encoded single letters depending on the current batch \n",
    "    cursor positions in the data.\n",
    "    Returned \"letter batch\" is of size batch_size x vocab_size\n",
    "    \"\"\"\n",
    "    letter_batch = np.zeros(shape=(self._batch_size, self._vocab_size), dtype=np.float)\n",
    "    # Iterate through \"samples\"\n",
    "    for b in range(self._batch_size):\n",
    "      # Set 1 in position pointed out by one-hot char encoding.\n",
    "      letter_batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return letter_batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    # First add last letter from previous batch (the \"additional one\").\n",
    "    batches = [self._last_letter_batch]\n",
    "    for step in range(self._seq_length):\n",
    "      batches.append(self._next_letter_batch())\n",
    "    # Store last \"letter batch\" for next batch.\n",
    "    self._last_letter_batch = batches[-1]\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set =  aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memote\n",
      "['z calloway centrust c', ' league promises the ', 'ion including quotron', 'r proposal for a full', 'n china is very compl', ' december that was su', 'ogilvy group was  unk', 'lis a general electri', 've earlier this year ', 'f philip morris cos  ']\n"
     ]
    }
   ],
   "source": [
    "# Trick - override first 10 chars\n",
    "#list1 = list(train_text)\n",
    "#for i in range(2):\n",
    "#    list1[i] = 'z'\n",
    "#train_text = ''.join(list1)\n",
    "print(\"Test set =\", train_text[0:100])\n",
    "\n",
    "# Create two objects for training and validation batch generation.\n",
    "train_batches = BatchGenerator(train_text, BATCH_SIZE, SEQ_LENGTH, vocabulary_size)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1, vocabulary_size)\n",
    "\n",
    "batch = train_batches.next()\n",
    "batch = train_batches.next()\n",
    "#print(\"Batch = \", batch)\n",
    "print(batches2string(batch))\n",
    "#print(\"batch len = num of enrollings\",len(batch))\n",
    "#for i in range(num_unrollings):\n",
    "#    print(\"i = \", i, \"letter=\", batches2string(batch)[0][i][0], \"bits = \", batch[i][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for calculation of loss =  - log2 prob (BPC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"\n",
    "  Log-probability of the true labels in a predicted batch.\n",
    "  Assumes that predictions/labels are of shape [batch_size x 1] (i.e. a batch of 1-char sequences)\n",
    "  \"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  # Divide by the batch size (shape[0]).\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state, name):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        # Calculate gates activations.\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib, name=\"Input_gate\")\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb, name=\"Forget_gate\")\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob, name=\"Output_gate\")\n",
    "\n",
    "        update = tf.add(tf.matmul(i, cx), tf.matmul(o, cm) + cb, name=\"Update\")\n",
    "        state = tf.add(forget_gate * state, input_gate * tf.tanh(update), name=\"State_update\")\n",
    "        output = output_gate * tf.tanh(state)\n",
    "        return output, state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Definition of tensor graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "(10, 59)\n",
      "20\n",
      "(10, 64)\n",
      "(200, 64)\n"
     ]
    }
   ],
   "source": [
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 0. Placeholders for inputs.\n",
    "with tf.name_scope(\"Input_data\"):\n",
    "  # Define input data buffers.\n",
    "  train_data = list()\n",
    "  for _ in range(SEQ_LENGTH + 1):\n",
    "    # Collect placeholders for inputs/labels.\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[BATCH_SIZE,vocabulary_size], name=\"Input_data\"))\n",
    "  # Collection of training inputs.\n",
    "  train_inputs = train_data[:SEQ_LENGTH]\n",
    "  # Labels are pointing to the same placeholders!\n",
    "  # Labels are inputs shifted by one time step.\n",
    "  train_labels = train_data[1:]  \n",
    "  print (len(train_inputs))\n",
    "  print (train_inputs[0].shape)\n",
    "  # Concatenate targets into 2D tensor.\n",
    "  targets = tf.concat(train_labels, 0)\n",
    "\n",
    "# 1. Inference ops.\n",
    "with tf.name_scope(\"Inference\"):\n",
    "  # Define parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1), name=\"ix\")\n",
    "  im = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"im\")\n",
    "  ib = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"ib\")\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1), name=\"fx\")\n",
    "  fm = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"fm\")\n",
    "  fb = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"fb\")\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1), name=\"cx\")\n",
    "  cm = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"cm\")\n",
    "  cb = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"cb\")\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1), name=\"ox\")\n",
    "  om = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"om\")\n",
    "  ob = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"ob\")\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([BATCH_SIZE, HIDDEN_SIZE]), trainable=False, name=\"saved_output\")\n",
    "  saved_state = tf.Variable(tf.zeros([BATCH_SIZE, HIDDEN_SIZE]), trainable=False, name=\"saved_state\")\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, vocabulary_size], -0.1, 0.1), name=\"w\")\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]), name=\"b\")\n",
    "  \n",
    "  # Unrolled LSTM loop.\n",
    "  # Build outpus of size SEQ_LENGTH.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state, \"cell\")\n",
    "    outputs.append(output)\n",
    "  print (len(outputs))\n",
    "  print (outputs[0].shape)\n",
    "  print (tf.concat(outputs, 0).shape)\n",
    "    \n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "    # Fully connected layer on top => classification.\n",
    "    # In fact we will create lots of FC layers (one for each output layer), with shared weights.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "\n",
    "# 2. Loss ops.\n",
    "with tf.name_scope(\"Loss\"):\n",
    "    # Loss function(s) - one for every output generated by every lstm cell.\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=targets, logits=logits))\n",
    "    # Add loss summary.\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "# 3. Training ops.  \n",
    "with tf.name_scope(\"Training\"):\n",
    "  # Optimizer-related variables.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "with tf.name_scope(\"Evaluation\") as scope:\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Subgraph responsible for generation of sample texts, char by char.\n",
    "with tf.name_scope(\"Sample_generation\") as scope:\n",
    "  # Create graphs for sampling and validation evaluation: batch 1, \"no unrolling\".\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size], name=\"Input_data\")\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"Output_data\")\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"Hidden_state\")\n",
    "\n",
    "  # Node responsible for resetting the state and output.\n",
    "  reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, HIDDEN_SIZE])),\n",
    "      saved_sample_state.assign(tf.zeros([1, HIDDEN_SIZE])))\n",
    "  # Single LSTM cell.\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state, \"cell\")\n",
    "  # Output depends on the hidden state.\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b, name=\"logits\"), name=\"outputs\")\n",
    "\n",
    "# Subgraph responsible for validation, char by char.\n",
    "#with tf.name_scope(\"Validation\") as scope:\n",
    "#  # Create graphs for sampling and validation evaluation: batch 1, \"no unrolling\".\n",
    "#  valid_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size], name=\"Input_data\")\n",
    "#  valid_target = tf.placeholder(tf.float32, shape=[1, vocabulary_size], name=\"Target_data\")\n",
    "#    \n",
    "#  prev_valid_output = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"Output_data\")\n",
    "#  prev_valid_state = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"Hidden_state\")\n",
    "#\n",
    "#  # Node responsible for resetting the state and output.\n",
    "#  reset_valid_state = tf.group(\n",
    "#      prev_valid_output.assign(tf.zeros([1, HIDDEN_SIZE])),\n",
    "#      prev_valid_state.assign(tf.zeros([1, HIDDEN_SIZE])))\n",
    "#    \n",
    "#  # Single LSTM cell.\n",
    "#  valid_output, valid_state = lstm_cell(valid_input, prev_valid_output, prev_valid_state, \"cell\")\n",
    "#  # Output depends on the hidden state.\n",
    "#  with tf.control_dependencies([prev_valid_output.assign(valid_output), prev_valid_state.assign(valid_state)]):\n",
    "#    valid_logits = tf.nn.xw_plus_b(valid_output, w, b, name=\"logits\")\n",
    "#  # Calculate validation loss.\n",
    "#  valid_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=valid_target, logits=valid_logits))\n",
    "#  # Add loss summary.\n",
    "#  valid_loss_summary = tf.summary.scalar(\"loss\", valid_loss)\n",
    "    \n",
    "    \n",
    "# Merge all summaries.\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "# 4. Init global variable.\n",
    "init = tf.global_variables_initializer()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for language generation (letter sampling etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Eventually clear the log dir.\n",
    "if tf.gfile.Exists(LOG_DIR):\n",
    "  tf.gfile.DeleteRecursively(LOG_DIR)\n",
    "# Create (new) log dir.\n",
    "tf.gfile.MakeDirs(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of iterations=  25508\n",
      "Initialized\n",
      "Average loss at step 0: 4.072471 learning rate: 10.000000\n",
      "Train set minibatch BPC: 4.07\n",
      "================================================================================\n",
      "zLOsqOZhoAd W oC sfixe_M  _ pz\\MfB Z o g c p[ l Gl^Eo[xQQwBVz_`ks]GYnPl` voEtY  \n",
      "EtUR  ewthwM Seji[v n tnSWR  AQ\\s   QE _   N thtqACiP_ XeCjvLwMFub Cv HBP\\ j ZiD\n",
      "R ZeV cHAcSnJXFxwFsJkyMgzXlWeR  gn WSuL WTNiuPRdLJMwAs\\N HsiQimqPK\\ o d^wiJ an j\n",
      "igj`yOHR  iUlqBuR  oei zZm rV y\\ch]u\\Awyt  lMecYSe non V]ZD  gFFyb K  \\W iuhDwae\n",
      "fsiRdM x_PAvofBj^WoPGy`OKAqalAuV nnR[ v jq wRM dbS lKyaQiC T j  Y [gie_ w  UynA \n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 100: 2.844692 learning rate: 10.000000\n",
      "Train set minibatch BPC: 2.50\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 200: 2.279402 learning rate: 10.000000\n",
      "Train set minibatch BPC: 2.23\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 300: 2.148984 learning rate: 10.000000\n",
      "Train set minibatch BPC: 2.18\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 400: 2.057230 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.90\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 500: 1.988623 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.91\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 600: 1.919877 learning rate: 10.000000\n",
      "Train set minibatch BPC: 2.06\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 700: 1.888196 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.81\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 800: 1.837057 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.69\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 900: 1.818658 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.89\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 1000: 1.781626 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.85\n",
      "================================================================================\n",
      "flealinged countle  unk  first sUederted concemsud wheat raded and   unkeeding t\n",
      "ded   this to beironal N N the an incher mast to malled contwanate pllawe in iti\n",
      "gold a geation a londuins  s wear hemedial respantares the and cbith at sales wo\n",
      "Coneing by dealeeftone and the   the the dac ditings amouj polethts beareftlinet\n",
      "Rices a  unk  trajen  unk  mosty and   unk  the dourt co sunsguces affact at yea\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 1100: 1.747380 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.63\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 1200: 1.719899 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.52\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 1300: 1.687242 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.67\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 1400: 1.689865 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.83\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 1500: 1.658134 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.58\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 1600: 1.650711 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.72\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 1700: 1.602318 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.43\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 1800: 1.625969 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.61\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 1900: 1.641552 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.55\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 2000: 1.690910 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.66\n",
      "================================================================================\n",
      "Ued wardgnawel from the    unk  an rase the sge wno  unk  it wrexhicugan in it w\n",
      "Tarce be agn inden  s his bechas mattet and the stock have fire apairl who the b\n",
      "has   hrod     unk  supp the re leaser talk ersice and in ruret dons in based fo\n",
      "Yand store tamen   lew the sexere   morga nasy  n yer or in ruid    unk  in rade\n",
      "Jank broung   boarder yas race of the restraccsd were teke hutters   N year in a\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 2100: 1.645759 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.61\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 2200: 1.581205 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.65\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 2300: 1.581983 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.40\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 2400: 1.565702 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.91\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 2500: 1.570274 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.47\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 2600: 1.571846 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.64\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 2700: 1.544682 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.62\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 2800: 1.569409 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.53\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 2900: 1.551461 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.66\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 3000: 1.601681 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.53\n",
      "================================================================================\n",
      "binid des also oberla the sect raje company regraused and N and do new the quirt\n",
      "cents is  unk  of new yosed the buid pecis agrouny includert now syex famt the b\n",
      "Eally mr    the maisded the these two  unk  survieng of senered sency by   N ads\n",
      "ceare are secer rose gas  unk  the secuasl its argush apaire repaster  s the een\n",
      "Hed this  newdes and cqilas rose negt  unk  geor soperer n g and lobn deed the a\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 3100: 1.587135 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.55\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 3200: 1.556957 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.60\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 3300: 1.523915 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.84\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 3400: 1.515635 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.56\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 3500: 1.593019 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.65\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 3600: 1.539049 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.59\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 3700: 1.551132 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.41\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 3800: 1.530609 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.54\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 3900: 1.515273 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.46\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 4000: 1.520893 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.49\n",
      "================================================================================\n",
      "and  unk  in own move fridive scbedies about regionn by to company judge purties\n",
      "[ed scamper stock mady said the co pufce as companying commercive of this  s   s\n",
      "Cuburage countries domp after m  sogy a s  how imong bod is N pone his also rece\n",
      "Ges dividen to bon een  unk  brodevy oppolicin ta be of her for  unk  is  unk  a\n",
      "Qenting a genes   so incenties hove the growing that or by subud as move supphy \n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 4100: 1.580942 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.58\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 4200: 1.558781 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.46\n",
      "Validation set BPC (1-char): 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4300: 1.544221 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.43\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 4400: 1.501718 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.53\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 4500: 1.532130 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.60\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 4600: 1.533934 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.40\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 4700: 1.519872 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.63\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 4800: 1.511225 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.49\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 4900: 1.522267 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.40\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 5000: 1.484552 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.37\n",
      "================================================================================\n",
      "Ted and advarched more gomengher lower  unk  those brieved on adminsame a gispam\n",
      "withs ericialed are ut furth work   N N at reaid  s the   N a were which to leco\n",
      "Wia that its chain even roge with N N to the over a hind   for had Eean imong N \n",
      "Zorted  caser delie    unk    in analyoh to comadentauctor wash its roborted leg\n",
      "Eemer princhabuled investor infections increased manif tradels   the more sether\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 5100: 1.512620 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.67\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 5200: 1.497741 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.70\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 5300: 1.477992 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.53\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 5400: 1.465312 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.67\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 5500: 1.466978 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.37\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 5600: 1.503212 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.39\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 5700: 1.472826 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.36\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 5800: 1.479401 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.57\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 5900: 1.514592 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.68\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 6000: 1.531007 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.59\n",
      "================================================================================\n",
      "ble  unk  with as and persomatile factors turnist  unk  this ledm nebally forson\n",
      "bif by the u s  million the mr    rone market doling sess investorias in less as\n",
      "usly the earlier mafagitive like san its lead that like in out smed of tran up y\n",
      "cibed to bose in much of additress is casany   the davy finaross becaup of deval\n",
      "on derick forceved they chemacatal under   market will it may exchange mareinist\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 6100: 1.471201 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.37\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 6200: 1.462900 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.45\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 6300: 1.446917 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.29\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 6400: 1.465867 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.40\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 6500: 1.450332 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.35\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 6600: 1.472817 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.61\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 6700: 1.436601 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.53\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 6800: 1.441274 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.53\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 6900: 1.419437 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.51\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 7000: 1.404015 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.42\n",
      "================================================================================\n",
      "d concentisuation as thes invested parution of by pas the noringlan air endele c\n",
      "`urunting partiboly or the ordy from the says why has  unk    have promius is co\n",
      "Tans about the boker yerrymanded and them offered trading tooth concome estaten \n",
      "Sidems services in naw a cahutersmeno an unk  ritors known by pigces for  unk  l\n",
      "Xs companies are in the core  unk  to N N to  unk  with week talks   N N madagan\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 7100: 1.442780 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.74\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 7200: 1.448077 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.49\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 7300: 1.449166 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.42\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 7400: 1.436445 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.36\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 7500: 1.399201 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.33\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 7600: 1.444993 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.38\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 7700: 1.423107 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.38\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 7800: 1.400748 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.57\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 7900: 1.399914 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.25\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 8000: 1.420139 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.45\n",
      "================================================================================\n",
      "cond exchandon bank  n time said he securilation of  s ride manuge came fourers \n",
      "K new for a N not sale michrow  s companies   N million if or yen earns investie\n",
      "Ged composable   money   concernss of swort cirement acquirea stanns  s  unk   u\n",
      "Ying viowed in excertaurs souther low official under norac fead  unk  minishings\n",
      "Hing assided dics to permitf mergation   on i  s kice tally facther the expectit\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 8100: 1.422312 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.47\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 8200: 1.399630 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.55\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 8300: 1.397913 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.57\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 8400: 1.435705 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.53\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 8500: 1.406635 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.42\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 8600: 1.442217 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.41\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 8700: 1.445383 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.47\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 8800: 1.396101 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.47\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 8900: 1.411105 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.58\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 9000: 1.402897 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.40\n",
      "================================================================================\n",
      "Zent of a distrimut ballets to securities and ratuip manage decently at the cass\n",
      "s to failly sepasing havizel invosteds mr  nears and  unk  agreedy of  unk  leas\n",
      "y if the televing a anstlents is  unk  for problem had lege N N N   mr  seriment\n",
      "\\less   souther beal by m  bunific   was n t every because in ending recent fran\n",
      "Hed make opposed to beanc shares by the inc steid to N N infelt planning ran cur\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 9100: 1.401855 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.56\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 9200: 1.448810 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.33\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 9300: 1.424940 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.38\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 9400: 1.440703 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.47\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 9500: 1.412038 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.50\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 9600: 1.447153 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.53\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 9700: 1.433803 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.60\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 9800: 1.427441 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.41\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 9900: 1.401412 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.38\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 10000: 1.401391 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.34\n",
      "================================================================================\n",
      "logion  unk  this services of N tochaince dotie also also whationuestaries  unk \n",
      "ment   are is grant ago as two earnirg market   in the counting and  unk  divid \n",
      "_ers and the   unk  mr  their habs consmand a  unk  in N N areand earned agrizer\n",
      "Es billion leaders cors    unk    and the new  unk  valped to damation hoveratio\n",
      "e tocims including  s  unk    quarters institiol post the suggre says of there l\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 10100: 1.449023 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.54\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 10200: 1.421143 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.62\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 10300: 1.416254 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.41\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 10400: 1.371566 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.25\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 10500: 1.378344 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.64\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 10600: 1.411814 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.51\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 10700: 1.434911 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.48\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 10800: 1.481561 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.35\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 10900: 1.419660 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.68\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 11000: 1.439256 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.43\n",
      "================================================================================\n",
      "Uased   this investments   in ormand that freasts that gread ob  unk  umpormatiz\n",
      "nistrant indo neforis in neest plan more the toning   the streets hrie to be agr\n",
      "ment if king fect rosewhat childusia lawing ha and in ad of the makina years to \n",
      "read money so  unk    but up and apberimay   more that fice have   N millions be\n",
      "Puned nov hhowe more for the price i mr   s in diam like i thirst traders towrbe\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 11100: 1.419347 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.58\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 11200: 1.401050 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.14\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 11300: 1.371907 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.34\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 11400: 1.422682 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.49\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 11500: 1.394405 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.28\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 11600: 1.404056 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.51\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 11700: 1.411575 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.54\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 11800: 1.400182 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.54\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 11900: 1.398784 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.40\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 12000: 1.413327 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.58\n",
      "================================================================================\n",
      "worm who  unk  for we the mobed poviouses of turs vilit business ofthing the num\n",
      "[oker of   N million and caking indo a is entaring any is just led tape has comp\n",
      "^ect cash   day group do ford   but u i   stocks for decale showned are  unk  to\n",
      "Lest we a  unk  of it  s promer and a foreign  unk  her kunk but ago offlegicula\n",
      "Xres with be today   trading the comments so a wantam in N million yints of said\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 12100: 1.435400 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.51\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 12200: 1.448743 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.47\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 12300: 1.412274 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.21\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 12400: 1.382438 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.42\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 12500: 1.363014 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.45\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 12600: 1.434753 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.35\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 12700: 1.419009 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.34\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 12800: 1.433890 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.43\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 12900: 1.456728 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.45\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 13000: 1.473403 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.33\n",
      "================================================================================\n",
      "\\er   the just sturt the side  unk    n respodts to yestery   of  unk  pailted  \n",
      "Jon that how the though cusesten posissive franted francing and common on glans \n",
      "wall trater bror whede tolinating through beanion   this lystle equipves mergar \n",
      "sime the  unk  to stringly knownday burger that   N million and have active wate\n",
      "Oowramciation that futures prices   adsont   fust for a composito oon collaters \n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 13100: 1.446614 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.33\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 13200: 1.396505 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.48\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 13300: 1.420017 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.37\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 13400: 1.427991 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.61\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 13500: 1.356927 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.44\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 13600: 1.335904 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.30\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 13700: 1.385025 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.52\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 13800: 1.358238 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.13\n",
      "Validation set BPC (1-char): 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 13900: 1.337479 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.23\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 14000: 1.371055 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.28\n",
      "================================================================================\n",
      "Mel richment are companies have declined as its agencated commerey sately into s\n",
      "Qing an accirunels in the and N   the times caps received other demandors in wei\n",
      "work and has term exchange N but equited it   infornal lly servern   that sport \n",
      "[er financis due sign probalt  s lears as under but one manage may offers force \n",
      "ging   the quarter  s department   are his inOecialed the deenation agreeing for\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 14100: 1.380042 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.24\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 14200: 1.404371 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.48\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 14300: 1.426401 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.43\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 14400: 1.418944 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.45\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 14500: 1.474105 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.38\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 14600: 1.459593 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.54\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 14700: 1.467119 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.65\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 14800: 1.438032 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.30\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 14900: 1.419302 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.36\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 15000: 1.381107 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.55\n",
      "================================================================================\n",
      "`asting incend   his abortions richahue of the declinently mebicoling been buyin\n",
      "taaktion   service excect   up for constrometing and just the  s offered to be w\n",
      " rate focked develubing a junk corp  in the said they the mich and a diet a mark\n",
      "Ging banking because of the you calied note in hival  unk  is tryageirnk clitura\n",
      "Ler that offreaces sen yort and his crived which resurse   by new roletily that \n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 15100: 1.436274 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.45\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 15200: 1.446853 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.18\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 15300: 1.419018 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.44\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 15400: 1.422711 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.52\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 15500: 1.430451 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.53\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 15600: 1.428273 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.44\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 15700: 1.384147 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.48\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 15800: 1.385396 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.24\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 15900: 1.373039 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.42\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 16000: 1.400189 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.31\n",
      "================================================================================\n",
      "Se the needers in N year in the many they crusinesse help inc  instrusitors actr\n",
      "Iolds claim have  unk  credit of  unk  funds chands investment a brhek worker hi\n",
      "Her for seourto   loakker a N N N educationces and a defiding east proor advose \n",
      "Aert gaination barges to but been estimet   at  unk  figures investedral roberti\n",
      "xa is  unk    predicts from more and magn also in with jorre the seefations boan\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 16100: 1.400476 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.45\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 16200: 1.397671 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.36\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 16300: 1.459647 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.69\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 16400: 1.446220 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.47\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 16500: 1.401299 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.28\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 16600: 1.375270 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.28\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 16700: 1.394350 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.50\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 16800: 1.379111 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.33\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 16900: 1.414068 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.24\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 17000: 1.399294 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.48\n",
      "================================================================================\n",
      "Xal  linen introciadding   n me worth term exchone is the ecroration continue wi\n",
      "jos rating trusborge share we rulig weal atquarber exchange who in coluded says \n",
      "N of its caphed   you area case banks   sold   broad   unjerdeal doper court yea\n",
      "Ier nich fall  one guoveria some  unk  a preelation dueving alset the afrowed sp\n",
      "Ecanes on  unk  plant salemers where it  unk   n t mrs  out of N N american   th\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 17100: 1.405339 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.27\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 17200: 1.429029 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.41\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 17300: 1.421461 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.35\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 17400: 1.436356 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.33\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 17500: 1.419076 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.52\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 17600: 1.438727 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.43\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 17700: 1.439068 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.34\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 17800: 1.418037 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.53\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 17900: 1.410841 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.49\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 18000: 1.438104 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.37\n",
      "================================================================================\n",
      "y money  unk  eacher specailation of last accounce had the prospect at this be n\n",
      "vide convice imredents shares prosonal lonnuals dolleting commons prowort  unk  \n",
      "lan when been tow pricerss piemer set at the femer aftman issuited investors ano\n",
      "bs   N N japanesial westerday for and potide with at its systems was N usputer i\n",
      "Aent concenseicy ord  s choshed  s about with  unk    inc strongest was chargeol\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 18100: 1.421708 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.53\n",
      "Validation set BPC (1-char): 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 18200: 1.384853 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.58\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 18300: 1.421635 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.61\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 18400: 1.452479 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.29\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 18500: 1.397409 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.38\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 18600: 1.458770 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.45\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 18700: 1.404414 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.44\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 18800: 1.393742 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.66\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 18900: 1.400680 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.17\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 19000: 1.396462 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.39\n",
      "================================================================================\n",
      "uncer have   many fuic p   formed francing said  unk  director  beet official th\n",
      "[roper the makes of apemicgaral implissed N N   heading have of the own  unk    \n",
      "Llities in the program to say unity conteres thoound and bebages in  unk  repres\n",
      "Tomer   in the company  re filance of the industrations of buyinesed bill be res\n",
      "mbor of the improve and deleavis of and caming or curact laad say   mr   unk  on\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 19100: 1.421035 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.45\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 19200: 1.388401 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.49\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 19300: 1.450917 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.26\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 19400: 1.400570 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.33\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 19500: 1.432987 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.22\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 19600: 1.404825 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.53\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 19700: 1.446163 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.38\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 19800: 1.403564 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.38\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 19900: 1.386017 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.41\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 20000: 1.380021 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.42\n",
      "================================================================================\n",
      "ved state ridsw  ambitional we cast boing   said in file to N analwenn earningso\n",
      "ving week in thing strugally and grept husicos N N on an pence  unk  told to pro\n",
      "Ued that readed city be unit industred suppliole lifetitial revenue for ormaic m\n",
      "qualiova that the prediction though company  unk    N in resuranch prices in del\n",
      "a market said its bond advisposition to a shares and recorday   percorh continue\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 20100: 1.362242 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.46\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 20200: 1.374338 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.38\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 20300: 1.361230 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.35\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 20400: 1.347274 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.39\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 20500: 1.401667 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.31\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 20600: 1.435368 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.27\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 20700: 1.408256 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.43\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 20800: 1.390303 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.33\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 20900: 1.422824 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.42\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 21000: 1.373801 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.47\n",
      "================================================================================\n",
      "[ersmers   the gainuchear comporting a paffit   conserver new main other volume \n",
      "Yams of many for gote taken paje mr   unk  in they the chies volull it he passes\n",
      "Ser   mr  paringshonly the market highiaf reserve it caw that activisions of cen\n",
      "Yer treiters chief evering by inc scapacages sudx theirs yith investors asban sa\n",
      "ressing   N billion is highoug but yesterday of fifcical to   unit large virex f\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 21100: 1.409076 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.56\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 21200: 1.394457 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.45\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 21300: 1.376177 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.38\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 21400: 1.398799 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.45\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 21500: 1.385568 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.25\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 21600: 1.387009 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.50\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 21700: 1.461155 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.51\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 21800: 1.424798 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.33\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 21900: 1.413342 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.44\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 22000: 1.404109 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.17\n",
      "================================================================================\n",
      "U month said N million applepi   thought a firm   knom the N to N to sell group \n",
      "ganing ca  save and longory up although pensed said   we differally   in N cust \n",
      "Med had the depen when equitiles virection as N of   N million analysts   some i\n",
      "oted freact when their must charking thancers of the u s  shall imbos   the capi\n",
      "xewsolity down courtonnon to  s no tracket   the made as the japanese to bonde d\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 22100: 1.420335 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.43\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 22200: 1.421275 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.30\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 22300: 1.387998 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.38\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 22400: 1.404738 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.25\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 22500: 1.385709 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.34\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 22600: 1.405791 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.26\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 22700: 1.388600 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.59\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 22800: 1.414460 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.32\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 22900: 1.407886 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.45\n",
      "Validation set BPC (1-char): 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 23000: 1.432812 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.51\n",
      "================================================================================\n",
      "bold is yorg to N of for fustrial novers  s wappend insuin  unk  sme nexemly a  \n",
      "Wes say  s moild refair even said thangs on adells ano unitich retulcations   wa\n",
      "om other a chairms at as   ebing otpordun   everman hit new yield for the ugh ad\n",
      "Qs pression  unk  to quarter late it in all congitbell the business and the more\n",
      "Qer to an adtiged chairms from yend spork but u in six from action are setaly am\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 23100: 1.378498 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.65\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 23200: 1.351503 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.17\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 23300: 1.438957 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.39\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 23400: 1.376633 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.31\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 23500: 1.393383 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.34\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 23600: 1.385563 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.66\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 23700: 1.426720 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.38\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 23800: 1.395207 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.33\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 23900: 1.397105 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.32\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 24000: 1.369637 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.34\n",
      "================================================================================\n",
      "Cs  s his burter in their court see  unk  far thaobotial ply which rupe nebt a N\n",
      "bic expected to  unk  switsitionss cinted the sap  unk   unk  a masotment by kee\n",
      "_er   the way program under  s argual for the nooker caid high of post to tech o\n",
      "Ded new vase some and compenting can national posturity litting he disecteres an\n",
      "pance   N   n t overal deportives and acquired this today stored volume force re\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 24100: 1.416858 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.42\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 24200: 1.418669 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.31\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 24300: 1.397319 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.35\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 24400: 1.365150 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.49\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 24500: 1.377076 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.22\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 24600: 1.374654 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.20\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 24700: 1.380114 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.32\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 24800: 1.381953 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.55\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 24900: 1.405076 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.50\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 25000: 1.367287 learning rate: 0.000100\n",
      "Train set minibatch BPC: 1.33\n",
      "================================================================================\n",
      "nton he provade to wheeners income agaed as the lost   state back has lond to ke\n",
      "Por and imleating become begense earthaol appear from the country at co    so up\n",
      "Hest hald the  unk  completed will cossee because  unk  are n t able fordings de\n",
      "jom houses in cent about be brumbest wall trading mortgakess after support proce\n",
      "dline because    unk  said month  unk  he i stating by the earlierss s stock are\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 25100: 1.398821 learning rate: 0.000100\n",
      "Train set minibatch BPC: 1.54\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 25200: 1.392513 learning rate: 0.000100\n",
      "Train set minibatch BPC: 1.80\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 25300: 1.405371 learning rate: 0.000100\n",
      "Train set minibatch BPC: 1.52\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 25400: 1.395490 learning rate: 0.000100\n",
      "Train set minibatch BPC: 1.33\n",
      "Validation set BPC (1-char): 0.00\n",
      "Average loss at step 25500: 1.444442 learning rate: 0.000100\n",
      "Train set minibatch BPC: 1.63\n",
      "Validation set BPC (1-char): 0.00\n"
     ]
    }
   ],
   "source": [
    "num_steps =  train_size // (BATCH_SIZE*SEQ_LENGTH) #70001\n",
    "print(\"Total number of iterations= \", num_steps)\n",
    "summary_frequency = 100\n",
    "\n",
    "# Create session.\n",
    "sess = tf.InteractiveSession()\n",
    "# Create summary writers, point them to LOG_DIR.\n",
    "train_writer = tf.summary.FileWriter(LOG_DIR + '/train', sess.graph)\n",
    "valid_writer = tf.summary.FileWriter(LOG_DIR + '/valid')\n",
    "#test_writer = tf.summary.FileWriter(LOG_DIR + '/test')\n",
    "\n",
    "# Initialize global variables.\n",
    "tf.global_variables_initializer().run()\n",
    "print('Initialized')\n",
    "\n",
    "mean_loss = 0\n",
    "for step in range(num_steps):\n",
    "    # Get next batch and create a dictionary.\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(SEQ_LENGTH + 1):\n",
    "        feed_dict[train_data[i]] = batches[i]\n",
    "    # Run graph.\n",
    "    summary, _, l, predictions, lr = sess.run([merged_summaries, optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    train_writer.add_summary(summary, step)\n",
    "    train_writer.flush()\n",
    "    # Add loss to mean.\n",
    "    mean_loss += l\n",
    "    # Every (100) steps collect statistics.\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Train set minibatch BPC: %.2f' % logprob(predictions, labels))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          # Reset LSTM hidden state.\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set BPC.\n",
    "      reset_sample_state.run()\n",
    "      mean_valid_logprob = 0\n",
    "      # Sum for a single batch of size 1 - i.e. predict depending only on a single input character.\n",
    "      for _ in range(1000): #valid_size):\n",
    "        b = valid_batches.next()\n",
    "        # Reset LSTM hidden state.\n",
    "        #reset_valid_state.run()\n",
    "        #summary, _, l, predictions, lr = sess.run([merged_summaries, optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        \n",
    "        #predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        #mean_valid_logprob += logprob(predictions, b[1])\n",
    "      print('Validation set BPC (1-char): %.2f' % float(mean_valid_logprob / (1000))) #valid_size)))\n",
    "    # End of statistics collection\n",
    "\n",
    "# Close writers and session.\n",
    "train_writer.close()\n",
    "valid_writer.close()\n",
    "#test_writer.flush()\n",
    "#test_writer.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
