{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import tarfile\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import shutil \n",
    "import random\n",
    "\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "# Dirs - must be absolute paths!\n",
    "LOG_DIR = '/tmp/tf/ptb_char_lstm/'\n",
    "# Local dir where PTB files will be stored.\n",
    "PTB_DIR = '/home/tkornuta/data/ptb/'\n",
    "\n",
    "# Filenames.\n",
    "TRAIN = \"ptb.train.txt\"\n",
    "VALID = \"ptb.valid.txt\"\n",
    "TEST = \"ptb.test.txt\"\n",
    "\n",
    "# A single recurrent layer of 2000 units\n",
    "#NUM_UNITS = 100\n",
    "# Size of the hidden state 64\n",
    "HIDDEN_SIZE = 64\n",
    "\n",
    "# A batch size of 100\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "# Sequences of length 200 bytes\n",
    "SEQ_LENGTH = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check/maybe download PTB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified /home/tkornuta/data/ptb/simple-examples.tgz ( 34869662 )\n"
     ]
    }
   ],
   "source": [
    "def maybe_download_ptb(path, \n",
    "                       filename='simple-examples.tgz', \n",
    "                       url='http://www.fit.vutbr.cz/~imikolov/rnnlm/', \n",
    "                       expected_bytes =34869662):\n",
    "  # Eventually create the PTB dir.\n",
    "  if not tf.gfile.Exists(path):\n",
    "    tf.gfile.MakeDirs(path)\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  _filename = path+filename\n",
    "  if not os.path.exists(_filename):\n",
    "    print('Downloading %s...' % filename)\n",
    "    _filename, _ = urlretrieve(url+filename, _filename)\n",
    "  statinfo = os.stat(_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', (_filename), '(', statinfo.st_size, ')')\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + _filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download_ptb(PTB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract dataset-related files from the PTB archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ptb(path, filename='simple-examples.tgz', files=[\"ptb.train.txt\", \"ptb.valid.txt\", \"ptb.test.txt\", \n",
    "                                       \"ptb.char.train.txt\", \"ptb.char.valid.txt\", \"ptb.char.test.txt\"]):\n",
    "    \"\"\"Extracts files from PTB archive.\"\"\"\n",
    "    # Extract\n",
    "    tar = tarfile.open(path+filename)\n",
    "    tar.extractall(path)\n",
    "    tar.close()\n",
    "    # Copy files\n",
    "    for file in files:\n",
    "        shutil.copyfile(PTB_DIR+\"simple-examples/data/\"+file, PTB_DIR+file)\n",
    "    # Delete directory\n",
    "    shutil.rmtree(PTB_DIR+\"simple-examples/\")        \n",
    "\n",
    "extract_ptb(PTB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train, valid and test texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5101618  aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memote\n",
      "399782  consumers may want to move their telephones a little closer to \n",
      "449945  no it was n't black monday \n",
      " but while the new york stock excha\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename, path):\n",
    "    with open(path+filename, 'r') as myfile:\n",
    "        data=myfile.read()# .replace('\\n', '')\n",
    "        return data\n",
    "\n",
    "train_text = read_data(TRAIN, PTB_DIR)\n",
    "train_size=len(train_text)\n",
    "print(train_size, train_text[:100])\n",
    "\n",
    "valid_text = read_data(VALID, PTB_DIR)\n",
    "valid_size=len(valid_text)\n",
    "print(valid_size, valid_text[:64])\n",
    "\n",
    "test_text = read_data(TEST, PTB_DIR)\n",
    "test_size=len(test_text)\n",
    "print(test_size, test_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size =  59\n",
      "65\n",
      "33 1 58 26 0 0\n",
      "a A\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 59 # [A-Z] + [a-z] + ' ' +few 'in between; + punctuation\n",
    "first_letter = ord(string.ascii_uppercase[0]) # ascii_uppercase before lowercase! \n",
    "print(\"vocabulary size = \", vocabulary_size)\n",
    "print(first_letter)\n",
    "\n",
    "def char2id(char):\n",
    "  \"\"\" Converts char to id (int) with one-hot encoding handling of unexpected characters\"\"\"\n",
    "  if char in string.ascii_letters:# or char in string.punctuation or char in string.digits:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    # print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  \"\"\" Converts single id (int) to character\"\"\"\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "#print(len(string.punctuation))\n",
    "#for i in string.ascii_letters:\n",
    "#    print (i, char2id(i))\n",
    "\n",
    "\n",
    "print(char2id('a'), char2id('A'), char2id('z'), char2id('Z'), char2id(' '), char2id('Ã¯'))\n",
    "print(id2char(char2id('a')), id2char(char2id('A')))\n",
    "#print(id2char(65), id2char(33), id2char(90), id2char(58), id2char(0))\n",
    "#bankno\n",
    "sample = np.zeros(shape=(1, vocabulary_size), dtype=np.float)\n",
    "sample[0, char2id(' ')] = 1.0\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper class for batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, seq_length, vocab_size):\n",
    "    \"\"\"\n",
    "    Initializes the batch generator object. Stores the variables and first \"letter batch\".\n",
    "    text is text to be processed\n",
    "    batch_size is size of batch (number of samples)\n",
    "    seq_length represents the length of sequence\n",
    "    vocab_size is number of words in vocabulary (assumes one-hot encoding)\n",
    "    \"\"\"\n",
    "    # Store input parameters.\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._seq_length = seq_length\n",
    "    self._vocab_size = vocab_size\n",
    "    # Divide text into segments depending on number of batches, each segment determines a cursor position for a batch.\n",
    "    segment = self._text_size // batch_size\n",
    "    # Set initial cursor position.\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    # Store first \"letter batch\".\n",
    "    self._last_letter_batch = self._next_letter_batch()\n",
    "  \n",
    "  def _next_letter_batch(self):\n",
    "    \"\"\"\n",
    "    Returns a batch containing of encoded single letters depending on the current batch \n",
    "    cursor positions in the data.\n",
    "    Returned \"letter batch\" is of size batch_size x vocab_size\n",
    "    \"\"\"\n",
    "    letter_batch = np.zeros(shape=(self._batch_size, self._vocab_size), dtype=np.float)\n",
    "    # Iterate through \"samples\"\n",
    "    for b in range(self._batch_size):\n",
    "      # Set 1 in position pointed out by one-hot char encoding.\n",
    "      letter_batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return letter_batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    # First add last letter from previous batch (the \"additional one\").\n",
    "    batches = [self._last_letter_batch]\n",
    "    for step in range(self._seq_length):\n",
    "      batches.append(self._next_letter_batch())\n",
    "    # Store last \"letter batch\" for next batch.\n",
    "    self._last_letter_batch = batches[-1]\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set =  aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memote\n",
      "['z calloway centrust c', ' league promises the ', 'ion including quotron', 'r proposal for a full', 'n china is very compl', ' december that was su', 'ogilvy group was  unk', 'lis a general electri', 've earlier this year ', 'f philip morris cos  ']\n"
     ]
    }
   ],
   "source": [
    "# Trick - override first 10 chars\n",
    "#list1 = list(train_text)\n",
    "#for i in range(2):\n",
    "#    list1[i] = 'z'\n",
    "#train_text = ''.join(list1)\n",
    "print(\"Test set =\", train_text[0:100])\n",
    "\n",
    "# Create two objects for training and validation batch generation.\n",
    "train_batches = BatchGenerator(train_text, BATCH_SIZE, SEQ_LENGTH, vocabulary_size)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1, vocabulary_size)\n",
    "\n",
    "batch = train_batches.next()\n",
    "batch = train_batches.next()\n",
    "#print(\"Batch = \", batch)\n",
    "print(batches2string(batch))\n",
    "#print(\"batch len = num of enrollings\",len(batch))\n",
    "#for i in range(num_unrollings):\n",
    "#    print(\"i = \", i, \"letter=\", batches2string(batch)[0][i][0], \"bits = \", batch[i][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for calculation of loss =  - log2 prob (BPC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"\n",
    "  Log-probability of the true labels in a predicted batch.\n",
    "  Assumes that predictions/labels are of shape [batch_size x 1] (i.e. a batch of 1-char sequences)\n",
    "  \"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  # Divide by the batch size (shape[0]).\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state, name):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        # Calculate gates activations.\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib, name=\"Input_gate\")\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb, name=\"Forget_gate\")\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob, name=\"Output_gate\")\n",
    "\n",
    "        update = tf.add(tf.matmul(i, cx), tf.matmul(o, cm) + cb, name=\"Update\")\n",
    "        state = tf.add(forget_gate * state, input_gate * tf.tanh(update), name=\"State_update\")\n",
    "        output = output_gate * tf.tanh(state)\n",
    "        return output, state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Definition of tensor graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 0. Placeholders for inputs.\n",
    "with tf.name_scope(\"Input_data\"):\n",
    "  # Define input data buffers.\n",
    "  train_data = list()\n",
    "  for _ in range(SEQ_LENGTH + 1):\n",
    "    # Collect placeholders for inputs/labels.\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[BATCH_SIZE,vocabulary_size], name=\"Input_data\"))\n",
    "  # Collection of training inputs.\n",
    "  train_inputs = train_data[:SEQ_LENGTH]\n",
    "  # Labels are pointing to the same placeholders!\n",
    "  # Labels are inputs shifted by one time step.\n",
    "  train_labels = train_data[1:]  \n",
    "\n",
    "# 1. Inference ops.\n",
    "with tf.name_scope(\"Inference\"):\n",
    "  # Define parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1), name=\"ix\")\n",
    "  im = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"im\")\n",
    "  ib = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"ib\")\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1), name=\"fx\")\n",
    "  fm = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"fm\")\n",
    "  fb = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"fb\")\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1), name=\"cx\")\n",
    "  cm = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"cm\")\n",
    "  cb = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"cb\")\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1), name=\"ox\")\n",
    "  om = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"om\")\n",
    "  ob = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"ob\")\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([BATCH_SIZE, HIDDEN_SIZE]), trainable=False, name=\"saved_output\")\n",
    "  saved_state = tf.Variable(tf.zeros([BATCH_SIZE, HIDDEN_SIZE]), trainable=False, name=\"saved_state\")\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, vocabulary_size], -0.1, 0.1), name=\"w\")\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]), name=\"b\")\n",
    "  \n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state, \"cell\")\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "    # Fully connected layer on top => classification.\n",
    "    # In fact we will create lots of FC layers (one for each output layer), with shared weights.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "\n",
    "# 2. Loss ops.\n",
    "with tf.name_scope(\"Loss\"):\n",
    "    # Loss function(s) - one for every output generated by every lstm cell.\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "    # Add loss summary.\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "# 3. Training ops.  \n",
    "with tf.name_scope(\"Training\"):\n",
    "  # Optimizer-related variables.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "with tf.name_scope(\"Evaluation\") as scope:\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "with tf.name_scope(\"Sample_generation\") as scope:\n",
    "  # Greate graphs for sampling and validation evaluation: batch 1, \"no unrolling\".\n",
    "\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size], name=\"Input_data\")\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"Hidden_state\")\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"Output_data\")\n",
    "\n",
    "  # Node responsible for resetting the state and output.\n",
    "  reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, HIDDEN_SIZE])),\n",
    "      saved_sample_state.assign(tf.zeros([1, HIDDEN_SIZE])))\n",
    "  # Single LSTM cell.\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state, \"cell\")\n",
    "  # Output depends on the hidden state.\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output), \n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b, name=\"logits\"), name=\"outputs\")\n",
    "\n",
    "# Merge all summaries.\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "# 4. Init global variable.\n",
    "init = tf.global_variables_initializer()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for language generation (letter sampling etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Eventually clear the log dir.\n",
    "if tf.gfile.Exists(LOG_DIR):\n",
    "  tf.gfile.DeleteRecursively(LOG_DIR)\n",
    "# Create (new) log dir.\n",
    "tf.gfile.MakeDirs(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of iterations=  25508\n",
      "Initialized\n",
      "Average loss at step 0: 4.078313 learning rate: 10.000000\n",
      "Train set minibatch BPC: 4.08\n",
      "================================================================================\n",
      "`Mf GsCFzfDGr Q rJcjwAMNtI` lkW[\\sbHhTjTxrl   oseIa t[ AmAexfovOWo KtbJMHf  lh][\n",
      "zEqzJwis^yLt[N\\ns ArJ^vteB RsaUopfeGOHr vCn QlAS yFtB\\rpFWe C aEp]YW Kx ofFNp[se\n",
      "TDnNNrJ]uvs HziKh LocDBm`uenWouz  v[d[ VCcZ  ji `thayQe lA hF[ kw[hat eAjWP EiM \n",
      "TdaZ SBqkE Emzztcsgd aYu   lxZC\\Gt^a zY JnOZRK v [DW_Q`]iSmp tasQtYwFWZUCokxit f\n",
      "RYTs soofyms Cae CLEQ[B q_ maD y t tD\\MPdt _RwQVcH Sr pHNsU vB Dl_R  `inyHS oIHg\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 3.44\n",
      "Average loss at step 100: 2.895574 learning rate: 10.000000\n",
      "Train set minibatch BPC: 2.57\n",
      "Validation set BPC (1-char): 2.38\n",
      "Average loss at step 200: 2.336279 learning rate: 10.000000\n",
      "Train set minibatch BPC: 2.07\n",
      "Validation set BPC (1-char): 2.02\n",
      "Average loss at step 300: 2.123134 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.97\n",
      "Validation set BPC (1-char): 1.73\n",
      "Average loss at step 400: 1.994215 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.81\n",
      "Validation set BPC (1-char): 1.71\n",
      "Average loss at step 500: 1.941371 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.84\n",
      "Validation set BPC (1-char): 1.81\n",
      "Average loss at step 600: 1.917497 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.84\n",
      "Validation set BPC (1-char): 1.86\n",
      "Average loss at step 700: 1.854320 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.81\n",
      "Validation set BPC (1-char): 1.78\n",
      "Average loss at step 800: 1.857386 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.85\n",
      "Validation set BPC (1-char): 1.81\n",
      "Average loss at step 900: 1.808512 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.87\n",
      "Validation set BPC (1-char): 1.81\n",
      "Average loss at step 1000: 1.751635 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.75\n",
      "================================================================================\n",
      "ke a munt orst acthomes showe colunt forting compayion worg byend plofed hank ha\n",
      "y ravias to irdut in repones sait   shoriectly grold earce the lonorh  unk  otte\n",
      " the vireting heulatine of  unk    thred mens up bechapalions to the the muns sc\n",
      "s in fortment surs hay comp siou s tasing bity stedgen east the a poond forwas s\n",
      "vere of theinet was und a fute sef thinks seact   tocl fore N by   murchir javit\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.99\n",
      "Average loss at step 1100: 1.750973 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.50\n",
      "Validation set BPC (1-char): 1.92\n",
      "Average loss at step 1200: 1.754258 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.90\n",
      "Validation set BPC (1-char): 1.85\n",
      "Average loss at step 1300: 1.748052 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.69\n",
      "Validation set BPC (1-char): 1.95\n",
      "Average loss at step 1400: 1.723895 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.70\n",
      "Validation set BPC (1-char): 1.70\n",
      "Average loss at step 1500: 1.711626 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.40\n",
      "Validation set BPC (1-char): 1.86\n",
      "Average loss at step 1600: 1.695227 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.85\n",
      "Validation set BPC (1-char): 1.76\n",
      "Average loss at step 1700: 1.646358 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.55\n",
      "Validation set BPC (1-char): 1.71\n",
      "Average loss at step 1800: 1.635288 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.73\n",
      "Validation set BPC (1-char): 1.88\n",
      "Average loss at step 1900: 1.638597 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.69\n",
      "Validation set BPC (1-char): 1.85\n",
      "Average loss at step 2000: 1.609090 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.47\n",
      "================================================================================\n",
      "Jers or loing ot  s tord   mones for goeld  unk  the real of theer jost  unk    \n",
      "ge to nation planed ay expejate  unk    the beclowings is approvanar mochs growe\n",
      "rise says boy presicuble work and have expece   new chansled of the is leadent b\n",
      "J of and is read scepen a  unk  compuder cake who to rave loak is its such mosse\n",
      "ley   the or sexter congerste scubity the phain and  unk  bank wain part   conce\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.74\n",
      "Average loss at step 2100: 1.592429 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.80\n",
      "Validation set BPC (1-char): 1.85\n",
      "Average loss at step 2200: 1.587820 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.49\n",
      "Validation set BPC (1-char): 1.80\n",
      "Average loss at step 2300: 1.539573 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.58\n",
      "Validation set BPC (1-char): 1.89\n",
      "Average loss at step 2400: 1.515081 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.47\n",
      "Validation set BPC (1-char): 1.81\n",
      "Average loss at step 2500: 1.539763 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.46\n",
      "Validation set BPC (1-char): 1.65\n",
      "Average loss at step 2600: 1.556420 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.66\n",
      "Validation set BPC (1-char): 1.51\n",
      "Average loss at step 2700: 1.579528 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.62\n",
      "Validation set BPC (1-char): 1.60\n",
      "Average loss at step 2800: 1.573388 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.67\n",
      "Validation set BPC (1-char): 1.62\n",
      "Average loss at step 2900: 1.583134 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.49\n",
      "Validation set BPC (1-char): 1.47\n",
      "Average loss at step 3000: 1.632236 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.69\n",
      "================================================================================\n",
      "C from see yeired milled norch onteming home lestreta bust blely sole ofs defice\n",
      "Uloter if resowed dosform the preative market emplects yoak memit its by twosie \n",
      "H le is foe buy another any on forche cappest could pressexter the syection onep\n",
      "Teds forginisuoishan  unk  a limeing ofter egrepust N    unk  norestion the spat\n",
      "Me jroal doss planies on cainst by trake ormaterisal brink toon that rev ithowin\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.48\n",
      "Average loss at step 3100: 1.594169 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.49\n",
      "Validation set BPC (1-char): 1.67\n",
      "Average loss at step 3200: 1.617946 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.42\n",
      "Validation set BPC (1-char): 1.63\n",
      "Average loss at step 3300: 1.566552 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.62\n",
      "Validation set BPC (1-char): 1.65\n",
      "Average loss at step 3400: 1.541904 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.79\n",
      "Validation set BPC (1-char): 1.79\n",
      "Average loss at step 3500: 1.547933 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.62\n",
      "Validation set BPC (1-char): 1.56\n",
      "Average loss at step 3600: 1.558363 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.45\n",
      "Validation set BPC (1-char): 1.65\n",
      "Average loss at step 3700: 1.572473 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.39\n",
      "Validation set BPC (1-char): 1.60\n",
      "Average loss at step 3800: 1.559844 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.69\n",
      "Validation set BPC (1-char): 1.58\n",
      "Average loss at step 3900: 1.566996 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.74\n",
      "Validation set BPC (1-char): 1.50\n",
      "Average loss at step 4000: 1.540618 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.61\n",
      "================================================================================\n",
      "_isced stremp corring l cal sinc   but and mage  unk  ms peymalion an declined e\n",
      "linely of cakon stock ow husures bs arid contared reasted a mangbinu colles for \n",
      "x grame which in loho geniced for specumes declined the  unk  profital merbited \n",
      "p specualing incomple company mr  shigen compaidsurs are instatires holizing all\n",
      "hing rohn N shurnic dourd crifutent on progranis its rol cnulting is forble a   \n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.65\n",
      "Average loss at step 4100: 1.545260 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.54\n",
      "Validation set BPC (1-char): 1.61\n",
      "Average loss at step 4200: 1.521221 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set BPC (1-char): 1.66\n",
      "Average loss at step 4300: 1.505644 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.56\n",
      "Validation set BPC (1-char): 1.61\n",
      "Average loss at step 4400: 1.495074 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.60\n",
      "Validation set BPC (1-char): 1.50\n",
      "Average loss at step 4500: 1.511834 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.60\n",
      "Validation set BPC (1-char): 1.66\n",
      "Average loss at step 4600: 1.515230 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.38\n",
      "Validation set BPC (1-char): 1.59\n",
      "Average loss at step 4700: 1.483917 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.63\n",
      "Validation set BPC (1-char): 1.57\n",
      "Average loss at step 4800: 1.547642 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.60\n",
      "Validation set BPC (1-char): 1.65\n",
      "Average loss at step 4900: 1.544378 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.70\n",
      "Validation set BPC (1-char): 1.61\n",
      "Average loss at step 5000: 1.503120 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.42\n",
      "================================================================================\n",
      "is for the tor companies  unk  and compared N N   generes cent   citer appeal  u\n",
      "ker stractdents siesting those insticus   the big stakements  unk  was we bletex\n",
      "V the examerx things mastmen  s ratus reforms equirantges stocks sie is prokotin\n",
      "jed planting  unk  very on the management comparare incessuence   N million a wa\n",
      "^e are up with monthses of allown  s wairs a yee muchs in survey N N an  unk  in\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.55\n",
      "Average loss at step 5100: 1.468456 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.30\n",
      "Validation set BPC (1-char): 1.46\n",
      "Average loss at step 5200: 1.466671 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.50\n",
      "Validation set BPC (1-char): 1.50\n",
      "Average loss at step 5300: 1.448002 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.31\n",
      "Validation set BPC (1-char): 1.50\n",
      "Average loss at step 5400: 1.452967 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.72\n",
      "Validation set BPC (1-char): 1.39\n",
      "Average loss at step 5500: 1.462608 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.57\n",
      "Validation set BPC (1-char): 1.48\n",
      "Average loss at step 5600: 1.465549 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.49\n",
      "Validation set BPC (1-char): 1.45\n",
      "Average loss at step 5700: 1.481721 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.31\n",
      "Validation set BPC (1-char): 1.52\n",
      "Average loss at step 5800: 1.465805 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.49\n",
      "Validation set BPC (1-char): 1.66\n",
      "Average loss at step 5900: 1.476525 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.55\n",
      "Validation set BPC (1-char): 1.60\n",
      "Average loss at step 6000: 1.452092 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.48\n",
      "================================================================================\n",
      "Ted person branging   the count warent are  unk  N only in set my whe big   wing\n",
      " bank official sario   that the at the notes nos max the wall is pay tows the cu\n",
      "guth were quition defense that the u s sireced   fact is boot co  s decimidue he\n",
      "zed of N vind  unk  survey of the  unk  oil expandent to ssitturng s echricn let\n",
      "L interest n t that country of a datulyicked resslon to build is   investment un\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.49\n",
      "Average loss at step 6100: 1.491131 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.31\n",
      "Validation set BPC (1-char): 1.46\n",
      "Average loss at step 6200: 1.450866 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.52\n",
      "Validation set BPC (1-char): 1.41\n",
      "Average loss at step 6300: 1.463610 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.42\n",
      "Validation set BPC (1-char): 1.46\n",
      "Average loss at step 6400: 1.415042 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.40\n",
      "Validation set BPC (1-char): 1.34\n",
      "Average loss at step 6500: 1.473246 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.38\n",
      "Validation set BPC (1-char): 1.54\n",
      "Average loss at step 6600: 1.473114 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.40\n",
      "Validation set BPC (1-char): 1.43\n",
      "Average loss at step 6700: 1.425011 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.64\n",
      "Validation set BPC (1-char): 1.38\n",
      "Average loss at step 6800: 1.439287 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.39\n",
      "Validation set BPC (1-char): 1.42\n",
      "Average loss at step 6900: 1.471694 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.28\n",
      "Validation set BPC (1-char): 1.36\n",
      "Average loss at step 7000: 1.433914 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.23\n",
      "================================================================================\n",
      "chel pricement to N N fill are  unk  can ago warry them tone to use memoteved th\n",
      "ters and entorl rachers of through chespan norisaming declined kt parted from ma\n",
      "ker lawee from partantees are are the toon on N   a fued do national out certib \n",
      "Qod that the legal a   N  unk  rate of  unk  third are products workers   a   N \n",
      "lowpers for the ectioncution alan back ment on monthral some   the even lonc the\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.38\n",
      "Average loss at step 7100: 1.464790 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.47\n",
      "Validation set BPC (1-char): 1.43\n",
      "Average loss at step 7200: 1.445567 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.51\n",
      "Validation set BPC (1-char): 1.40\n",
      "Average loss at step 7300: 1.403208 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.48\n",
      "Validation set BPC (1-char): 1.49\n",
      "Average loss at step 7400: 1.426796 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.61\n",
      "Validation set BPC (1-char): 1.31\n",
      "Average loss at step 7500: 1.396783 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.34\n",
      "Validation set BPC (1-char): 1.47\n",
      "Average loss at step 7600: 1.418653 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.42\n",
      "Validation set BPC (1-char): 1.42\n",
      "Average loss at step 7700: 1.397098 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.41\n",
      "Validation set BPC (1-char): 1.59\n",
      "Average loss at step 7800: 1.446292 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.60\n",
      "Validation set BPC (1-char): 1.36\n",
      "Average loss at step 7900: 1.440111 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.46\n",
      "Validation set BPC (1-char): 1.54\n",
      "Average loss at step 8000: 1.440618 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.54\n",
      "================================================================================\n",
      "lands said court in medisty are at the N N foreeway battle an here etcusticalss \n",
      "nes was bond  s privational on commory acquisition even sly insteem affects sign\n",
      "gations year term were for four libely also  unk  gravily of the nearly cable an\n",
      "Jed tok  unk   unk  worker of N it   in the tales in vote on three with vine con\n",
      "E inchupsor of its   N negteraging N N that executive to take it in the bignas o\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.41\n",
      "Average loss at step 8100: 1.427985 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.35\n",
      "Validation set BPC (1-char): 1.48\n",
      "Average loss at step 8200: 1.413412 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.47\n",
      "Validation set BPC (1-char): 1.50\n",
      "Average loss at step 8300: 1.452456 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.41\n",
      "Validation set BPC (1-char): 1.44\n",
      "Average loss at step 8400: 1.387958 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.28\n",
      "Validation set BPC (1-char): 1.41\n",
      "Average loss at step 8500: 1.386694 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.33\n",
      "Validation set BPC (1-char): 1.46\n",
      "Average loss at step 8600: 1.382943 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.39\n",
      "Validation set BPC (1-char): 1.52\n",
      "Average loss at step 8700: 1.378190 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.25\n",
      "Validation set BPC (1-char): 1.50\n",
      "Average loss at step 8800: 1.351600 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.33\n",
      "Validation set BPC (1-char): 1.59\n",
      "Average loss at step 8900: 1.372413 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.44\n",
      "Validation set BPC (1-char): 1.39\n",
      "Average loss at step 9000: 1.389837 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.29\n",
      "================================================================================\n",
      "N   athing the minchome is on shares   since N importmonts a   unit other with N\n",
      "Qeed then forme   ruyer begofistrows such on earnings rally pretics with more as\n",
      "jees in what the milturs you n t  unk  is the will due its cluin reported issuot\n",
      "jeese a repare sumfilias   out intrang hus thought may boars  unk    whether as \n",
      "Arooged chief study to  unk  the reseascority cash namin   at  unk  plan at depa\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set BPC (1-char): 1.47\n",
      "Average loss at step 9100: 1.431967 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.58\n",
      "Validation set BPC (1-char): 1.55\n",
      "Average loss at step 9200: 1.432613 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.60\n",
      "Validation set BPC (1-char): 1.53\n",
      "Average loss at step 9300: 1.376810 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.54\n",
      "Validation set BPC (1-char): 1.50\n",
      "Average loss at step 9400: 1.419293 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.73\n",
      "Validation set BPC (1-char): 1.43\n",
      "Average loss at step 9500: 1.409923 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.51\n",
      "Validation set BPC (1-char): 1.46\n",
      "Average loss at step 9600: 1.398950 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.68\n",
      "Validation set BPC (1-char): 1.48\n",
      "Average loss at step 9700: 1.408198 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.31\n",
      "Validation set BPC (1-char): 1.34\n",
      "Average loss at step 9800: 1.386627 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.38\n",
      "Validation set BPC (1-char): 1.30\n",
      "Average loss at step 9900: 1.391765 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.39\n",
      "Validation set BPC (1-char): 1.21\n",
      "Average loss at step 10000: 1.414187 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.20\n",
      "================================================================================\n",
      "guer the cesticip signed its mr  even progrestibed they intression used the perv\n",
      "vest but for cloid to said   N   excensed the judgers official offer   the hucti\n",
      "U will commitations firrs parts was heon year  unk  bigger in brangevely be at d\n",
      "Zeshics of   N   mr  roops than early chief actuonal bliae market an hide  unk  \n",
      "D for all aruley laws high bucke meding and  unk  edipaimers cinisiation interpt\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.48\n",
      "Average loss at step 10100: 1.354263 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.45\n",
      "Validation set BPC (1-char): 1.56\n",
      "Average loss at step 10200: 1.447528 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.58\n",
      "Validation set BPC (1-char): 1.52\n",
      "Average loss at step 10300: 1.471265 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.33\n",
      "Validation set BPC (1-char): 1.53\n",
      "Average loss at step 10400: 1.425148 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.33\n",
      "Validation set BPC (1-char): 1.52\n",
      "Average loss at step 10500: 1.416244 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.42\n",
      "Validation set BPC (1-char): 1.55\n",
      "Average loss at step 10600: 1.425746 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.50\n",
      "Validation set BPC (1-char): 1.44\n",
      "Average loss at step 10700: 1.458518 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.25\n",
      "Validation set BPC (1-char): 1.45\n",
      "Average loss at step 10800: 1.396875 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.19\n",
      "Validation set BPC (1-char): 1.38\n",
      "Average loss at step 10900: 1.404464 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.40\n",
      "Validation set BPC (1-char): 1.31\n",
      "Average loss at step 11000: 1.399238 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.52\n",
      "================================================================================\n",
      "Ooused richn williamed as leasts hin spectfession   but far ratiop awouity   the\n",
      "wich a trading    unk  exchange to home junding the close tro shisble  s hower i\n",
      "Ker operations   N billion and common and couble the bulling capit sub  s virey \n",
      "T   N   N N opsurtments    unk  and meachem to  s mig gorate the hoping tell on \n",
      "y those think fert   do abso with an arto worrage on parts cosisling steckle was\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.41\n",
      "Average loss at step 11100: 1.396604 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.64\n",
      "Validation set BPC (1-char): 1.47\n",
      "Average loss at step 11200: 1.401055 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.28\n",
      "Validation set BPC (1-char): 1.35\n",
      "Average loss at step 11300: 1.405045 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.50\n",
      "Validation set BPC (1-char): 1.55\n",
      "Average loss at step 11400: 1.414248 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.33\n",
      "Validation set BPC (1-char): 1.45\n",
      "Average loss at step 11500: 1.442020 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.54\n",
      "Validation set BPC (1-char): 1.41\n",
      "Average loss at step 11600: 1.381199 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.47\n",
      "Validation set BPC (1-char): 1.38\n",
      "Average loss at step 11700: 1.375302 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.32\n",
      "Validation set BPC (1-char): 1.39\n",
      "Average loss at step 11800: 1.382769 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.46\n",
      "Validation set BPC (1-char): 1.20\n",
      "Average loss at step 11900: 1.441124 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.37\n",
      "Validation set BPC (1-char): 1.20\n",
      "Average loss at step 12000: 1.377187 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.45\n",
      "================================================================================\n",
      "Ied longing stock  unk  group somes seciated are licted  unk  includings stakes \n",
      "]itor home for market in righting   N singi sunding a completer control estating\n",
      "Ging sere  unk  accentuping   the weeker competed ca moring to fell under operat\n",
      "N the will become income and atband   N million from   N N officiave one who whi\n",
      "Dal expand by the conspete buse that the dividery ted N auvkesneted   it in merg\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.41\n",
      "Average loss at step 12100: 1.398320 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.41\n",
      "Validation set BPC (1-char): 1.39\n",
      "Average loss at step 12200: 1.412150 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.57\n",
      "Validation set BPC (1-char): 1.18\n",
      "Average loss at step 12300: 1.427279 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.47\n",
      "Validation set BPC (1-char): 1.33\n",
      "Average loss at step 12400: 1.412963 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.39\n",
      "Validation set BPC (1-char): 1.39\n",
      "Average loss at step 12500: 1.373502 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.16\n",
      "Validation set BPC (1-char): 1.38\n",
      "Average loss at step 12600: 1.421448 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.43\n",
      "Validation set BPC (1-char): 1.39\n",
      "Average loss at step 12700: 1.433667 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.33\n",
      "Validation set BPC (1-char): 1.43\n",
      "Average loss at step 12800: 1.408303 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.45\n",
      "Validation set BPC (1-char): 1.31\n",
      "Average loss at step 12900: 1.412687 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.53\n",
      "Validation set BPC (1-char): 1.26\n",
      "Average loss at step 13000: 1.386987 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.27\n",
      "================================================================================\n",
      "s a  N bullwal suore ldw standarry reportmently acquirept what   N year  unk  co\n",
      "verion company dol and from the guart of it and   law control to year and totarl\n",
      "Fencing take the sapeement plansing   in in they lants part its developal said t\n",
      "\\ers planted  unk  in one  unk  replitment being theor sett to majary to how and\n",
      "Ming papeed busher backed   so  unk  concemmenting grate cregif   unk   unkshica\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.38\n",
      "Average loss at step 13100: 1.378452 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.31\n",
      "Validation set BPC (1-char): 1.30\n",
      "Average loss at step 13200: 1.386702 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.43\n",
      "Validation set BPC (1-char): 1.20\n",
      "Average loss at step 13300: 1.384444 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.29\n",
      "Validation set BPC (1-char): 1.17\n",
      "Average loss at step 13400: 1.411533 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.54\n",
      "Validation set BPC (1-char): 1.38\n",
      "Average loss at step 13500: 1.391625 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.76\n",
      "Validation set BPC (1-char): 1.40\n",
      "Average loss at step 13600: 1.388355 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.42\n",
      "Validation set BPC (1-char): 1.25\n",
      "Average loss at step 13700: 1.405086 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.65\n",
      "Validation set BPC (1-char): 1.37\n",
      "Average loss at step 13800: 1.409002 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set BPC (1-char): 1.26\n",
      "Average loss at step 13900: 1.423380 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.50\n",
      "Validation set BPC (1-char): 1.33\n",
      "Average loss at step 14000: 1.440885 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.43\n",
      "================================================================================\n",
      "jortition   computers and when the seviral increase he selling  unk    street a \n",
      "N more   N sho service on   N timely blibus earlier back l  the halfing to   N N\n",
      "J  restibut eartorned it production the reare jusying the magan and next years f\n",
      "Iee contricutry and mober other  unk  year vereing  s   oftetive bank throks chi\n",
      "Fer is   N a shrecover N N are chealy   the instandownes of hevdan stone his rat\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.48\n",
      "Average loss at step 14100: 1.434483 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.56\n",
      "Validation set BPC (1-char): 1.45\n",
      "Average loss at step 14200: 1.427328 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.28\n",
      "Validation set BPC (1-char): 1.43\n",
      "Average loss at step 14300: 1.436871 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.34\n",
      "Validation set BPC (1-char): 1.49\n",
      "Average loss at step 14400: 1.404713 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.31\n",
      "Validation set BPC (1-char): 1.33\n",
      "Average loss at step 14500: 1.435044 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.46\n",
      "Validation set BPC (1-char): 1.39\n",
      "Average loss at step 14600: 1.439625 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.31\n",
      "Validation set BPC (1-char): 1.47\n",
      "Average loss at step 14700: 1.390442 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.55\n",
      "Validation set BPC (1-char): 1.58\n",
      "Average loss at step 14800: 1.405225 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.52\n",
      "Validation set BPC (1-char): 1.57\n",
      "Average loss at step 14900: 1.403445 learning rate: 0.100000\n",
      "Train set minibatch BPC: 1.52\n",
      "Validation set BPC (1-char): 1.39\n",
      "Average loss at step 15000: 1.433726 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.33\n",
      "================================================================================\n",
      "Vally informentages first market of co   N mincuand for  unk  third revenue chie\n",
      "\\ors med viren gal and N N vitta inventor of nouder alread to currently stopan c\n",
      "vicial can giflist are have ewsule ability who quarter do gor tod exervice   the\n",
      "Qers to carril     N a share manfed offere  unk  was n t junks N maide said reme\n",
      "orded hals net market dirlegighn asserrapituria   N million  s venation again a \n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.56\n",
      "Average loss at step 15100: 1.408554 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.29\n",
      "Validation set BPC (1-char): 1.57\n",
      "Average loss at step 15200: 1.397839 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.27\n",
      "Validation set BPC (1-char): 1.37\n",
      "Average loss at step 15300: 1.390210 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.44\n",
      "Validation set BPC (1-char): 1.36\n",
      "Average loss at step 15400: 1.404222 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.39\n",
      "Validation set BPC (1-char): 1.27\n",
      "Average loss at step 15500: 1.427676 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.33\n",
      "Validation set BPC (1-char): 1.42\n",
      "Average loss at step 15600: 1.405438 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.43\n",
      "Validation set BPC (1-char): 1.54\n",
      "Average loss at step 15700: 1.375677 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.09\n",
      "Validation set BPC (1-char): 1.50\n",
      "Average loss at step 15800: 1.368074 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.28\n",
      "Validation set BPC (1-char): 1.42\n",
      "Average loss at step 15900: 1.389159 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.48\n",
      "Validation set BPC (1-char): 1.44\n",
      "Average loss at step 16000: 1.456152 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.45\n",
      "================================================================================\n",
      "umity which headyorden should nessichical bases possible plant a deportives from\n",
      "^ic most stacts   small sevides a dechandent raphie it   flond   the commimcte t\n",
      "]ed mr  to line the great  y planting other in the many   any higheing on would \n",
      "bing  unk  in requred   debide that the shares or equistics months to adman in n\n",
      "y franch heupe need who expected   home at a prospective promitions hearity in m\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.41\n",
      "Average loss at step 16100: 1.463738 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.40\n",
      "Validation set BPC (1-char): 1.38\n",
      "Average loss at step 16200: 1.426277 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.23\n",
      "Validation set BPC (1-char): 1.37\n",
      "Average loss at step 16300: 1.404066 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.40\n",
      "Validation set BPC (1-char): 1.46\n",
      "Average loss at step 16400: 1.411098 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.23\n",
      "Validation set BPC (1-char): 1.38\n",
      "Average loss at step 16500: 1.370617 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.20\n",
      "Validation set BPC (1-char): 1.34\n",
      "Average loss at step 16600: 1.387906 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.32\n",
      "Validation set BPC (1-char): 1.38\n",
      "Average loss at step 16700: 1.373977 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.52\n",
      "Validation set BPC (1-char): 1.42\n",
      "Average loss at step 16800: 1.394813 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.33\n",
      "Validation set BPC (1-char): 1.56\n",
      "Average loss at step 16900: 1.392527 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.20\n",
      "Validation set BPC (1-char): 1.48\n",
      "Average loss at step 17000: 1.430755 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.42\n",
      "================================================================================\n",
      "y from anverthray merisharge back discapion rays   demand held telleviers effect\n",
      "hachased what is the widet he suning meet cost of the  unk  stock own than be re\n",
      "K   p complay regaided said an proposative by N operations  s up presidentarletm\n",
      "ue  unk  entrol  unk  than the liteloday wostest a year as it franning marazals \n",
      "d year start mr   unk  said in trade several industry saleUon collagion bank inc\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.36\n",
      "Average loss at step 17100: 1.421108 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.28\n",
      "Validation set BPC (1-char): 1.45\n",
      "Average loss at step 17200: 1.395645 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.44\n",
      "Validation set BPC (1-char): 1.44\n",
      "Average loss at step 17300: 1.396796 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.36\n",
      "Validation set BPC (1-char): 1.42\n",
      "Average loss at step 17400: 1.381017 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.51\n",
      "Validation set BPC (1-char): 1.39\n",
      "Average loss at step 17500: 1.426234 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.38\n",
      "Validation set BPC (1-char): 1.44\n",
      "Average loss at step 17600: 1.436846 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.35\n",
      "Validation set BPC (1-char): 1.37\n",
      "Average loss at step 17700: 1.432532 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.43\n",
      "Validation set BPC (1-char): 1.46\n",
      "Average loss at step 17800: 1.413331 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.30\n",
      "Validation set BPC (1-char): 1.33\n",
      "Average loss at step 17900: 1.432302 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.53\n",
      "Validation set BPC (1-char): 1.37\n",
      "Average loss at step 18000: 1.396613 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.27\n",
      "================================================================================\n",
      " reseasific crava to quatter bann possible crive to led our takh age book seficu\n",
      "bonters failed up chome dispressovenced that   N   acquired to years said   N mi\n",
      "Ced cot siff consent damaer first some said N N N lawen trefed pressure keft rel\n",
      "T mr  buy   they earthcontiop brote administration widlevel two art of the busin\n",
      "Oations  joced to about N u s  its the of concerns and it N N a shiffed plant wi\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.48\n",
      "Average loss at step 18100: 1.462148 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set BPC (1-char): 1.45\n",
      "Average loss at step 18200: 1.486317 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.52\n",
      "Validation set BPC (1-char): 1.32\n",
      "Average loss at step 18300: 1.477225 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.54\n",
      "Validation set BPC (1-char): 1.39\n",
      "Average loss at step 18400: 1.400212 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.41\n",
      "Validation set BPC (1-char): 1.27\n",
      "Average loss at step 18500: 1.451151 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.56\n",
      "Validation set BPC (1-char): 1.35\n",
      "Average loss at step 18600: 1.458757 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.48\n",
      "Validation set BPC (1-char): 1.39\n",
      "Average loss at step 18700: 1.430721 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.39\n",
      "Validation set BPC (1-char): 1.52\n",
      "Average loss at step 18800: 1.412596 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.41\n",
      "Validation set BPC (1-char): 1.46\n",
      "Average loss at step 18900: 1.438346 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.49\n",
      "Validation set BPC (1-char): 1.51\n",
      "Average loss at step 19000: 1.417867 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.32\n",
      "================================================================================\n",
      "ly its for euthil could hold pacialies and residents doly naded that indivivi  u\n",
      "y Zotes in ponume from was buir N N in the dividoul   said the in reuports repub\n",
      "N many wrotes share executive  s hold corp  with people sut interest so two debs\n",
      "rical costs a gooks a todume is airst interligy news  unk  salideral thould N N \n",
      "bard a current profitab stear of lipetines of one it  unk   s afto chairman an N\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.35\n",
      "Average loss at step 19100: 1.445697 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.54\n",
      "Validation set BPC (1-char): 1.29\n",
      "Average loss at step 19200: 1.414027 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.57\n",
      "Validation set BPC (1-char): 1.26\n",
      "Average loss at step 19300: 1.412705 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.21\n",
      "Validation set BPC (1-char): 1.29\n",
      "Average loss at step 19400: 1.397150 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.50\n",
      "Validation set BPC (1-char): 1.43\n",
      "Average loss at step 19500: 1.391164 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.34\n",
      "Validation set BPC (1-char): 1.41\n",
      "Average loss at step 19600: 1.405350 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.68\n",
      "Validation set BPC (1-char): 1.33\n",
      "Average loss at step 19700: 1.450172 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.53\n",
      "Validation set BPC (1-char): 1.34\n",
      "Average loss at step 19800: 1.436442 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.34\n",
      "Validation set BPC (1-char): 1.37\n",
      "Average loss at step 19900: 1.483581 learning rate: 0.010000\n",
      "Train set minibatch BPC: 1.47\n",
      "Validation set BPC (1-char): 1.38\n",
      "Average loss at step 20000: 1.487553 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.59\n",
      "================================================================================\n",
      "Oing leacher formenvuy  umr   N plandoy   mr   unk   unk  befaer const drieing t\n",
      "cate   count market well nov  sen dollared trading founds for fewtred a for in s\n",
      "s in agher to plants parqual notweed wall broker company creation   polence id o\n",
      "Xe earlier law shower owner profit a N N fthe conglo all office  unk  aball the \n",
      "veem an exchinme purcheend of co  obliters funds whion and mr  but hand in pecia\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.30\n",
      "Average loss at step 20100: 1.458711 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.62\n",
      "Validation set BPC (1-char): 1.33\n",
      "Average loss at step 20200: 1.427135 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.31\n",
      "Validation set BPC (1-char): 1.38\n",
      "Average loss at step 20300: 1.424678 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.54\n",
      "Validation set BPC (1-char): 1.40\n",
      "Average loss at step 20400: 1.436740 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.44\n",
      "Validation set BPC (1-char): 1.52\n",
      "Average loss at step 20500: 1.413109 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.47\n",
      "Validation set BPC (1-char): 1.21\n",
      "Average loss at step 20600: 1.431052 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.54\n",
      "Validation set BPC (1-char): 1.19\n",
      "Average loss at step 20700: 1.412890 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.41\n",
      "Validation set BPC (1-char): 1.40\n",
      "Average loss at step 20800: 1.423660 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.42\n",
      "Validation set BPC (1-char): 1.07\n",
      "Average loss at step 20900: 1.414452 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.27\n",
      "Validation set BPC (1-char): 1.30\n",
      "Average loss at step 21000: 1.402239 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.68\n",
      "================================================================================\n",
      "^ic   awterd led over level generally off of expropse maince the lawines whether\n",
      "Qed to alds with guphop a proposial speton are  unk  he proiked and this chairma\n",
      "very he jults   insurattly coaxones as amenge texpspecial contract black said th\n",
      "Blated to monoustring the mr  orgory  s as   N billion to to a dokuation inflodi\n",
      "Cical its back il ara   u k   unk    relein   the sermenity as   N million than \n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.45\n",
      "Average loss at step 21100: 1.411220 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.53\n",
      "Validation set BPC (1-char): 1.43\n",
      "Average loss at step 21200: 1.442668 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.64\n",
      "Validation set BPC (1-char): 1.43\n",
      "Average loss at step 21300: 1.446826 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.65\n",
      "Validation set BPC (1-char): 1.58\n",
      "Average loss at step 21400: 1.436650 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.44\n",
      "Validation set BPC (1-char): 1.53\n",
      "Average loss at step 21500: 1.404494 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.40\n",
      "Validation set BPC (1-char): 1.29\n",
      "Average loss at step 21600: 1.393803 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.35\n",
      "Validation set BPC (1-char): 1.43\n",
      "Average loss at step 21700: 1.429194 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.35\n",
      "Validation set BPC (1-char): 1.47\n",
      "Average loss at step 21800: 1.395936 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.28\n",
      "Validation set BPC (1-char): 1.49\n",
      "Average loss at step 21900: 1.374333 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.32\n",
      "Validation set BPC (1-char): 1.45\n",
      "Average loss at step 22000: 1.410168 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.40\n",
      "================================================================================\n",
      "R rones in  unk  from a notely nst it embliate a quarter earlier  s onlyhee said\n",
      "quates this centing reportant as fiomed istome limes he democorad  unk  again ma\n",
      "Mer of the fanimia of settle to regars homed theme wared from   N million that n\n",
      "Kory succomplar musy like i side azministration   they foreign when may committe\n",
      "N from are contral about two streans as u s    sharehody   situations activitias\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.31\n",
      "Average loss at step 22100: 1.430018 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.39\n",
      "Validation set BPC (1-char): 1.33\n",
      "Average loss at step 22200: 1.408127 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.48\n",
      "Validation set BPC (1-char): 1.39\n",
      "Average loss at step 22300: 1.377835 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.52\n",
      "Validation set BPC (1-char): 1.33\n",
      "Average loss at step 22400: 1.424094 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.24\n",
      "Validation set BPC (1-char): 1.30\n",
      "Average loss at step 22500: 1.421668 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.34\n",
      "Validation set BPC (1-char): 1.28\n",
      "Average loss at step 22600: 1.410568 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.35\n",
      "Validation set BPC (1-char): 1.42\n",
      "Average loss at step 22700: 1.440232 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.46\n",
      "Validation set BPC (1-char): 1.39\n",
      "Average loss at step 22800: 1.400944 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.42\n",
      "Validation set BPC (1-char): 1.39\n",
      "Average loss at step 22900: 1.415118 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set BPC (1-char): 1.38\n",
      "Average loss at step 23000: 1.411109 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.32\n",
      "================================================================================\n",
      "Ted to deparytys these net tark prises prival panawer mr  tor program to gop sec\n",
      "He says a  unk  policies  unk  is an as the literly do   the provide ones  unk  \n",
      "   idever been shares rasestaniles are the quarters with ipp N p the namers indi\n",
      "P N about n t janading of ines was plancity also it throking at the units some  \n",
      " the  unk  is portance cleim to rigatilative such dowednarizate earlier in N to \n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.46\n",
      "Average loss at step 23100: 1.379511 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.59\n",
      "Validation set BPC (1-char): 1.45\n",
      "Average loss at step 23200: 1.450076 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.31\n",
      "Validation set BPC (1-char): 1.38\n",
      "Average loss at step 23300: 1.430366 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.59\n",
      "Validation set BPC (1-char): 1.38\n",
      "Average loss at step 23400: 1.429731 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.34\n",
      "Validation set BPC (1-char): 1.45\n",
      "Average loss at step 23500: 1.458252 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.31\n",
      "Validation set BPC (1-char): 1.32\n",
      "Average loss at step 23600: 1.444860 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.64\n",
      "Validation set BPC (1-char): 1.52\n",
      "Average loss at step 23700: 1.404139 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.54\n",
      "Validation set BPC (1-char): 1.25\n",
      "Average loss at step 23800: 1.458528 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.37\n",
      "Validation set BPC (1-char): 1.24\n",
      "Average loss at step 23900: 1.417027 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.36\n",
      "Validation set BPC (1-char): 1.50\n",
      "Average loss at step 24000: 1.399213 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.33\n",
      "================================================================================\n",
      "Ling of the the deported   the was revenue of go for N the also  unk  felled   t\n",
      "Bitia cancented resald anouter warres ic   about income young and iterses and N \n",
      "ver becanch   the tradels will been lawter chitfillly cantoral a market help an \n",
      "Zory estimated a greated irs cpreceisk trame said to the back with   N knymed it\n",
      "e perchoes in decidity fell alrinagent it has a sumsk secork funds and in revenu\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.49\n",
      "Average loss at step 24100: 1.421849 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.57\n",
      "Validation set BPC (1-char): 1.35\n",
      "Average loss at step 24200: 1.449968 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.26\n",
      "Validation set BPC (1-char): 1.38\n",
      "Average loss at step 24300: 1.415375 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.23\n",
      "Validation set BPC (1-char): 1.50\n",
      "Average loss at step 24400: 1.386470 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.27\n",
      "Validation set BPC (1-char): 1.29\n",
      "Average loss at step 24500: 1.362770 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.48\n",
      "Validation set BPC (1-char): 1.37\n",
      "Average loss at step 24600: 1.403186 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.24\n",
      "Validation set BPC (1-char): 1.36\n",
      "Average loss at step 24700: 1.438350 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.44\n",
      "Validation set BPC (1-char): 1.33\n",
      "Average loss at step 24800: 1.445279 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.41\n",
      "Validation set BPC (1-char): 1.40\n",
      "Average loss at step 24900: 1.443716 learning rate: 0.001000\n",
      "Train set minibatch BPC: 1.68\n",
      "Validation set BPC (1-char): 1.38\n",
      "Average loss at step 25000: 1.459071 learning rate: 0.000100\n",
      "Train set minibatch BPC: 1.19\n",
      "================================================================================\n",
      "]ase this realing standity to ball to finage last for it been intemptions is a c\n",
      "` reporter it   that   those adginie william as rumbertes indity are N that allo\n",
      "queting conxpect hust closed mr  geat of anded have gritable in our one vide in \n",
      "X planted rut to volub of national milling oct  shares among belay rall apprinti\n",
      "zer and insteried holder esteme gruman   poble blamesling tax president pounting\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.23\n",
      "Average loss at step 25100: 1.423075 learning rate: 0.000100\n",
      "Train set minibatch BPC: 1.56\n",
      "Validation set BPC (1-char): 1.31\n",
      "Average loss at step 25200: 1.425814 learning rate: 0.000100\n",
      "Train set minibatch BPC: 1.66\n",
      "Validation set BPC (1-char): 1.26\n",
      "Average loss at step 25300: 1.373857 learning rate: 0.000100\n",
      "Train set minibatch BPC: 1.49\n",
      "Validation set BPC (1-char): 1.50\n",
      "Average loss at step 25400: 1.382083 learning rate: 0.000100\n",
      "Train set minibatch BPC: 1.63\n",
      "Validation set BPC (1-char): 1.33\n",
      "Average loss at step 25500: 1.411790 learning rate: 0.000100\n",
      "Train set minibatch BPC: 1.28\n",
      "Validation set BPC (1-char): 1.14\n"
     ]
    }
   ],
   "source": [
    "num_steps =  train_size // (BATCH_SIZE*SEQ_LENGTH) #70001\n",
    "print(\"Total number of iterations= \", num_steps)\n",
    "summary_frequency = 100\n",
    "\n",
    "# Create session.\n",
    "sess = tf.InteractiveSession()\n",
    "# Create summary writers, point them to LOG_DIR.\n",
    "train_writer = tf.summary.FileWriter(LOG_DIR + '/train', sess.graph)\n",
    "valid_writer = tf.summary.FileWriter(LOG_DIR + '/valid')\n",
    "#test_writer = tf.summary.FileWriter(LOG_DIR + '/test')\n",
    "\n",
    "# Initialize global variables.\n",
    "tf.global_variables_initializer().run()\n",
    "print('Initialized')\n",
    "\n",
    "mean_loss = 0\n",
    "for step in range(num_steps):\n",
    "    # Get next batch and create a dictionary.\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(SEQ_LENGTH + 1):\n",
    "        feed_dict[train_data[i]] = batches[i]\n",
    "    # Run graph.\n",
    "    summary, _, l, predictions, lr = sess.run([merged_summaries, optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    train_writer.add_summary(summary, step)\n",
    "    train_writer.flush()\n",
    "    # Add loss to mean.\n",
    "    mean_loss += l\n",
    "    # Every (100) steps collect statistics.\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Train set minibatch BPC: %.2f' % logprob(predictions, labels))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          # Reset LSTM hidden state.\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set BPC.\n",
    "      reset_sample_state.run()\n",
    "      mean_valid_logprob = 0\n",
    "      # Sum for a single batch of size 1 - i.e. predict depending only on a single input character.\n",
    "      for _ in range(1000): #valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        mean_valid_logprob += logprob(predictions, b[1])\n",
    "      print('Validation set BPC (1-char): %.2f' % float(mean_valid_logprob / (1000))) #valid_size)))\n",
    "    # End of statistics collection\n",
    "\n",
    "# Close writers and session.\n",
    "train_writer.close()\n",
    "valid_writer.close()\n",
    "#test_writer.flush()\n",
    "#test_writer.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
