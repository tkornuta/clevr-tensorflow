{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import tarfile\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import shutil \n",
    "import random\n",
    "\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "# Dirs - must be absolute paths!\n",
    "LOG_DIR = '/tmp/tf/ptb_char_lstm/'\n",
    "# Local dir where PTB files will be stored.\n",
    "PTB_DIR = '/home/tkornuta/data/ptb/'\n",
    "\n",
    "# Filenames.\n",
    "TRAIN = \"ptb.train.txt\"\n",
    "VALID = \"ptb.valid.txt\"\n",
    "TEST = \"ptb.test.txt\"\n",
    "\n",
    "# A single recurrent layer of 2000 units\n",
    "#NUM_UNITS = 100\n",
    "# Size of the hidden state 64\n",
    "HIDDEN_SIZE = 64\n",
    "\n",
    "# A batch size of 100\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "# Sequences of length 200 bytes\n",
    "SEQ_LENGTH = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check/maybe download PTB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified /home/tkornuta/data/ptb/simple-examples.tgz ( 34869662 )\n"
     ]
    }
   ],
   "source": [
    "def maybe_download_ptb(path, \n",
    "                       filename='simple-examples.tgz', \n",
    "                       url='http://www.fit.vutbr.cz/~imikolov/rnnlm/', \n",
    "                       expected_bytes =34869662):\n",
    "  # Eventually create the PTB dir.\n",
    "  if not tf.gfile.Exists(path):\n",
    "    tf.gfile.MakeDirs(path)\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  _filename = path+filename\n",
    "  if not os.path.exists(_filename):\n",
    "    print('Downloading %s...' % filename)\n",
    "    _filename, _ = urlretrieve(url+filename, _filename)\n",
    "  statinfo = os.stat(_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', (_filename), '(', statinfo.st_size, ')')\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + _filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download_ptb(PTB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract dataset-related files from the PTB archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ptb(path, filename='simple-examples.tgz', files=[\"ptb.train.txt\", \"ptb.valid.txt\", \"ptb.test.txt\", \n",
    "                                       \"ptb.char.train.txt\", \"ptb.char.valid.txt\", \"ptb.char.test.txt\"]):\n",
    "    \"\"\"Extracts files from PTB archive.\"\"\"\n",
    "    # Extract\n",
    "    tar = tarfile.open(path+filename)\n",
    "    tar.extractall(path)\n",
    "    tar.close()\n",
    "    # Copy files\n",
    "    for file in files:\n",
    "        shutil.copyfile(PTB_DIR+\"simple-examples/data/\"+file, PTB_DIR+file)\n",
    "    # Delete directory\n",
    "    shutil.rmtree(PTB_DIR+\"simple-examples/\")        \n",
    "\n",
    "extract_ptb(PTB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train, valid and test texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5101618  aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memote\n",
      "399782  consumers may want to move their telephones a little closer to \n",
      "449945  no it was n't black monday \n",
      " but while the new york stock excha\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename, path):\n",
    "    with open(path+filename, 'r') as myfile:\n",
    "        data=myfile.read()# .replace('\\n', '')\n",
    "        return data\n",
    "\n",
    "train_text = read_data(TRAIN, PTB_DIR)\n",
    "train_size=len(train_text)\n",
    "print(train_size, train_text[:100])\n",
    "\n",
    "valid_text = read_data(VALID, PTB_DIR)\n",
    "valid_size=len(valid_text)\n",
    "print(valid_size, valid_text[:64])\n",
    "\n",
    "test_text = read_data(TEST, PTB_DIR)\n",
    "test_size=len(test_text)\n",
    "print(test_size, test_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size =  59\n",
      "65\n",
      "33 1 58 26 0 0\n",
      "a A\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 59 # [A-Z] + [a-z] + ' ' +few 'in between; + punctuation\n",
    "first_letter = ord(string.ascii_uppercase[0]) # ascii_uppercase before lowercase! \n",
    "print(\"vocabulary size = \", vocabulary_size)\n",
    "print(first_letter)\n",
    "\n",
    "def char2id(char):\n",
    "  \"\"\" Converts char to id (int) with one-hot encoding handling of unexpected characters\"\"\"\n",
    "  if char in string.ascii_letters:# or char in string.punctuation or char in string.digits:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    # print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  \"\"\" Converts single id (int) to character\"\"\"\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "#print(len(string.punctuation))\n",
    "#for i in string.ascii_letters:\n",
    "#    print (i, char2id(i))\n",
    "\n",
    "\n",
    "print(char2id('a'), char2id('A'), char2id('z'), char2id('Z'), char2id(' '), char2id('Ã¯'))\n",
    "print(id2char(char2id('a')), id2char(char2id('A')))\n",
    "#print(id2char(65), id2char(33), id2char(90), id2char(58), id2char(0))\n",
    "#bankno\n",
    "sample = np.zeros(shape=(1, vocabulary_size), dtype=np.float)\n",
    "sample[0, char2id(' ')] = 1.0\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper class for batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, seq_length, vocab_size):\n",
    "    \"\"\"\n",
    "    Initializes the batch generator object. Stores the variables and first \"letter batch\".\n",
    "    text is text to be processed\n",
    "    batch_size is size of batch (number of samples)\n",
    "    seq_length represents the length of sequence\n",
    "    vocab_size is number of words in vocabulary (assumes one-hot encoding)\n",
    "    \"\"\"\n",
    "    # Store input parameters.\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._seq_length = seq_length\n",
    "    self._vocab_size = vocab_size\n",
    "    # Divide text into segments depending on number of batches, each segment determines a cursor position for a batch.\n",
    "    segment = self._text_size // batch_size\n",
    "    # Set initial cursor position.\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    # Store first \"letter batch\".\n",
    "    self._last_letter_batch = self._next_letter_batch()\n",
    "  \n",
    "  def _next_letter_batch(self):\n",
    "    \"\"\"\n",
    "    Returns a batch containing of encoded single letters depending on the current batch \n",
    "    cursor positions in the data.\n",
    "    Returned \"letter batch\" is of size batch_size x vocab_size\n",
    "    \"\"\"\n",
    "    letter_batch = np.zeros(shape=(self._batch_size, self._vocab_size), dtype=np.float)\n",
    "    # Iterate through \"samples\"\n",
    "    for b in range(self._batch_size):\n",
    "      # Set 1 in position pointed out by one-hot char encoding.\n",
    "      letter_batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return letter_batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    # First add last letter from previous batch (the \"additional one\").\n",
    "    batches = [self._last_letter_batch]\n",
    "    for step in range(self._seq_length):\n",
    "      batches.append(self._next_letter_batch())\n",
    "    # Store last \"letter batch\" for next batch.\n",
    "    self._last_letter_batch = batches[-1]\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set =  aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memote\n",
      "['z calloway centrust c', ' league promises the ', 'ion including quotron', 'r proposal for a full', 'n china is very compl', ' december that was su', 'ogilvy group was  unk', 'lis a general electri', 've earlier this year ', 'f philip morris cos  ']\n"
     ]
    }
   ],
   "source": [
    "# Trick - override first 10 chars\n",
    "#list1 = list(train_text)\n",
    "#for i in range(2):\n",
    "#    list1[i] = 'z'\n",
    "#train_text = ''.join(list1)\n",
    "print(\"Test set =\", train_text[0:100])\n",
    "\n",
    "# Create two objects for training and validation batch generation.\n",
    "train_batches = BatchGenerator(train_text, BATCH_SIZE, SEQ_LENGTH, vocabulary_size)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1, vocabulary_size)\n",
    "\n",
    "batch = train_batches.next()\n",
    "batch = train_batches.next()\n",
    "#print(\"Batch = \", batch)\n",
    "print(batches2string(batch))\n",
    "#print(\"batch len = num of enrollings\",len(batch))\n",
    "#for i in range(num_unrollings):\n",
    "#    print(\"i = \", i, \"letter=\", batches2string(batch)[0][i][0], \"bits = \", batch[i][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for calculation of loss =  - log2 prob (BPC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"\n",
    "  Log-probability of the true labels in a predicted batch.\n",
    "  Assumes that predictions/labels are of shape [batch_size x 1] (i.e. a batch of 1-char sequences)\n",
    "  \"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  # Divide by the batch size (shape[0]).\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Definition of tensor graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Define input data buffers.\n",
    "  train_data = list()\n",
    "  for _ in range(SEQ_LENGTH + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[BATCH_SIZE,vocabulary_size]))\n",
    "  train_inputs = train_data[:SEQ_LENGTH]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Define parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, HIDDEN_SIZE]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, HIDDEN_SIZE]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, HIDDEN_SIZE]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, HIDDEN_SIZE], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, HIDDEN_SIZE]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([BATCH_SIZE, HIDDEN_SIZE]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([BATCH_SIZE, HIDDEN_SIZE]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "    # Fully connected layer on top => classification.\n",
    "    # In fact we will create lots of FC layers (one for each output layer), with shared weights.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    # Loss function(s) - one for every output generated by every lstm cell.\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer-related variables.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Greate graphs for sampling and validation evaluation: batch 1, \"no unrolling\".\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, HIDDEN_SIZE]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, HIDDEN_SIZE]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, HIDDEN_SIZE])),\n",
    "    saved_sample_state.assign(tf.zeros([1, HIDDEN_SIZE])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for language generation (letter sampling etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of iterations=  25508\n",
      "Initialized\n",
      "Average loss at step 0: 4.069476 learning rate: 10.000000\n",
      "Train set minibatch BPC: 4.07\n",
      "================================================================================\n",
      "GIbKo z  AsejRV[aojdwZhbkPV_a\\_AkLhvIwMBjBrrOWQeTJJr_cZntDH fU[ MaBp ejdAodUteqr\n",
      "DsawKioYa`dLFaa YOAOHzz_yre]ic rxcw`nnoKeUp wge klF_da es\\at_ ^[ez WoqOXeZGcnE c\n",
      "^t\\t\\ leiQsHVd_UbfnAm rTc ufq iEMnNR e e  s tyWmwVbzde n  k C K`Mtu[UzufwBM`iqTN\n",
      "wmo vIdW gduQsccR [uv Kors\\rR WoXZR  SWJ sspa`usD p\\nfIRB_etROs vYv CrfoIL`\\gG  \n",
      "y hrG tUDzNNC[HNw^JzosNeMpMaxLT`^KzSdQQe^c\\beA N DtojuLBio   zbHo[WEOAgIYoIV` ZV\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 3.44\n",
      "Average loss at step 100: 2.750074 learning rate: 10.000000\n",
      "Train set minibatch BPC: 2.66\n",
      "Validation set BPC (1-char): 2.42\n",
      "Average loss at step 200: 2.277129 learning rate: 10.000000\n",
      "Train set minibatch BPC: 2.24\n",
      "Validation set BPC (1-char): 2.18\n",
      "Average loss at step 300: 2.155371 learning rate: 10.000000\n",
      "Train set minibatch BPC: 2.27\n",
      "Validation set BPC (1-char): 2.07\n",
      "Average loss at step 400: 2.056683 learning rate: 10.000000\n",
      "Train set minibatch BPC: 2.28\n",
      "Validation set BPC (1-char): 2.07\n",
      "Average loss at step 500: 1.987632 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.93\n",
      "Validation set BPC (1-char): 2.03\n",
      "Average loss at step 600: 1.920607 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.98\n",
      "Validation set BPC (1-char): 2.08\n",
      "Average loss at step 700: 1.878192 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.75\n",
      "Validation set BPC (1-char): 1.77\n",
      "Average loss at step 800: 1.823616 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.86\n",
      "Validation set BPC (1-char): 1.88\n",
      "Average loss at step 900: 1.808947 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.80\n",
      "Validation set BPC (1-char): 1.98\n",
      "Average loss at step 1000: 1.766791 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.85\n",
      "================================================================================\n",
      "`ation N from whith  unk    the firm corpacaor m   unk  worBeds spited  unk  wo \n",
      "Say   elent its metors companate cating saye of more   on stock in N frimalion d\n",
      "Tal rewearying it ereed anveed tome whill hish the such eareces concers tracs wa\n",
      "Det million the he fucl if win the sales a for and n  no mespor in expaned co st\n",
      "Ialed a citer N Nearic and the soues marogist abrost sting recoungers morent dis\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.83\n",
      "Average loss at step 1100: 1.740312 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.85\n",
      "Validation set BPC (1-char): 1.88\n",
      "Average loss at step 1200: 1.712844 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.55\n",
      "Validation set BPC (1-char): 1.88\n",
      "Average loss at step 1300: 1.683738 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.72\n",
      "Validation set BPC (1-char): 1.77\n",
      "Average loss at step 1400: 1.698701 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.80\n",
      "Validation set BPC (1-char): 1.61\n",
      "Average loss at step 1500: 1.647528 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.71\n",
      "Validation set BPC (1-char): 1.61\n",
      "Average loss at step 1600: 1.649003 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.83\n",
      "Validation set BPC (1-char): 1.75\n",
      "Average loss at step 1700: 1.592555 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.43\n",
      "Validation set BPC (1-char): 1.60\n",
      "Average loss at step 1800: 1.613681 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.81\n",
      "Validation set BPC (1-char): 1.68\n",
      "Average loss at step 1900: 1.640100 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.71\n",
      "Validation set BPC (1-char): 1.62\n",
      "Average loss at step 2000: 1.696131 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.62\n",
      "================================================================================\n",
      "I shaselgial   cas  unk  a the stoughobs about mr  raperon  inm in stinct   if t\n",
      "adymancise has asuvilial reservation of the corp  into reparnment ration a the l\n",
      "as a monthic rrsublity into aroouse whaq interness the tograd from exasted addit\n",
      "]iss aalct of  unk  owbillionns an the   erveet wroser week sider winag off depa\n",
      "N in dreal gomist a sovied N miilion of with   news reses plain masion more next\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.65\n",
      "Average loss at step 2100: 1.637942 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.66\n",
      "Validation set BPC (1-char): 1.61\n",
      "Average loss at step 2200: 1.564127 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.39\n",
      "Validation set BPC (1-char): 1.62\n",
      "Average loss at step 2300: 1.590827 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.84\n",
      "Validation set BPC (1-char): 1.79\n",
      "Average loss at step 2400: 1.565091 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.82\n",
      "Validation set BPC (1-char): 1.50\n",
      "Average loss at step 2500: 1.559204 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.44\n",
      "Validation set BPC (1-char): 1.72\n",
      "Average loss at step 2600: 1.562353 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.48\n",
      "Validation set BPC (1-char): 1.62\n",
      "Average loss at step 2700: 1.542640 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.59\n",
      "Validation set BPC (1-char): 1.64\n",
      "Average loss at step 2800: 1.557216 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.55\n",
      "Validation set BPC (1-char): 1.43\n",
      "Average loss at step 2900: 1.546148 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.51\n",
      "Validation set BPC (1-char): 1.44\n",
      "Average loss at step 3000: 1.591284 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.52\n",
      "================================================================================\n",
      "rups as at the quast soorsiss to truak   the new york delang be the despritted i\n",
      "N has sales in a   N mritaition setital   kroburs menter advility or chip   relo\n",
      "Ean syntempive the a desse to to regationational marketser for yor  unk  ites co\n",
      "Hion from a reaf orgingts  unk  year of wo salist   dis accordion cato with of a\n",
      "Kistations   the gerter filit   relatias   N   mr   unk  extist augeffers to bec\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.56\n",
      "Average loss at step 3100: 1.569869 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.45\n",
      "Validation set BPC (1-char): 1.62\n",
      "Average loss at step 3200: 1.552757 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.33\n",
      "Validation set BPC (1-char): 1.63\n",
      "Average loss at step 3300: 1.516064 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.53\n",
      "Validation set BPC (1-char): 1.74\n",
      "Average loss at step 3400: 1.504527 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.31\n",
      "Validation set BPC (1-char): 1.64\n",
      "Average loss at step 3500: 1.592643 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.60\n",
      "Validation set BPC (1-char): 1.77\n",
      "Average loss at step 3600: 1.535088 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.45\n",
      "Validation set BPC (1-char): 1.47\n",
      "Average loss at step 3700: 1.545772 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.35\n",
      "Validation set BPC (1-char): 1.59\n",
      "Average loss at step 3800: 1.531675 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.49\n",
      "Validation set BPC (1-char): 1.59\n",
      "Average loss at step 3900: 1.514828 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.61\n",
      "Validation set BPC (1-char): 1.62\n",
      "Average loss at step 4000: 1.515830 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.57\n",
      "================================================================================\n",
      "y revent bsk their an a sualls for the ple wallih   chi hide on   the work tide \n",
      "` last posiliiotision lude in budge the the i chiese additiate that  unk  the un\n",
      "Jlily than company   the saler n t heach   mr  smon od themes last he   the is f\n",
      "ty avilical her  unk  ory his vilals from   puenting the ntway  unk  the bade th\n",
      "D maching a  unk  aid turkly   the  unk  they shared to said kud   sase of      \n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.66\n",
      "Average loss at step 4100: 1.575909 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.71\n",
      "Validation set BPC (1-char): 1.76\n",
      "Average loss at step 4200: 1.554953 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set BPC (1-char): 1.63\n",
      "Average loss at step 4300: 1.528672 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.72\n",
      "Validation set BPC (1-char): 1.56\n",
      "Average loss at step 4400: 1.499070 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.50\n",
      "Validation set BPC (1-char): 1.62\n",
      "Average loss at step 4500: 1.526216 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.45\n",
      "Validation set BPC (1-char): 1.74\n",
      "Average loss at step 4600: 1.526227 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.36\n",
      "Validation set BPC (1-char): 1.79\n",
      "Average loss at step 4700: 1.516036 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.47\n",
      "Validation set BPC (1-char): 1.91\n",
      "Average loss at step 4800: 1.506119 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.56\n",
      "Validation set BPC (1-char): 1.68\n",
      "Average loss at step 4900: 1.518493 learning rate: 10.000000\n",
      "Train set minibatch BPC: 1.69\n",
      "Validation set BPC (1-char): 1.84\n",
      "Average loss at step 5000: 1.471808 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.45\n",
      "================================================================================\n",
      "` a sher a quarqon prolds becade of shotchunt by the work from the wir frattlan \n",
      "st grouplubloch cax N N mr   unk  the computer exwerning had profected said N do\n",
      "ed tog why itery   N shared the quebetoring quebe the brooks generally goronn th\n",
      "Jand the year  unk  with case feencaned with seburation hangenangest mr  year ro\n",
      "t may  unk   unk  corps   john more  s  unk  too saleshing the a wn the N N of N\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.59\n",
      "Average loss at step 5100: 1.492998 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.42\n",
      "Validation set BPC (1-char): 1.77\n",
      "Average loss at step 5200: 1.498290 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.42\n",
      "Validation set BPC (1-char): 1.72\n",
      "Average loss at step 5300: 1.470781 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.42\n",
      "Validation set BPC (1-char): 1.46\n",
      "Average loss at step 5400: 1.459878 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.48\n",
      "Validation set BPC (1-char): 1.56\n",
      "Average loss at step 5500: 1.446067 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.29\n",
      "Validation set BPC (1-char): 1.50\n",
      "Average loss at step 5600: 1.505127 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.63\n",
      "Validation set BPC (1-char): 1.46\n",
      "Average loss at step 5700: 1.462292 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.53\n",
      "Validation set BPC (1-char): 1.57\n",
      "Average loss at step 5800: 1.476874 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.50\n",
      "Validation set BPC (1-char): 1.61\n",
      "Average loss at step 5900: 1.505113 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.53\n",
      "Validation set BPC (1-char): 1.52\n",
      "Average loss at step 6000: 1.512301 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.41\n",
      "================================================================================\n",
      "\\ers and here they federal nor it willion of the  unk    no   this hove equities\n",
      "jeble to ear incause of have up year   such action mirely packs  s a still by a \n",
      "B  unk  formeg of thmee or defeick or servipeds   as  unk  clution naturaable sh\n",
      "vared   charge with  unk  just indemademident chaiges a spekied real had n t hun\n",
      "_   N billion free an and moothingence to goversion to such a whith inagreute in\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.46\n",
      "Average loss at step 6100: 1.470343 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.68\n",
      "Validation set BPC (1-char): 1.48\n",
      "Average loss at step 6200: 1.452667 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.16\n",
      "Validation set BPC (1-char): 1.51\n",
      "Average loss at step 6300: 1.451292 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.74\n",
      "Validation set BPC (1-char): 1.47\n",
      "Average loss at step 6400: 1.453074 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.38\n",
      "Validation set BPC (1-char): 1.52\n",
      "Average loss at step 6500: 1.451688 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.52\n",
      "Validation set BPC (1-char): 1.44\n",
      "Average loss at step 6600: 1.462119 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.48\n",
      "Validation set BPC (1-char): 1.49\n",
      "Average loss at step 6700: 1.429925 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.51\n",
      "Validation set BPC (1-char): 1.48\n",
      "Average loss at step 6800: 1.428014 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.31\n",
      "Validation set BPC (1-char): 1.42\n",
      "Average loss at step 6900: 1.410885 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.37\n",
      "Validation set BPC (1-char): 1.33\n",
      "Average loss at step 7000: 1.403807 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.37\n",
      "================================================================================\n",
      "und remeasion earlier by edulility will corp  have by said moved comel belies of\n",
      "quate   the pay a start in prosident said american have price  unk  commence sec\n",
      "Wess longs and no offender apergicigies   interest as would be and those and sai\n",
      "D highretherly than senious comel handa will reyused chids gorturile in N N by a\n",
      "fle  jost   u t   limiling investment bourt weak  unk     unk    hil common  unk\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.42\n",
      "Average loss at step 7100: 1.440511 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.59\n",
      "Validation set BPC (1-char): 1.34\n",
      "Average loss at step 7200: 1.441655 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.39\n",
      "Validation set BPC (1-char): 1.36\n",
      "Average loss at step 7300: 1.446623 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.36\n",
      "Validation set BPC (1-char): 1.34\n",
      "Average loss at step 7400: 1.427293 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.48\n",
      "Validation set BPC (1-char): 1.32\n",
      "Average loss at step 7500: 1.399923 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.40\n",
      "Validation set BPC (1-char): 1.31\n",
      "Average loss at step 7600: 1.444786 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.43\n",
      "Validation set BPC (1-char): 1.38\n",
      "Average loss at step 7700: 1.412414 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.33\n",
      "Validation set BPC (1-char): 1.49\n",
      "Average loss at step 7800: 1.387851 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.28\n",
      "Validation set BPC (1-char): 1.43\n",
      "Average loss at step 7900: 1.391534 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.54\n",
      "Validation set BPC (1-char): 1.41\n",
      "Average loss at step 8000: 1.411294 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.57\n",
      "================================================================================\n",
      "U instrcime columa voinfly companar  unk   unk  that bening ment of N  unk  inte\n",
      "Fetical car phinas underveration current of   N million traarledain tran account\n",
      "y the precerce fruea to the jumyporenuble will chibleds   a company expects deal\n",
      "zian   open  unk  canada colboter  unk  developmency of bess junk capical tow of\n",
      "_erves drige has a but  unk  also  s chamber   quire if umded ad sider inc  abou\n",
      "================================================================================\n",
      "Validation set BPC (1-char): 1.43\n",
      "Average loss at step 8100: 1.415023 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.48\n",
      "Validation set BPC (1-char): 1.38\n",
      "Average loss at step 8200: 1.393496 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.34\n",
      "Validation set BPC (1-char): 1.57\n",
      "Average loss at step 8300: 1.404979 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.43\n",
      "Validation set BPC (1-char): 1.56\n",
      "Average loss at step 8400: 1.438120 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.44\n",
      "Validation set BPC (1-char): 1.33\n",
      "Average loss at step 8500: 1.405593 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.37\n",
      "Validation set BPC (1-char): 1.43\n",
      "Average loss at step 8600: 1.440421 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.52\n",
      "Validation set BPC (1-char): 1.45\n",
      "Average loss at step 8700: 1.437881 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.30\n",
      "Validation set BPC (1-char): 1.37\n",
      "Average loss at step 8800: 1.391123 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.33\n",
      "Validation set BPC (1-char): 1.39\n",
      "Average loss at step 8900: 1.414711 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.25\n",
      "Validation set BPC (1-char): 1.34\n",
      "Average loss at step 9000: 1.392217 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.47\n",
      "================================================================================\n",
      "cy avo to post belotation rexott exchange  unk  diffination to assetoce of rated\n",
      "s buy recutical that the massury was strlhum on the called   miac by calif europ\n",
      "Vising of a rige an ownance the mainting farrition traders them  s stock divisio\n",
      "at it orking topers in frating   with consuystaved the mountiliswing major   the\n",
      "king has the officy  unk   unk   unk   unk  laviannices in is play more all had \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set BPC (1-char): 1.29\n",
      "Average loss at step 9100: 1.400687 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.67\n",
      "Validation set BPC (1-char): 1.23\n",
      "Average loss at step 9200: 1.447746 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.48\n",
      "Validation set BPC (1-char): 1.31\n",
      "Average loss at step 9300: 1.423694 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.61\n",
      "Validation set BPC (1-char): 1.49\n",
      "Average loss at step 9400: 1.443195 learning rate: 1.000000\n",
      "Train set minibatch BPC: 1.40\n",
      "Validation set BPC (1-char): 1.38\n"
     ]
    }
   ],
   "source": [
    "num_steps =  train_size // (BATCH_SIZE*SEQ_LENGTH) #70001\n",
    "print(\"Total number of iterations= \", num_steps)\n",
    "summary_frequency = 100\n",
    "    \n",
    "with tf.Session(graph=graph) as session:\n",
    "\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "    \n",
    "  # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "  merged = tf.summary.merge_all()\n",
    "  train_writer = tf.summary.FileWriter(LOG_DIR + '/train', session.graph)\n",
    "  valid_writer = tf.summary.FileWriter(LOG_DIR + '/valid')\n",
    "    \n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(SEQ_LENGTH + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    # Add loss to mean.\n",
    "    mean_loss += l\n",
    "    # Every (100) steps collect statistics.\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Train set minibatch BPC: %.2f' % logprob(predictions, labels))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set BPC.\n",
    "      reset_sample_state.run()\n",
    "      mean_valid_logprob = 0\n",
    "      # Sum for a single batch of size 1 - i.e. predict depending only on a single input character.\n",
    "      for _ in range(1000): #valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        mean_valid_logprob += logprob(predictions, b[1])\n",
    "      print('Validation set BPC (1-char): %.2f' % float(mean_valid_logprob / (1000))) #valid_size)))\n",
    "    # End of statistics collection\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
