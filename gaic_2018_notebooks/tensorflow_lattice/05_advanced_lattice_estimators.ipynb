{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Lattice estimators\n",
    "In this tutorial, we will TensorFlow Lattice estimators.\n",
    "The more detailed version of this notebook can be found in\n",
    "https://github.com/tensorflow/lattice/blob/master/g3doc/tutorial/index.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_lattice as tfl\n",
    "import tempfile\n",
    "#import urllib\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data is downloaded to /tmp/tmplaao4fkt\n",
      "Test data is downloaded to /tmp/tmpomu_76ao\n"
     ]
    }
   ],
   "source": [
    "def download_if_not_exists(train_data, test_data):\n",
    "    \"\"\"Maybe downloads training data and returns train and test file names.\"\"\"\n",
    "    train_file_name = train_data\n",
    "    if not os.path.exists(train_file_name):\n",
    "        train_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "        urlretrieve(\n",
    "            \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n",
    "            train_file.name)  # pylint: disable=line-too-long\n",
    "        train_file_name = train_file.name\n",
    "        train_file.close()\n",
    "        print(\"Training data is downloaded to %s\" % train_file_name)\n",
    "\n",
    "    test_file_name = test_data\n",
    "    if not os.path.exists(test_file_name):\n",
    "        test_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "        urlretrieve(\n",
    "            \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\",\n",
    "            test_file.name)  # pylint: disable=line-too-long\n",
    "        test_file_name = test_file.name\n",
    "        test_file.close()\n",
    "        print(\"Test data is downloaded to %s\"% test_file_name)\n",
    "    \n",
    "    return (train_file_name, test_file_name)\n",
    "\n",
    "# Specify the dataset\n",
    "(TRAIN_DATA, TEST_DATA) = download_if_not_exists(\"/tmp/tfl-data/adult.data\",\n",
    "                                                 \"/tmp/tfl-data/adult.test\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CSV_COLUMNS = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
    "    \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n",
    "    \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n",
    "    \"income_bracket\"\n",
    "]\n",
    "\n",
    "def get_input_fn(file_path, batch_size, num_epochs, shuffle):\n",
    "    df_data = pd.read_csv(\n",
    "        tf.gfile.Open(file_path),\n",
    "        names=CSV_COLUMNS,\n",
    "        skipinitialspace=True,\n",
    "        engine=\"python\",\n",
    "        skiprows=1)\n",
    "    # Drop missing for the time being.\n",
    "    df_data = df_data.dropna(how=\"any\", axis=0)\n",
    "    labels = df_data[\"income_bracket\"].apply(lambda x: \">50K\" in x).astype(int)\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x=df_data,\n",
    "        y=labels,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        shuffle=shuffle,\n",
    "        num_threads=5)\n",
    "\n",
    "def get_train_input_fn(batch_size, num_epochs=1, shuffle=False):\n",
    "    train_data = TRAIN_DATA\n",
    "    return get_input_fn(train_data, batch_size, num_epochs, shuffle)\n",
    "\n",
    "\n",
    "def densify(fc, make_dense):\n",
    "    if not make_dense:\n",
    "        return fc\n",
    "    return tf.feature_column.embedding_column(fc, 4)\n",
    "\n",
    "\n",
    "def get_feature_columns(make_dense=False):\n",
    "    # Categorical features.\n",
    "    gender = densify(\n",
    "        tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "            \"gender\", [\"Female\", \"Male\"]), make_dense)\n",
    "    education = densify(\n",
    "        tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "            \"education\", [\n",
    "                \"Bachelors\", \"HS-grad\", \"11th\", \"Masters\", \"9th\", \"Some-college\",\n",
    "                \"Assoc-acdm\", \"Assoc-voc\", \"7th-8th\", \"Doctorate\", \"Prof-school\",\n",
    "                \"5th-6th\", \"10th\", \"1st-4th\", \"Preschool\", \"12th\"\n",
    "            ]), make_dense)\n",
    "    marital_status = densify(\n",
    "        tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "            \"marital_status\", [\n",
    "                \"Married-civ-spouse\", \"Divorced\", \"Married-spouse-absent\",\n",
    "                \"Never-married\", \"Separated\", \"Married-AF-spouse\", \"Widowed\"\n",
    "            ]), make_dense)\n",
    "    relationship = densify(\n",
    "        tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "            \"relationship\", [\n",
    "                \"Husband\", \"Not-in-family\", \"Wife\", \"Own-child\", \"Unmarried\",\n",
    "                \"Other-relative\"\n",
    "            ]), make_dense)\n",
    "    workclass = densify(\n",
    "        tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "            \"workclass\", [\n",
    "                \"Self-emp-not-inc\", \"Private\", \"State-gov\", \"Federal-gov\",\n",
    "                \"Local-gov\", \"?\", \"Self-emp-inc\", \"Without-pay\", \"Never-worked\"\n",
    "            ]), make_dense)\n",
    "\n",
    "    # To show an example of hashing:\n",
    "    occupation = densify(\n",
    "        tf.feature_column.categorical_column_with_hash_bucket(\n",
    "            \"occupation\", hash_bucket_size=1000), make_dense)\n",
    "    native_country = densify(\n",
    "        tf.feature_column.categorical_column_with_hash_bucket(\n",
    "            \"native_country\", hash_bucket_size=1000), make_dense)\n",
    "\n",
    "    # Continuous base columns.\n",
    "    age = tf.feature_column.numeric_column(\"age\")\n",
    "    education_num = tf.feature_column.numeric_column(\"education_num\")\n",
    "    capital_gain = tf.feature_column.numeric_column(\"capital_gain\")\n",
    "    capital_loss = tf.feature_column.numeric_column(\"capital_loss\")\n",
    "    hours_per_week = tf.feature_column.numeric_column(\"hours_per_week\")\n",
    "    \n",
    "    return [\n",
    "        age,\n",
    "        education_num,\n",
    "        capital_gain,\n",
    "        capital_loss,\n",
    "        hours_per_week,\n",
    "        gender,\n",
    "        education,\n",
    "        marital_status,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a histogram\n",
    "This information will be used to initialize the calibrator input keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:enqueue_data was called with num_epochs and num_threads > 1. num_epochs is applied per thread, so this will produce more epochs than you probably intend. If you want to limit epochs, use one thread.\n",
      "WARNING:tensorflow:enqueue_data was called with shuffle=False and num_threads > 1. This will create multiple threads, all reading the array/dataframe in order. If you want examples read in order, use one thread; if you want multiple threads, enable shuffling.\n"
     ]
    }
   ],
   "source": [
    "quantiles_dir = tempfile.mkdtemp()\n",
    "\n",
    "def create_histogram(quantiles_dir):\n",
    "    input_fn = get_train_input_fn(batch_size=10000, num_epochs=1, shuffle=False)\n",
    "    tfl.save_quantiles_for_keypoints(\n",
    "        input_fn=input_fn,\n",
    "        save_dir=quantiles_dir,\n",
    "        feature_columns=get_feature_columns(make_dense=False),\n",
    "        num_steps=10)\n",
    "    \n",
    "create_histogram(quantiles_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _build_linear_estimator(model_dir, feature_columns, learning_rate):\n",
    "    \"\"\"Build linear estimator.\"\"\"\n",
    "    feature_names = [fc.name for fc in feature_columns]\n",
    "    hparams = tfl.CalibratedLinearHParams(\n",
    "        feature_names=feature_names,\n",
    "        learning_rate=learning_rate,\n",
    "        num_keypoints=20)\n",
    "\n",
    "    m = tfl.calibrated_linear_classifier(\n",
    "        model_dir=model_dir,\n",
    "        quantiles_dir=quantiles_dir,\n",
    "        feature_columns=feature_columns,\n",
    "        hparams=hparams)\n",
    "    return m\n",
    "\n",
    "def _build_rtl_estimator(model_dir, feature_columns, learning_rate):\n",
    "    \"\"\"Build rtl estimator.\"\"\"\n",
    "    feature_names = [fc.name for fc in feature_columns]\n",
    "    # Create 100 number of 2 x 2 x 2 x 2 lattices.\n",
    "    hparams = tfl.CalibratedRtlHParams(\n",
    "        feature_names=feature_names,\n",
    "        learning_rate=learning_rate,\n",
    "        lattice_rank=4,\n",
    "        num_lattices=100,\n",
    "        num_keypoints=20)\n",
    "    \n",
    "    m = tfl.calibrated_rtl_classifier(\n",
    "        model_dir=model_dir,\n",
    "        quantiles_dir=quantiles_dir,\n",
    "        feature_columns=feature_columns,\n",
    "        hparams=hparams)\n",
    "    return m\n",
    "\n",
    "def build_estimator(model_dir, learning_rate, model_type='rtl'):\n",
    "    \"\"\"Build an estimator.\"\"\"\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        tf.gfile.MkDir(model_dir)\n",
    "    feature_columns = get_feature_columns(make_dense=False)\n",
    "    if model_type == 'rtl':\n",
    "        return _build_rtl_estimator(model_dir, feature_columns, learning_rate)\n",
    "    elif model_type == 'linear':\n",
    "        return _build_linear_estimator(model_dir, feature_columns, learning_rate)\n",
    "    else:\n",
    "        raise ValueError('unsupported model type: %s' % model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/tmpoo1em44h', '_task_id': 0, '_keep_checkpoint_max': 5, '_save_checkpoints_secs': 600, '_task_type': 'worker', '_service': None, '_save_checkpoints_steps': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f42be948ef0>, '_is_chief': True, '_num_worker_replicas': 1, '_master': '', '_session_config': None, '_save_summary_steps': 100, '_num_ps_replicas': 0, '_log_step_count_steps': 100, '_tf_random_seed': None}\n",
      "WARNING:tensorflow:enqueue_data was called with num_epochs and num_threads > 1. num_epochs is applied per thread, so this will produce more epochs than you probably intend. If you want to limit epochs, use one thread.\n",
      "WARNING:tensorflow:enqueue_data was called with shuffle=True, num_threads > 1, and num_epochs. This will create multiple threads, all reading the array/dataframe in order adding to the same shuffling queue; the results will likely not be sufficiently shuffled.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpoo1em44h/model.ckpt.\n",
      "INFO:tensorflow:loss = 48.070786, step = 1\n",
      "INFO:tensorflow:global_step/sec: 207.008\n",
      "INFO:tensorflow:loss = 29.85246, step = 101 (0.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.638\n",
      "INFO:tensorflow:loss = 31.883923, step = 201 (0.429 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.962\n",
      "INFO:tensorflow:loss = 29.580906, step = 301 (0.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.886\n",
      "INFO:tensorflow:loss = 38.947212, step = 401 (0.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.706\n",
      "INFO:tensorflow:loss = 29.207964, step = 501 (0.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.992\n",
      "INFO:tensorflow:loss = 26.561161, step = 601 (0.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.956\n",
      "INFO:tensorflow:loss = 29.027798, step = 701 (0.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.378\n",
      "INFO:tensorflow:loss = 26.409687, step = 801 (0.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 238.057\n",
      "INFO:tensorflow:loss = 25.840906, step = 901 (0.420 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.901\n",
      "INFO:tensorflow:loss = 29.280529, step = 1001 (0.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.627\n",
      "INFO:tensorflow:loss = 26.60662, step = 1101 (0.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.184\n",
      "INFO:tensorflow:loss = 28.879429, step = 1201 (0.410 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.567\n",
      "INFO:tensorflow:loss = 28.285454, step = 1301 (0.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.38\n",
      "INFO:tensorflow:loss = 25.619736, step = 1401 (0.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.215\n",
      "INFO:tensorflow:loss = 25.025127, step = 1501 (0.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.614\n",
      "INFO:tensorflow:loss = 25.021313, step = 1601 (0.428 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1628 into /tmp/tmpoo1em44h/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 32.15826.\n",
      "=====Training set=====\n",
      "WARNING:tensorflow:enqueue_data was called with num_epochs and num_threads > 1. num_epochs is applied per thread, so this will produce more epochs than you probably intend. If you want to limit epochs, use one thread.\n",
      "WARNING:tensorflow:enqueue_data was called with shuffle=False and num_threads > 1. This will create multiple threads, all reading the array/dataframe in order. If you want examples read in order, use one thread; if you want multiple threads, enable shuffling.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-19-01:13:15\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo1em44h/model.ckpt-1628\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-19-01:13:23\n",
      "INFO:tensorflow:Saving dict for global step 1628: accuracy = 0.85737103, accuracy_baseline = 0.75918305, auc = 0.9141835, auc_precision_recall = 0.79565287, average_loss = 0.30250782, global_step = 1628, label/mean = 0.24081695, loss = 30.250782, prediction/mean = 0.23982407\n",
      "accuracy: 0.85737103\n",
      "accuracy_baseline: 0.75918305\n",
      "auc: 0.9141835\n",
      "auc_precision_recall: 0.79565287\n",
      "average_loss: 0.30250782\n",
      "global_step: 1628\n",
      "label/mean: 0.24081695\n",
      "loss: 30.250782\n",
      "prediction/mean: 0.23982407\n"
     ]
    }
   ],
   "source": [
    "model_dir = tempfile.mkdtemp()\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "\n",
    "m = build_estimator(model_dir, learning_rate, model_type='linear')\n",
    "m.train(input_fn=get_train_input_fn(batch_size=100, num_epochs=1, shuffle=True))\n",
    "\n",
    "print('=====Training set=====')\n",
    "results = m.evaluate(input_fn=get_train_input_fn(batch_size=batch_size))\n",
    "for key in sorted(results):\n",
    "    print('%s: %s' % (key, results[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RTL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/tmpzlxizk_c', '_task_id': 0, '_keep_checkpoint_max': 5, '_save_checkpoints_secs': 600, '_task_type': 'worker', '_service': None, '_save_checkpoints_steps': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f42bec8cf28>, '_is_chief': True, '_num_worker_replicas': 1, '_master': '', '_session_config': None, '_save_summary_steps': 100, '_num_ps_replicas': 0, '_log_step_count_steps': 100, '_tf_random_seed': None}\n",
      "WARNING:tensorflow:enqueue_data was called with num_epochs and num_threads > 1. num_epochs is applied per thread, so this will produce more epochs than you probably intend. If you want to limit epochs, use one thread.\n",
      "WARNING:tensorflow:enqueue_data was called with shuffle=True, num_threads > 1, and num_epochs. This will create multiple threads, all reading the array/dataframe in order adding to the same shuffling queue; the results will likely not be sufficiently shuffled.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpzlxizk_c/model.ckpt.\n",
      "INFO:tensorflow:loss = 69.31474, step = 1\n",
      "INFO:tensorflow:global_step/sec: 42.3544\n",
      "INFO:tensorflow:loss = 36.53876, step = 101 (2.362 sec)\n",
      "INFO:tensorflow:global_step/sec: 54.2894\n",
      "INFO:tensorflow:loss = 45.341515, step = 201 (1.842 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.8215\n",
      "INFO:tensorflow:loss = 35.09292, step = 301 (1.432 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.7409\n",
      "INFO:tensorflow:loss = 37.810112, step = 401 (1.455 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.6707\n",
      "INFO:tensorflow:loss = 32.21786, step = 501 (1.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.5909\n",
      "INFO:tensorflow:loss = 36.674644, step = 601 (1.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.8085\n",
      "INFO:tensorflow:loss = 33.75888, step = 701 (1.453 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.1466\n",
      "INFO:tensorflow:loss = 34.680466, step = 801 (1.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.2054\n",
      "INFO:tensorflow:loss = 39.45854, step = 901 (1.488 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.9737\n",
      "INFO:tensorflow:loss = 33.57366, step = 1001 (1.471 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.7746\n",
      "INFO:tensorflow:loss = 38.014687, step = 1101 (1.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.5587\n",
      "INFO:tensorflow:loss = 26.299692, step = 1201 (1.480 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.8839\n",
      "INFO:tensorflow:loss = 35.395386, step = 1301 (1.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.3511\n",
      "INFO:tensorflow:loss = 32.981606, step = 1401 (1.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.6483\n",
      "INFO:tensorflow:loss = 41.206738, step = 1501 (1.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.545\n",
      "INFO:tensorflow:loss = 32.881874, step = 1601 (1.625 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1628 into /tmp/tmpzlxizk_c/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 36.00863.\n",
      "=====Training set=====\n",
      "WARNING:tensorflow:enqueue_data was called with num_epochs and num_threads > 1. num_epochs is applied per thread, so this will produce more epochs than you probably intend. If you want to limit epochs, use one thread.\n",
      "WARNING:tensorflow:enqueue_data was called with shuffle=False and num_threads > 1. This will create multiple threads, all reading the array/dataframe in order. If you want examples read in order, use one thread; if you want multiple threads, enable shuffling.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-19-01:14:00\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpzlxizk_c/model.ckpt-1628\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-19-01:14:13\n",
      "INFO:tensorflow:Saving dict for global step 1628: accuracy = 0.85316336, accuracy_baseline = 0.75918305, auc = 0.9064318, auc_precision_recall = 0.78465253, average_loss = 0.31743434, global_step = 1628, label/mean = 0.24081695, loss = 31.743435, prediction/mean = 0.22160159\n",
      "accuracy: 0.85316336\n",
      "accuracy_baseline: 0.75918305\n",
      "auc: 0.9064318\n",
      "auc_precision_recall: 0.78465253\n",
      "average_loss: 0.31743434\n",
      "global_step: 1628\n",
      "label/mean: 0.24081695\n",
      "loss: 31.743435\n",
      "prediction/mean: 0.22160159\n"
     ]
    }
   ],
   "source": [
    "model_dir = tempfile.mkdtemp()\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "\n",
    "m = build_estimator(model_dir, learning_rate, model_type='rtl')\n",
    "m.train(input_fn=get_train_input_fn(batch_size=100, num_epochs=1, shuffle=True))\n",
    "\n",
    "print('=====Training set=====')\n",
    "results = m.evaluate(input_fn=get_train_input_fn(batch_size=batch_size))\n",
    "for key in sorted(results):\n",
    "    print('%s: %s' % (key, results[key]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
