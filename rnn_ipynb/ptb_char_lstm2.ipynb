{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import tarfile\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import shutil \n",
    "import random\n",
    "\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "# Dirs - must be absolute paths!\n",
    "LOG_DIR = '/tmp/tf/ptb_char_lstm2/hidden64_batch100_seq10/'\n",
    "# Local dir where PTB files will be stored.\n",
    "PTB_DIR = '/home/tkornuta/data/ptb/'\n",
    "\n",
    "# Filenames.\n",
    "TRAIN = \"ptb.train.txt\"\n",
    "VALID = \"ptb.valid.txt\"\n",
    "TEST = \"ptb.test.txt\"\n",
    "\n",
    "# Size of the hidden state.\n",
    "HIDDEN_SIZE = 64\n",
    "\n",
    "# Batch size.\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# A single recurrent layer of number of units = sequences of length\n",
    "# e.g. 200 bytes\n",
    "SEQ_LENGTH = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check/maybe download PTB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified /home/tkornuta/data/ptb/simple-examples.tgz ( 34869662 )\n"
     ]
    }
   ],
   "source": [
    "def maybe_download_ptb(path, \n",
    "                       filename='simple-examples.tgz', \n",
    "                       url='http://www.fit.vutbr.cz/~imikolov/rnnlm/', \n",
    "                       expected_bytes =34869662):\n",
    "  # Eventually create the PTB dir.\n",
    "  if not tf.gfile.Exists(path):\n",
    "    tf.gfile.MakeDirs(path)\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  _filename = path+filename\n",
    "  if not os.path.exists(_filename):\n",
    "    print('Downloading %s...' % filename)\n",
    "    _filename, _ = urlretrieve(url+filename, _filename)\n",
    "  statinfo = os.stat(_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', (_filename), '(', statinfo.st_size, ')')\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + _filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download_ptb(PTB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract dataset-related files from the PTB archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_ptb(path, filename='simple-examples.tgz', files=[\"ptb.train.txt\", \"ptb.valid.txt\", \"ptb.test.txt\", \n",
    "                                       \"ptb.char.train.txt\", \"ptb.char.valid.txt\", \"ptb.char.test.txt\"]):\n",
    "    \"\"\"Extracts files from PTB archive.\"\"\"\n",
    "    # Extract\n",
    "    tar = tarfile.open(path+filename)\n",
    "    tar.extractall(path)\n",
    "    tar.close()\n",
    "    # Copy files\n",
    "    for file in files:\n",
    "        shutil.copyfile(PTB_DIR+\"simple-examples/data/\"+file, PTB_DIR+file)\n",
    "    # Delete directory\n",
    "    shutil.rmtree(PTB_DIR+\"simple-examples/\")        \n",
    "\n",
    "extract_ptb(PTB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train, valid and test texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5101618  aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memote\n",
      "399782  consumers may want to move their telephones a little closer to \n",
      "449945  no it was n't black monday \n",
      " but while the new york stock excha\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename, path):\n",
    "    with open(path+filename, 'r') as myfile:\n",
    "        data=myfile.read()# .replace('\\n', '')\n",
    "        return data\n",
    "\n",
    "train_text = read_data(TRAIN, PTB_DIR)\n",
    "train_size=len(train_text)\n",
    "print(train_size, train_text[:100])\n",
    "\n",
    "valid_text = read_data(VALID, PTB_DIR)\n",
    "valid_size=len(valid_text)\n",
    "print(valid_size, valid_text[:64])\n",
    "\n",
    "test_text = read_data(TEST, PTB_DIR)\n",
    "test_size=len(test_text)\n",
    "print(test_size, test_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size =  59\n",
      "65\n",
      "33 1 58 26 0 0\n",
      "a A\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 59 # [A-Z] + [a-z] + ' ' +few 'in between; + punctuation\n",
    "first_letter = ord(string.ascii_uppercase[0]) # ascii_uppercase before lowercase! \n",
    "print(\"vocabulary size = \", vocabulary_size)\n",
    "print(first_letter)\n",
    "\n",
    "def char2id(char):\n",
    "  \"\"\" Converts char to id (int) with one-hot encoding handling of unexpected characters\"\"\"\n",
    "  if char in string.ascii_letters:# or char in string.punctuation or char in string.digits:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    # print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  \"\"\" Converts single id (int) to character\"\"\"\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "#print(len(string.punctuation))\n",
    "#for i in string.ascii_letters:\n",
    "#    print (i, char2id(i))\n",
    "\n",
    "\n",
    "print(char2id('a'), char2id('A'), char2id('z'), char2id('Z'), char2id(' '), char2id('Ã¯'))\n",
    "print(id2char(char2id('a')), id2char(char2id('A')))\n",
    "#print(id2char(65), id2char(33), id2char(90), id2char(58), id2char(0))\n",
    "#bankno\n",
    "sample = np.zeros(shape=(1, vocabulary_size), dtype=np.float)\n",
    "sample[0, char2id(' ')] = 1.0\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper class for batch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, seq_length, vocab_size):\n",
    "    \"\"\"\n",
    "    Initializes the batch generator object. Stores the variables and first \"letter batch\".\n",
    "    text is text to be processed\n",
    "    batch_size is size of batch (number of samples)\n",
    "    seq_length represents the length of sequence\n",
    "    vocab_size is number of words in vocabulary (assumes one-hot encoding)\n",
    "    \"\"\"\n",
    "    # Store input parameters.\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._seq_length = seq_length\n",
    "    self._vocab_size = vocab_size\n",
    "    # Divide text into segments depending on number of batches, each segment determines a cursor position for a batch.\n",
    "    segment = self._text_size // batch_size\n",
    "    # Set initial cursor position.\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    # Store first \"letter batch\".\n",
    "    self._last_letter_batch = self._next_letter_batch()\n",
    "  \n",
    "  def _next_letter_batch(self):\n",
    "    \"\"\"\n",
    "    Returns a batch containing of encoded single letters depending on the current batch \n",
    "    cursor positions in the data.\n",
    "    Returned \"letter batch\" is of size batch_size x vocab_size\n",
    "    \"\"\"\n",
    "    letter_batch = np.zeros(shape=(self._batch_size, self._vocab_size), dtype=np.float)\n",
    "    # Iterate through \"samples\"\n",
    "    for b in range(self._batch_size):\n",
    "      # Set 1 in position pointed out by one-hot char encoding.\n",
    "      letter_batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return letter_batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    # First add last letter from previous batch (the \"additional one\").\n",
    "    batches = [self._last_letter_batch]\n",
    "    for step in range(self._seq_length):\n",
    "      batches.append(self._next_letter_batch())\n",
    "    # Store last \"letter batch\" for next batch.\n",
    "    self._last_letter_batch = batches[-1]\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "(100, 59)\n"
     ]
    }
   ],
   "source": [
    "# Trick - override first 10 chars\n",
    "#list1 = list(train_text)\n",
    "#for i in range(2):\n",
    "#    list1[i] = 'z'\n",
    "#train_text = ''.join(list1)\n",
    "#print(\"Train set =\", train_text[0:100])\n",
    "\n",
    "# Create objects for training, validation and testing batch generation.\n",
    "train_batches = BatchGenerator(train_text, BATCH_SIZE, SEQ_LENGTH, vocabulary_size)\n",
    "\n",
    "# Get first training batch.\n",
    "batch = train_batches.next()\n",
    "print(len(batch))\n",
    "print(batch[0].shape)\n",
    "#print(\"Batch = \", batch)\n",
    "#print(batches2string(batch))\n",
    "#print(\"batch len = num of enrollings\",len(batch))\n",
    "#for i in range(num_unrollings):\n",
    "#    print(\"i = \", i, \"letter=\", batches2string(batch)[0][i][0], \"bits = \", batch[i][0])\n",
    "\n",
    "\n",
    "# For validation  - process the whole text as one big batch.\n",
    "VALID_BATCH_SIZE = int(np.floor(valid_size/SEQ_LENGTH))\n",
    "valid_batches = BatchGenerator(valid_text, VALID_BATCH_SIZE, SEQ_LENGTH, vocabulary_size)\n",
    "valid_batch = valid_batches.next()\n",
    "#print (VALID_BATCH_SIZE)\n",
    "#print(len(valid_batch))\n",
    "#print(valid_batch[0].shape)\n",
    "\n",
    "# For texting  - process the whole text as one big batch.\n",
    "TEST_BATCH_SIZE = int(np.floor(test_size/SEQ_LENGTH))\n",
    "test_batches = BatchGenerator(test_text, TEST_BATCH_SIZE, SEQ_LENGTH, vocabulary_size)\n",
    "# Get single batch! \n",
    "test_batch = test_batches.next()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function defining the LSTM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(input_, prev_output_, prev_cell_state_, name_):\n",
    "    \"\"\"Create a LSTM cell\"\"\"\n",
    "    with tf.name_scope(name_):\n",
    "        # Equations according to C. Olah blog.\n",
    "        \n",
    "        # Concatenate h_prev (\"prev output\") with x.\n",
    "        h_prev_x = tf.concat([prev_output_, input_], 1)\n",
    "        \n",
    "        # Calculate forget and input gates activations.\n",
    "        forget_gate = tf.sigmoid(tf.matmul(h_prev_x, Wf) + bf, name=\"forget_gate\")\n",
    "        input_gate = tf.sigmoid(tf.matmul(h_prev_x, Wi) + bi, name=\"Input_gate\")\n",
    "\n",
    "        # Update of the cell state C~.\n",
    "        cell_update = tf.tanh(tf.matmul(h_prev_x, Wc) + bc, name=\"Cell_update\")\n",
    "        # New cell state C.\n",
    "        cell_state = tf.add(forget_gate * prev_cell_state_, input_gate * cell_update, name = \"Cell_state\")\n",
    "        \n",
    "        # Calculate output gate.\n",
    "        output_gate = tf.sigmoid(tf.matmul(h_prev_x, Wo) + bo, name=\"Output_gate\")\n",
    "        # Calculate h - \"output\".\n",
    "        output = output_gate * tf.tanh(cell_state)\n",
    "\n",
    "        return output, cell_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Definition of tensor graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_buffer shape = <unknown>\n",
      "Seq length  = 10\n",
      "Batch shape = <unknown>\n",
      "10\n",
      "<unknown>\n",
      "<unknown>\n"
     ]
    }
   ],
   "source": [
    "# Reset graph - just in case.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 0. Shared variables ops.\n",
    "with tf.name_scope(\"Shared_Variables\"):\n",
    "  # Define parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  Wf = tf.Variable(tf.truncated_normal([vocabulary_size+HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"Wf\")\n",
    "  bf = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"bf\")\n",
    "\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  Wi = tf.Variable(tf.truncated_normal([vocabulary_size+HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"Wi\")\n",
    "  bi = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"bi\")\n",
    "\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  Wc = tf.Variable(tf.truncated_normal([vocabulary_size+HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"Wc\")\n",
    "  bc = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"bc\")\n",
    "\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  Wo = tf.Variable(tf.truncated_normal([vocabulary_size+HIDDEN_SIZE, HIDDEN_SIZE], -0.1, 0.1), name=\"Wo\")\n",
    "  bo = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"bo\")\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, vocabulary_size], -0.1, 0.1), name=\"w\")\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]), name=\"b\")\n",
    "\n",
    "  # Placeholders for previous (the oldest) state and output.\n",
    "  prev_output = tf.placeholder(tf.float32, shape=None, name=\"prev_output\")\n",
    "  prev_state = tf.placeholder(tf.float32, shape=None, name=\"prev_state\")\n",
    "\n",
    "# 0. Placeholders for inputs.\n",
    "with tf.name_scope(\"Input_data\"):\n",
    "  # Define input data buffers.\n",
    "  input_buffer = list()\n",
    "  for _ in range(SEQ_LENGTH + 1):\n",
    "    # Collect placeholders for inputs/labels.\n",
    "    input_buffer.append(tf.placeholder(tf.float32, shape=None, name=\"Input_data\"))\n",
    "  print (\"input_buffer shape =\", input_buffer[0].shape)\n",
    "  # Collection of training inputs.\n",
    "  train_inputs = input_buffer[:SEQ_LENGTH]\n",
    "  # Labels are pointing to the same placeholders!\n",
    "  # Labels are inputs shifted by one time step.\n",
    "  train_labels = input_buffer[1:]  \n",
    "  print (\"Seq length  =\", len(train_inputs))\n",
    "  print (\"Batch shape =\", train_inputs[0].shape)\n",
    "  # Concatenate targets into 2D tensor.\n",
    "  targets = tf.concat(train_labels, 0)\n",
    "\n",
    " # 2. Training LSTM ops.\n",
    "with tf.name_scope(\"LSTM\"):\n",
    "  # Unrolled LSTM loop.\n",
    "  # Build outpus of size SEQ_LENGTH.\n",
    "  outputs = list()\n",
    "  output = prev_output\n",
    "  state = prev_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state, \"cell\")\n",
    "    outputs.append(output)\n",
    "  print (len(outputs))\n",
    "  print (outputs[0].shape)\n",
    "  print (tf.concat(outputs, 0).shape)\n",
    "\n",
    "# Fully connected layer on top => classification.\n",
    "# In fact we will create lots of FC layers (one for each output layer), with shared weights.\n",
    "logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b, name = \"Final_FC\")\n",
    "\n",
    "# 2. Loss ops.\n",
    "with tf.name_scope(\"Loss\"):\n",
    "    # Loss function(s) - one for every output generated by every lstm cell.\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=targets, logits=logits))\n",
    "    # Add loss summary.\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "# 3. Training ops.  \n",
    "with tf.name_scope(\"Optimization\"):\n",
    "    # Learning rate decay.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.1, global_step, 5000, 0.9, staircase=True)\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    # Gradient clipping.\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "# 4. Predictions ops.  \n",
    "with tf.name_scope(\"Evaluation\") as scope:\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgraph responsible for generation of sample texts, char by char."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Sample_generation\") as scope:\n",
    "  # Create graphs for sampling and validation evaluation: batch 1, \"no unrolling\".\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size], name=\"Input_data\")\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"Output_data\")\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, HIDDEN_SIZE]), name=\"Hidden_state\")\n",
    "\n",
    "  # Node responsible for resetting the state and output.\n",
    "  reset_sample_state = tf.group(\n",
    "      saved_sample_output.assign(tf.zeros([1, HIDDEN_SIZE])),\n",
    "      saved_sample_state.assign(tf.zeros([1, HIDDEN_SIZE])))\n",
    "  # Single LSTM cell.\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state, \"cell\")\n",
    "  # Output depends on the hidden state.\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b, name=\"logits\"), name=\"outputs\")\n",
    "\n",
    "# Merge all summaries.\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "# 4. Init global variable.\n",
    "init = tf.global_variables_initializer() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for language generation (letter sampling etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_feed_dict(dataset):\n",
    "  \"\"\"Creates a dictionaries for different sets: maps data onto Tensor placeholders.\"\"\"\n",
    "  feed_dict = dict()\n",
    "  if dataset==\"train\":\n",
    "    # Get next batch and create a feed dict.\n",
    "    next_batch = train_batches.next()\n",
    "    for i in range(SEQ_LENGTH + 1):\n",
    "        feed_dict[input_buffer[i]] = next_batch[i]\n",
    "    # Reset previous state and output\n",
    "    feed_dict[prev_output] = np.zeros([BATCH_SIZE, HIDDEN_SIZE])\n",
    "    feed_dict[prev_state] = np.zeros([BATCH_SIZE, HIDDEN_SIZE])\n",
    "        \n",
    "  elif dataset==\"valid\":\n",
    "    for i in range(SEQ_LENGTH + 1):\n",
    "        feed_dict[input_buffer[i]] = valid_batch[i]\n",
    "    # Reset previous state and output\n",
    "    feed_dict[prev_output] = np.zeros([VALID_BATCH_SIZE, HIDDEN_SIZE])\n",
    "    feed_dict[prev_state] = np.zeros([VALID_BATCH_SIZE, HIDDEN_SIZE])\n",
    "    \n",
    "  else: # test\n",
    "    for i in range(SEQ_LENGTH + 1):\n",
    "        feed_dict[input_buffer[i]] = test_batch[i]\n",
    "    # Reset previous state and output\n",
    "    feed_dict[prev_output] = np.zeros([TEST_BATCH_SIZE, HIDDEN_SIZE])\n",
    "    feed_dict[prev_state] = np.zeros([TEST_BATCH_SIZE, HIDDEN_SIZE])\n",
    "    \n",
    "  return feed_dict # {prev_output: train_output_zeros, prev_state: train_state_zeros }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Eventually clear the log dir.\n",
    "if tf.gfile.Exists(LOG_DIR):\n",
    "  tf.gfile.DeleteRecursively(LOG_DIR)\n",
    "# Create (new) log dir.\n",
    "tf.gfile.MakeDirs(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Number of iterations per epoch = 5101\n",
      "Training set BPC at step 0: 4.07772 learning rate: 0.100000\n",
      "================================================================================\n",
      "]fYBdeFIrs TNM[spiPUIPznBwesace[[B`Cgum^ylTklomogheiX\\p dUulmlX itn\\tuDsluegnftn\n",
      "rrBLhpUPolUtsMEvTlthv xa`sxiptbKd_]dZopzi eI_SteydkmGgIKBbeaphd sleaubiKxtSKgZcK\n",
      "C`qv`zvQ_KS`eadrmdafdxdbjBkfalxsue`ShKccuiPsGaMhpeoQAC cNtCiRq uomiWsr  pin]zsrn\n",
      "shRaZwAfXPZybZmgpon HLpGeqouveRw dtjatg  eirl Gdlnpmel ocfDNsRh xh ei ateshQEulp\n",
      "n]ndyKVZtGtXdx  u  vidsrm n kpmNadFlrsga twSUhlcJimersgr  a f lr  rw^Naaqbcnprnt\n",
      "================================================================================\n",
      "Validation set BPC: 3.53230\n",
      "Training set BPC at step 100: 2.37653 learning rate: 0.100000\n",
      "Training set BPC at step 200: 2.22288 learning rate: 0.100000\n",
      "Training set BPC at step 300: 2.04186 learning rate: 0.100000\n",
      "Training set BPC at step 400: 2.00319 learning rate: 0.100000\n",
      "Training set BPC at step 500: 1.98040 learning rate: 0.100000\n",
      "Training set BPC at step 600: 1.90804 learning rate: 0.100000\n",
      "Training set BPC at step 700: 1.94499 learning rate: 0.100000\n",
      "Training set BPC at step 800: 1.92651 learning rate: 0.100000\n",
      "Training set BPC at step 900: 1.89043 learning rate: 0.100000\n",
      "Training set BPC at step 1000: 1.91463 learning rate: 0.100000\n",
      "================================================================================\n",
      "G ann was ly expam of ass ontly sy   N  the for   N on euserson sectory in mont \n",
      "er  unk  of nor wegiod   justers almen   bute ands N vosido deding incal expecte\n",
      "come in  unk  thense sport of adds allows sple  pplan   fo masinust to deduct   \n",
      "ontonstqued housition are wigh a compalino and   was by execter of t diel  unk  \n",
      "Orid  unk  aldion  acal  unk  was to wavior neenet them was re  unk  that amonvi\n",
      "================================================================================\n",
      "Validation set BPC: 1.88961\n",
      "Training set BPC at step 1100: 1.91163 learning rate: 0.100000\n",
      "Training set BPC at step 1200: 1.78839 learning rate: 0.100000\n",
      "Training set BPC at step 1300: 1.91680 learning rate: 0.100000\n",
      "Training set BPC at step 1400: 1.82663 learning rate: 0.100000\n",
      "Training set BPC at step 1500: 1.91032 learning rate: 0.100000\n",
      "Training set BPC at step 1600: 1.83530 learning rate: 0.100000\n",
      "Training set BPC at step 1700: 1.86253 learning rate: 0.100000\n",
      "Training set BPC at step 1800: 1.86638 learning rate: 0.100000\n",
      "Training set BPC at step 1900: 1.90945 learning rate: 0.100000\n",
      "Training set BPC at step 2000: 1.84285 learning rate: 0.100000\n",
      "================================================================================\n",
      "Ggedder gore a sead comrul op as eurliding in not of mesainione is to N treal in\n",
      "nd frocaligarl othid othe put in couse said hiders lattly ans N N oas or joone p\n",
      "handim tail says in dolie wive over inn own aultoffory intering stase thes syies\n",
      "Crecauge lasution pendue he   tip  N to N N abillen saltive N N  unk  pond hault\n",
      "B breat for to companqu stowiateater by gone ackter inc   unk  of  unk  krods se\n",
      "================================================================================\n",
      "Validation set BPC: 1.84988\n",
      "Training set BPC at step 2100: 1.86759 learning rate: 0.100000\n",
      "Training set BPC at step 2200: 1.81625 learning rate: 0.100000\n",
      "Training set BPC at step 2300: 1.78211 learning rate: 0.100000\n",
      "Training set BPC at step 2400: 1.86805 learning rate: 0.100000\n",
      "Training set BPC at step 2500: 1.82819 learning rate: 0.100000\n",
      "Training set BPC at step 2600: 1.87756 learning rate: 0.100000\n",
      "Training set BPC at step 2700: 1.84246 learning rate: 0.100000\n",
      "Training set BPC at step 2800: 1.88943 learning rate: 0.100000\n",
      "Training set BPC at step 2900: 1.85559 learning rate: 0.100000\n",
      "Training set BPC at step 3000: 1.88455 learning rate: 0.100000\n",
      "================================================================================\n",
      "y spliction suppen wousts  s it will with depisamen  unk  retering about to  unk\n",
      "y prightered in  unk  to ans in and the britweadions  unk  fire through tiness  \n",
      "the to five gently bowmolate   pip griessokege which the the the confime  unk di\n",
      "hen s ugh quartized said that ascurpion jand  unk  into died  unk  bilrount mart\n",
      "t on mean to would the sales by too  unk  share ind a rees gropysting to evinsue\n",
      "================================================================================\n",
      "Validation set BPC: 1.83491\n",
      "Training set BPC at step 3100: 1.77685 learning rate: 0.100000\n",
      "Training set BPC at step 3200: 1.85734 learning rate: 0.100000\n",
      "Training set BPC at step 3300: 1.90832 learning rate: 0.100000\n",
      "Training set BPC at step 3400: 1.84810 learning rate: 0.100000\n",
      "Training set BPC at step 3500: 1.86746 learning rate: 0.100000\n",
      "Training set BPC at step 3600: 1.78346 learning rate: 0.100000\n",
      "Training set BPC at step 3700: 1.89120 learning rate: 0.100000\n",
      "Training set BPC at step 3800: 1.86282 learning rate: 0.100000\n",
      "Training set BPC at step 3900: 1.76462 learning rate: 0.100000\n",
      "Training set BPC at step 4000: 1.90771 learning rate: 0.100000\n",
      "================================================================================\n",
      "er union  unk  falcrucentie the sects searclosogavidust inpor earntas that compa\n",
      "Minter explils univelowne marp to owlom  unk  advank ward expord excrece assudai\n",
      "ctive  s lictich shound   ofiats by one but wadgoury sural sed with year main wo\n",
      "y aimidented  s the bored co  unk  orde yeptings   um  unk  intervers bote bonan\n",
      "N in gad of said throuch of N offor offor co new at not and to  unk  sevendawing\n",
      "================================================================================\n",
      "Validation set BPC: 1.83330\n",
      "Training set BPC at step 4100: 1.87602 learning rate: 0.100000\n",
      "Training set BPC at step 4200: 1.76934 learning rate: 0.100000\n",
      "Training set BPC at step 4300: 1.83668 learning rate: 0.100000\n",
      "Training set BPC at step 4400: 1.86795 learning rate: 0.100000\n",
      "Training set BPC at step 4500: 1.89849 learning rate: 0.100000\n",
      "Training set BPC at step 4600: 1.83478 learning rate: 0.100000\n",
      "Training set BPC at step 4700: 1.83699 learning rate: 0.100000\n",
      "Training set BPC at step 4800: 1.87167 learning rate: 0.100000\n",
      "Training set BPC at step 4900: 1.89073 learning rate: 0.100000\n",
      "Training set BPC at step 5000: 1.84884 learning rate: 0.090000\n",
      "================================================================================\n",
      "Se resiscated fimure aring  unk  new to cusinez to index are its sorild hand fra\n",
      "\\ed   ares the the  unk daitised  unk  trand of that reder   N  unk   unk  cers \n",
      " of the have the and expenses the the buitted mric sales mosts  unk  executtly o\n",
      "he reculivos alreek dided to meaita sugs thippliced the sanist of supheon N mite\n",
      "O in whs  itils to coanils the salation beonew  unk  isp badital tokeermite terk\n",
      "================================================================================\n",
      "Validation set BPC: 1.82198\n",
      "Training set BPC at step 5100: 1.78626 learning rate: 0.090000\n",
      "Calculating BPC on test dataset\n",
      "Final test set BPC: 1.80089\n"
     ]
    }
   ],
   "source": [
    "# How often the test loss on validation batch will be computed. \n",
    "summary_frequency = 100\n",
    "\n",
    "# Create session.\n",
    "sess = tf.InteractiveSession()\n",
    "# Create summary writers, point them to LOG_DIR.\n",
    "train_writer = tf.summary.FileWriter(LOG_DIR + '/train', sess.graph)\n",
    "valid_writer = tf.summary.FileWriter(LOG_DIR + '/valid')\n",
    "test_writer = tf.summary.FileWriter(LOG_DIR + '/test')\n",
    "\n",
    "# Initialize global variables.\n",
    "tf.global_variables_initializer().run()\n",
    "print('Initialized')\n",
    "\n",
    "num_steps =  train_size // (BATCH_SIZE*SEQ_LENGTH) #70001\n",
    "print(\"Number of iterations per epoch =\", num_steps)\n",
    "for step in range(num_steps):\n",
    "    # Run training graph.\n",
    "    batch = train_batches.next()\n",
    "    summary, _, t_loss, lr = sess.run([merged_summaries, optimizer, loss, learning_rate], \n",
    "                                      feed_dict=create_feed_dict(\"train\"))\n",
    "    # Add summary.\n",
    "    train_writer.add_summary(summary, step*BATCH_SIZE*SEQ_LENGTH)\n",
    "    train_writer.flush()\n",
    "\n",
    "    # Every (100) steps collect statistics.\n",
    "    if step % summary_frequency == 0:\n",
    "      # Print loss from last batch.\n",
    "      print('Training set BPC at step %d: %0.5f learning rate: %f' % (step, t_loss, lr))\n",
    "    \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate sample text...\n",
    "        print('=' * 80)\n",
    "        # consisting of 5 lines...\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          # Reset LSTM hidden state.\n",
    "          reset_sample_state.run()\n",
    "          # with 79 characters in each.\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "        \n",
    "        # Validation set BPC.\n",
    "        v_summary, v_loss = sess.run([merged_summaries, loss], feed_dict=create_feed_dict(\"valid\"))\n",
    "        print(\"Validation set BPC: %.5f\" % v_loss)\n",
    "        valid_writer.add_summary(v_summary, step*BATCH_SIZE*SEQ_LENGTH)\n",
    "        valid_writer.flush()\n",
    "    # End of statistics collection\n",
    "\n",
    "# Test set BPC.\n",
    "print(\"Calculating BPC on test dataset\")\n",
    "t_summary, t_loss = sess.run([merged_summaries, loss], feed_dict=create_feed_dict(\"test\"))\n",
    "print(\"Final test set BPC: %.5f\" % t_loss)\n",
    "test_writer.add_summary(t_summary, step*BATCH_SIZE*SEQ_LENGTH)\n",
    "test_writer.flush()\n",
    "    \n",
    "# Close writers and session.\n",
    "train_writer.close()\n",
    "valid_writer.close()\n",
    "test_writer.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
